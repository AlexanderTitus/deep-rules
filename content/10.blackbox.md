## Tip 8: Deep learning models can be made more transparent {#blackbox}

While model interpretability is a broad concept, in much of the machine learning literature (including in our guidelines), it refers to the ability to identify the discriminative features that influence or sway the predictions.
In certain cases, the goal behind interpretation is to understand the underlying data generating processes and biological mechanisms [@doi.org/10.3390/biom10030454].
In other cases, the goal is to understand why a model made the prediction that it did for a specific example or set of examples.
Machine learning models vary widely in terms of interpretability: some are fully transparent while others are considered "black-boxes" that make predictions with little ability to examine why.
Logistic regression and decision tree models are generally considered interpretable [@doi:10.1007/978-1-4939-7756-7_16].
In contrast, deep neural networks are often considered among the most difficult to interpret because they can have many parameters and non-linear relationships. 

Knowing which of the input variables influences a model's predictions, and potentially in what ways, can help with the application or extrapolation of machine learning models.
This is particularly important in biomedicine, where subsequent decision making often requires human input, and where models are employed with the hope of better understanding why relationships exist in the first place.
Furthermore, while prediction rules can be derived from high-throughput molecular datasets, most affordable clinical tests still rely on lower-dimensional measurements of a limited number of biomarkers.
Therefore, it is often still unclear how to translate the predictive capacity of deep learning models that encompassing non-linear relationships between countless input variables into clinically digestible terms.
As a result, selecting which biomarkers to use for decision making remains an important modeling and interpretation challenge.
In fact, many authors attribute a lower uptake of deep learning tools in healthcare to interpretability challenges [@doi:10.1109/JBHI.2016.2636665; @doi:10.1038/s41551-018-0315-x]. 
Nonetheless, strategies to interpret both machine learning and deep learning models are rapidly emerging, and the literature on the topic is growing exponentially [@arxiv:2001.02522].
Instead of recommending specific methods for either deep learning-specific or general-purpose model interpretation, we suggest consulting [@url:https://christophm.github.io/interpretable-ml-book/], which is freely available and continually updated.

While active research into model interpretability is enabling increased interpretation of models with many parameters and non-linear relationships, simpler traditional machine learning models often remain substantially easier to interpret.
When deciding on a machine learning approach and model architecture, consider an interpretability versus accuracy tradeoff.
A challenge in considering this tradeoff is that the extent to which one trades interpretability for accuracy depends on the problem itself.
Recent research has also shown which interpretability method is best for which model also depends on the model's predictive performance for a given problem and dataset regarding the reliability of local and global explanations [@arXiv:2011.09903].
As a rule of thumb, when the features provided to the model are already highly relevant to the task at hand, a simpler and more interpretable model that gives up only a little performance is often more useful.
On the other hand, if features must be combined in complex ways to be meaningful for the task, the performance difference of a model capable of capturing that structure may outweigh the interpretability costs.
An appropriate choice can only be made after careful consideration, which often includes estimating the performance of a simple linear model that serves as a [baseline](#baselines).
In cases where models are learned from high-throughput datasets, a small subset of features in the dataset may be strongly correlated with the complex combination of the larger feature set defined from the deep learning model.
In this case, this more limited number of features can be used in the subsequent simplified model to enhance the model's interpretability further.
This feature reduction can be essential when defining biomarker panels for use in clinical applications.
