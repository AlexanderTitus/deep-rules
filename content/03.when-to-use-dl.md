## Tip 1: Decide whether deep learning is appropriate for your problem {#appropriate}

In recent years, the number of publications implementing DL in biology have risen tremendously.
Given DL's usefulness across a range of scientific questions and data modalities, it may appear that it is a panacea for modeling problems.
Indeed, neural networks are universal function approximators, meaning that they are in principle capable of learning any function [@doi:10.1007/BF02551274; @tag:hornik-approximation].
If DL is so powerful and popular, why would one ever not choose to use it?

The reason is simple: DL is not suited to every situation in reality.
Training DL models requires a significant amount of data, computing power, and expertise.
In some areas of biology where data collection is thoroughly automated, such as DNA sequencing, large amounts of quality data may be available.
For other areas which rely on manual data collection, there may not be enough data to effectively train models.
Though there are methods to increase the amount of training data such as data augmentation (in which existing data is slightly manipulated to yield "new" samples) and weak supervision (in which simple labeling heuristics are combined to produce noisy, probabilistic labels) [@arxiv:1605.07723v3], these methods cannot overcome a complete shortage of data.
As a rule of thumb, DL should only be considered for datasets with at least one thousand samples, though it is best suited to cases when datasets contain orders of magnitude more samples.

Furthermore, training DL models can be very demanding, often requiring extensive computing infrastructure and patience to achieve state-of-the-art performance [@doi:10.1109/JPROC.2017.2761740].
In some DL contexts, such as generating human-like text, state-of-the-art models have over one hundred billion parameters [@arxiv:2005.14165].
Training such large models from scratch can be a costly and time-consuming undertaking [@arxiv:1906.02243].
Luckily, most DL research in biology will not require nearly as much computation, and there are methods for reducing the amount of training required in some cases (described in [Tip 5](#architecture)).
Specialized hardware such as discrete graphics processing units (GPUs) or custom DL accelerators can dramatically reduce the time and cost required to train models, but this hardware is not universally accessible.
Currently, both GPU- and DL-optimized accelerator-equipped servers can be rented from cloud providers, though working with these servers adds additional cost and complexity.
As DL becomes more popular, DL-optimized accelerators are likely to be more broadly available (for example, recent-generation iPhones already have such hardware).
In contrast, traditional ML training can often be done on a laptop (or even a \$5 computer [@arxiv:1809.00238]) in seconds to minutes.

Beyond the necessity for greater data and computational capacity in DL, building and training DL models generally requires more expertise than traditional ML models.
Currently, there are several competing programming frameworks for DL such as Tensorflow [@arxiv:1603.04467] and PyTorch [@arxiv:1912.01703].
These frameworks allow users to create and deploy entirely novel model architectures and are widely used in DL research as well as in indutry.
This flexibility combined with the rapid development of the DL field has resulted in large, complex frameworks that can be daunting to new users.
For readers new to software development but experienced in biology, gaining computational skills while interfacing with such complex industrial-grade tools can be a challenge.
An advantage of ML over DL is that currently there are more tools capable of automating the model selection and training process.
Automated ML (AutoML) tools such as TPOT [@doi:10.1007/978-3-319-31204-0_9], which is capable of using genetic programming to optimize ML pipelines, and Turi Create [@https://github.com/apple/turicreate], a task-oriented ML and DL framework which automatically tests multiple ML models when training, allow users to achieve competitive performance with only a few lines of code.
Luckily, there are efforts underway to reduce the expertise required to build and use DL models.
Indeed, both TPOT and Turi Create, as well as other tools such as AutoKeras [@arxiv:1806.10282], are capable of abstracting away much of the programming required for "standard" DL tasks.
Projects such as Keras [@https://keras.io], a high-level interface for TensorFlow, make it relatively straightforward to design and test custom DL architectures.
In the future, projects such as these are likely to bring DL experimentation within reach to even more researchers.

There are some types of problems in which using DL is strongly indicated over ML.
Assuming a sufficient quantity of quality data is available, applications such as computer vision and natural language processing are likely to benefit from DL.
In fact, these areas were the first to see significant breakthroughs through the application of DL [@doi:10.1145/3065386] during the recent "DL revolution."
For example, Ferreira et al. used DL to recognize individual birds from images [@doi:10.1111/2041-210X.13436].
This problem was historically difficult but, by combining automatic data collection using RFID tags with data augmentation and transfer learning (explained in [Tip 5](#architecture), the authors were able to use DL achieve 90% accuracy in several species.
Other areas include generative models, in which new samples are able to be created based on the training data, and reinforcement learning, in which agents are trained to interact with their environments.
In general, before using DL, investigate whether similar problems (including analogous ones in other domains) have been solved successfully using DL.

Depending on the amount and the nature of the available data, as well as the task to be performed, DL may not always be able to outperform conventional methods.
As an illustration, Rajkomar et al. [@doi:10.1038/s41746-018-0029-1] found that simpler baseline models achieved performance comparable with that of DL in a number of clinical prediction tasks using electronic health records, which may be a surprise to many.
Another example is provided by Koutsoukas et al., who benchmarked several traditional machine learning approaches against deep neural networks for modeling bioactivity data on moderately sized datasets [@doi:10.1186/s13321-017-0226-y].
The researchers found that while well tuned DL approaches generally tend to outperform conventional classifiers, simple methods such as Naive Bayes classification tend to outperform DL as the noise in the dataset increases.
Similarly, Chen et al. [@doi:s41746-019-0122-0] tested DL and a variety of traditional ML methods such as logistic regression and random forests on five different clinical datasets, finding that the non-DL methods matched or exceeded the accuracy of the DL model in all cases while requiring an order of magnitude less training time.

DL is a tool and, like any other tool, must be used after consideration of its strengths and weaknesses for the problem at hand.
Once settled upon DL as a potential solution, practitioners should follow the scientific method and compare its performance to traditional methods, as we will see next.
