## Tip 6: Tune your hyperparameters extensively and systematically {#hyperparameters}

Given at least one hidden layer, a non-linear activation function, and a large number of hidden units, multi-layer neural networks can approximate arbitrary continuous functions that relate input and output variables [@tag:hornik-approximation; @doi:10.1016/S0893-6080(05)80131-5]. 
Deeper architectures that feature additional hidden layers and an increasing number of overall hidden units and learnable weight parameters (the so-called increasing "capacity" of neural networks) allow for solving increasingly complex problems.
However, this increased capacity results in many more parameters to fit and hyperparameters to tune, which can pose additional challenges during model training.
In general, one should expect to systematically evaluate the impact of numerous hyperparameters when applying deep neural networks to new data or challenges.
Hyperparameters typically manifest as choices or settings of optimization algorithms, loss function, learning rate, activation functions, number of hidden layers and hidden units, size of the training batches, weight initialization schemes, and seeds for pseudo-random number generators used for dataset shuffling and weight initialization.
Moreover, additional hyperparameters are introduced by common techniques that facilitate the training of deeper architectures.
These include parameter norm penalties (typically in the form of $L^2$ regularization), dropout [@tag:srivastava-dropout], and batch normalization [@tag:ioffe-batchnorm], which can reduce the effect of the so-called vanishing or exploding gradient problem when working with deep neural networks.

This wide array of potential parameters can make it difficult to evaluate the extent to which neural network methods are well suited to solving a task, as it can be unclear to practitioners whether previous successful applications were the result of interactions between unique data attributes and specific hyperparameter settings.
Similar to the Continental Breakfast Included effect discussed in [Tip 2](#baselines), a lack of clarity on how extensive arrays of hyperparameters were tested and/or chosen can affect method developers as they attempt to compare techniques.
This effect also has implications for those seeking to use existing deep learning methods, as performance estimates from deep neural networks are often provided after tuning.
The implication of this effect on users of deep neural networks is that attaining performance numbers that match those reported in publications is likely to require significant effort towards temporally expensive hyperparameter optimization.

Ultimately, to get the best performance of your model, be sure to systematically optimize your hyperparameters on your training dataset, as introduced in the next section.
