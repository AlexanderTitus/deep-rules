## Tip 9: Don't over-interpret predictions {#interpretation}

Once we have trained an accurate deep learning model, we often want to use it to deduce relationships and inform scientific findings.
However, in doing this, we need to be careful to interpret the model's predictions correctly.
Given that deep learning models can be difficult to interpret intuitively, there is often a temptation to overinterpret the predictions in indulgent and/or inaccurate ways.
In accordance with the classic statistical saying, "correlation doesn't imply causation," predictions by deep learning models don't necessarily speak to certain causal relationships.
While we generally know this and understand that accurately predicting an outcome doesn't imply the learning of any causal mechanism, it can be easy to forget this lesson when the predictions are extremely accurate.
A poignant example of this lesson is from work where authors evaluated the capacities of several models to predict the probability of death for patients with pneumonia admitted to an intensive care unit [@tag:predicting-pneumonia-mortality; @doi:10.1145/2783258.2788613].
Unsurprisingly, the neural network model achieved the best predictive accuracy.
However, after fitting a rule-based model to understand the relationships inherent to their data better, the authors discovered that the hospital data implied the rule "$\text{HasAsthma}(x) \Rightarrow \text{LowerRisk}(x)$."
This rule contradicts medical understanding, as having asthma doesn't make pneumonia better!
Nonetheless, the data supported this rule, as pneumonia patients with a history of asthma tended to receive more aggressive care.
The neural network had, therefore, also learned to make predictions according to this rule despite the fact that it has nothing to do with causality or mechanism.
According to the predictions of the neural network, guiding treatment decisions would have been disastrous, even though the neural network had high predictive accuracy.

To trust deep learning models, we must combine knowledge of the training data ([Tip 4](#know-your-problem)) with inspection of the model ([Tip 8](#blackbox)).
To move beyond fitting predictive models and towards the building of an understanding that can inform scientific deduction, we suggest working to disentangle a model's internal logic by comparing data domains where models succeed to those in which they fail.
By doing so, we can avoid overinterpreting models and view them for what they are: complex statistical models trained on high dimensional data.
