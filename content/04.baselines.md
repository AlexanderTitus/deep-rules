## Tip 2: Use traditional methods to establish performance baselines {#baselines}

Deep learning requires practitioners to consider a larger number and variety of tuning parameters (_i.e._ algorithmic settings) than more traditional machine learning methods.
These settings are often called hyperparameters, and their extensiveness can make it easy to fall into the trap of performing an unnecessarily convoluted analysis.
Hence, before applying deep learning to a given problem, we highly recommend implementing a simpler model with fewer hyperparameters at the beginning of each study. 
Such models include logistic regression, random forests, k-nearest neighbors, naive Bayes, and support vector machines, and using them can help to establish baseline performance expectations.
While performance baselines available from existing literature can also serve as helpful guides, an implementation of a simpler model that uses the same software framework as planned for DL can greatly help with assessing the correctness of data processing steps, performance evaluation pipelines, resource requirement estimates, and computational performance estimates.
Furthermore, in some cases, it can even be useful to combine simpler baseline models with deep neural networks, as such hybrid models can improve generalization performance, model interpretability, and confidence estimation [@arxiv:1803.04765; @arxiv:1805.11783].

However, it is important to gauge the relative effectiveness of baseline and DL models by comparing them with established tools (_e.g._ bioinformatics pipelines or image analysis workflows), as conventional methods (machine learning based or not) can potentially perform equivalently to or better than newer ML/DL methods.
While this seems unintutive, it can in fact be the case when the available data are of limited size and/or atypical in nature. 
For example, Rajkomar et al. [@doi:10.1038/s41746-018-0029-1] found that simpler baseline models achieved performance comparable with that of DL in a number of clinical prediction tasks using electronic health records.
Another example is provided by Koutsoukas et al., who benchmarked several traditional machine learning approaches against deep neural networks for modeling bioactivity data on moderately sized datasets [@doi:10.1186/s13321-017-0226-y].
The researchers found that while well tuned deep learning approaches generally tend to outperform conventional classifiers, simpler conventional methods such as Naive Bayes classification tend to outperform deep learning as the noise in the dataset increases.

Another potential pitfall arises from comparing the performance of baseline conventional models trained with default settings with the performance of DL models that have undergone rigorous tuning and optimization.
Since conventional off-the-shelf machine learning algorithms (_e.g._ support vector machines and random forests) are also likely to benefit from hyperparameter tuning, such incongreuty prevents the comparison of equally optimized models and can lead to false conclusions about model efficacy. 
Hu and Greene [@doi:10.1142/9789813279827_0033] discuss this under the umbrella of what they call the "Continental Breakfast Included" effect, and they describe how the unequal tuning of hyperparameters across different learning algorithms can especially skew evaluation when the performance of an algorithm varies substantially with modest changes to its hyperparameters.
Therefore, practitioners should tune the settings of both traditional and DL-based methods before making claims about relative performance differences, as performance comparisons among ML and DL models are only informative when the models are equally well optimized.

To sum this tip up, practitions are encouraged to create and fully tune several traditional models and standard pipelines before implementing a DL model.
