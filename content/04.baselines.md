## Tip 2: Use traditional methods to establish performance baselines {#baselines}

It is important to implement a simple model to establish an adequate performance baseline, since it is easy to fall into the trap of an unnecessarily convoluted analysis.
For example, a researcher could first a build a multinomial logistic regression or random forest model using the same software framework planned for DL to first evaluate the classification performance.
This approach will help researchers with assessing the complexity of the task at hand and debugging more complex DL architectures.
The utility of these methods is evidenced by the recent development of hybrid models which combine DL and simpler models to improve robustness, interpretability, and confidence estimation [@arxiv:1803.04765; @arxiv:1805.11783].
Depending on the amount of available data and the type of tasks, DL models may not necessarily perform the best.
As an illustration, the simple baseline models by Rajkomar et al. [@doi:10.1038/s41746-018-0029-1] achieved performance comparable with that of DL in a number of clinical prediction tasks using electronic health records, which may be a surprise to many.

It is worth noting that conventional machine learning methods (e.g., support vector machines, random forests) are also likely to benefit from parameter tuning.
It can be tempting to train baseline models with these conventional methods using default parameters, which may provide acceptable but not stellar performance, but then tune the parameters for DL models to optimize performance.
Hu and Greene [@doi:10.1101/385534] discuss a "Continental Breakfast Included" effect by which unequal hyperparameter tuning skews the evaluation of methods, especially those with performance that varies substantially with modest changes to hyperparameters.
Those wishing to compare methods should tune the parameters of traditional and DL to optimize performance before making claims about relative performance differences.
The performance comparison among DL models and many other ML approaches is informative only when the models are similarly tuned.
