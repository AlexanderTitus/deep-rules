## Tip 8: Your DL models can be more transparent {#blackbox} 
In ML, interpretability refers to the study of the discriminative features used for classification or regression task.
ML models can vary in terms of interpretability from a “transparent” to a “black-box” model, the first with a clear description of features importance found, for example, in common random forests implementations [﻿Breiman, Leo. “Random forests.” Machine learning 45.1 (2001): 5-32.]. 
The second for the most widely used DL implementations.  
Because of the large number of parameters and non-linear relationships among features, DL models are hard to interpret when compared to other ML models. 

There are, however, many strategies to interpret a DL models.
For example, autoencoders (AE) is a family of unsupervised methods that aim to learn a new (encoded) representation and minimize the error between the new representation and the input data. 
Tan et al., [@doi:10.1142/9789814644730_0014] used a denoising AE to summarize key features from breast cancer dataset.
The authors could map encoded features to clinical characteristics relevant to the disease. 

Model transparency is notably important in the biomedical field.
Many authors attribute the lack of pervasiveness of deep learning tools in healthcare because of the inability to understand what these models learn [10.1109/JBHI.2016.2636665,10.1038/s41551-018-0315-x]. 
In conclusion, we encourage beginners of the DL to train in parallel a base model that is easier to interpret. 
In case the difference in accuracy is too high to trade-off with the DL model, pre-training AE may help to dissect which are discriminative features. 
Alternatively, algorithms based on Garson work [Garson:1991 https://dl.acm.org/citation.cfm?id=129452] can help to investigate the weights of a DL model to better understand it [TODO detail the Garson’s algorithm?].
