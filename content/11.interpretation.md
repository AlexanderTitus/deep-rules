## Tip 9: Don't over-interpret predictions {#interpretation}

Once you have trained an accurate deep model, you'll likely want to use it to deduce scientific findings.
In doing so, you need to take care to correctly interpret the model's predictions.
Although you know that the basic tenets of machine learning also apply to deep learning ([Tip 1](#concepts)), because deep models can be difficult to interpret intuitively, there is a temptation to anthropomorphize the models.
Resist this temptation.

A common saying in statistics classes is "correlation doesn't imply causality".
Although accurately predicting an outcome doesn't imply learning the causal mechanism, it can be easy to forget this lesson when the predictions are extremely accurate.
A poignant example of this lesson is [@tag:predicting-pneumonia-mortality; @doi:10.1145/2783258.2788613].
In this study, the authors evaluated the capacities of several models to predict the probability of death for patients admitted to an intensive care unit with pneumonia.
Unsurprisingly, the neural network model achieved the best predictive accuracy.
However, after fitting a rule-based model, the authors discovered that the hospital data implied the rule "HasAsthma(x) => LowerRisk(x)".
This rule contradicts medical understanding - having asthma doesn't make pneumonia better!
This rule was supported by the data (pneumonia patients with a history of pneumonia tended to receive more aggressive care), so the neural network also learned to make predictions according to this rule.
Guiding treatment decisions according to the predictions of the neural network would have been disastrous, even though the neural network had high predictive accuracy.

To trust deep learning models, you must combine knowledge of the training data ([Tip 4](#know-your-problem)) with inspection of the model ([Tip 8](#blackbox)).
By probing data domains where the model succeeds and contrasting with domains where the model fails, you can identify the internal logic and deduce scientific conclusions.
In this way, you can move beyond fitting predictive models toward building understanding.
