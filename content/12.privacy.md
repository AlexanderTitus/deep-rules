## Tip 10: Don't share models trained on sensitive data {#privacy}

Practitioners may encounter datasets that cannot be shared, such as ones for which there would be significant ethical or legal issues associated with release [@doi:10.1371/journal.pcbi.1005399].
Examples of such data include classified, confidential, trade secret biological data as well as medical records, certain genomic assays, and personally identifiable information [@doi:10.1038/s41576-020-0257-5].
One of the greatest opportunities for deep learning in biology is the ability for these techniques to extract information that cannot readily be captured by traditional methods [@arxiv:1509.09292].
The representation learning of the deep learning models can capture information-rich abstractions of multiple features of the data during the training process.
However, these features may be more prone to leak the data that they were trained over if the model is shared or allowed to be queried with arbitrary inputs.
Thus, with both deep learning and certain traditional machine learning methods (_e.g._, _k_-nearest neighbors models, which learn by memorizing the full training data), it is imperative not to share models trained on sensitive data.

Techniques to train deep neural networks without sharing unencrypted access to data are being advanced through implementations of homomorphic encryption [@doi:10.1371/journal.pcbi.1006454; @arxiv:1811.00778], but adversarial training techniques such as model inversion attacks can be used to exploit model predictions to recover recognizable images of people's faces used for training [@doi:10.1145/2810103.2813677].
Privacy preserving techniques [@arxiv:1811.04017], such as differential privacy [@doi:10.1145/2976749.2978318; @doi:10.1101/159756; @arxiv:1812.01484], can help to mitigate risks as long as the assumptions underlying these techniques are met.
These techniques provide a path towards a future where models can be shared, but more software development and theoretical advances will be required to make these techniques easy to apply in many settings.
Until then, don't share models trained on sensitive data.
