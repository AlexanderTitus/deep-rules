## Tip 8: Your DL models can be more transparent {#blackbox}

In ML, a model's interpretability refers to the ability to identify the discriminative features used for learning and that influence or sway the predictions.
ML models vary widely in terms of interpretability, ranging from being fully transparent to being a “black-box”.
In the former case, a clear description of the features' importance can found, such as with logistic regression or decision trees. 
However, DL models are harder to interpret compared to other ML models because of their large number of parameters and the non-linear relationships between their features.

Model interpretability is particularly important in biomedicine, where subsequent decision making requires human input.
Many authors attribute the lack of pervasiveness of DL tools in healthcare because of the inability to understand what these models learn [@doi:10.1109/JBHI.2016.2636665; @doi:10.1038/s41551-018-0315-x]. 
Luckily, there are many strategies to interpret both ML and DL models.
In fact, due to the increasing demand for interpretable models, the literature on the topic is currently growing at an exponential rate [@arxiv:2001.02522].
Therefore, instead of recommending specific methods for either DL-specific or general-purpose model interpretation, we suggest consulting a recent overview.
We particularly recommend [@url:https://christophm.github.io/interpretable-ml-book/] which is freely available and continually updated.

In conclusion, analyze your DL models to identify how their inputs are interacting as they generate their predictions, especially when your models are being used for critical decision making.
