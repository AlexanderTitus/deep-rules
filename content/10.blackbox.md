#Rule 8: Do not necessarily consider a DL model as a black box
In ML, interpretability refers to the study of the discriminative features used for classification or regression task. In general, DL models need little data engineering and have a very large number of parameters (**TODO** link it somewhere else in the manuscript). Thus, DL models are relatively hard to interpret when compared to other ML models, such as random forests.

However, methods such as Autoencoder (AE, **TODO** check if autoencoder is mentioned elsewhere) are DL strategies that can be used for model interpretation. AE is a family of unsupervised methods that aim to learn a new (encoded) representation and minimize the error between the new representation and the input data. Tan et al., [@doi:10.1142/9789814644730_0014] has uses denoising autoencoders to extract useful genomics features a from breast cancer dataset. Another example of  model interpretation is the Deep Bind [@doi:10.1038/nbt.3300], which predicts DNA and RNA binding sequences potentially involved in regulatory processes. Deep bind used mutation maps to understand the impact of genetic variants of these binding sites ... **TODO** 

Unfortunelly, the literature on interpretable DL methods is extremily limited. **TODO** what should we suggest here?