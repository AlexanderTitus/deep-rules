## Rule 10: Don't share models trained on sensitive data

One of the greatest opportunities for deep learning in biology is the ability for deep learning techniques to incorporate representation learning to extract information that can not readily be captured by traditional methods [@arxiv:1509.09292]. 
The abundance of features for each training example means that the representation learning of the deep learning models can capture information-rich abstractions of data during the training process. Therefore with deep learning, and even with some traditional machine learning models such as k-nearest neighbors which stores the full training data, it is important not to share models trained on sensitive data.
Applying deep learning to images of cats from the internet does not pose significant privacy problems, but in the field of human health, this starts to approach privacy concerns. 
Adversarial training techniques, such as model inversion attacks, can be used to exploit model predictions to recover recognizable images of people's faces used for training [@doi:10.1145/2810103.2813677]. 
These risks are higher in deep learning compared to traditional machine learning techniques because of the greater representational capacity of the models. 
This is achieved by the high volume of model weights, even in a relatively small project, that allow deep learning to model high-dimensional non-linear relationships among data. 
This enhanced modeling capacity allows the model to learn more robust and nuanced features of specific data, leading to the chance of revealing the underlying sensitive data. 
When you train deep learning models on sensitive data, be sure not to share the model weights directly, and use privacy preserving techniques [@arxiv:1811.04017] such as differential privacy [@doi:10.1145/2976749.2978318, @arxiv:1812.01484] and homomorphic encryption [@doi:10.1371/journal.pcbi.1006454, @arxiv:1811.00778] to protect sensitive data.

