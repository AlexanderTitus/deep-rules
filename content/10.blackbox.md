## Tip 8: Your DL models can be more transparent {#blackbox}

Model interpretability is a broad concept.
In certain cases, the goal behind interpretation is to understand the underlying data generating processes while in other cases the goal is to understand why a model made the prediction that it did for a specific example or set of examples.
In much of the ML literature, including in our guidelines,  the concept of model interpretability refers to the ability to identify the discriminative features that influence or sway the predictions.
ML models vary widely in terms of interpretability: some are fully transparent while others are considered to be "black-boxes" that make predictions with little ability to examine why.
Logistic regression and decision tree models are generally considered interpretable, while deep neural networks are often considered among the most difficult to interpret because they can have many parameters and non-linear relationships. 

Model interpretability is particularly important in biomedicine, where subsequent decision making often requires human input.
Many authors attribute a lower uptake of DL tools in healthcare to interpretability challenges [@doi:10.1109/JBHI.2016.2636665; @doi:10.1038/s41551-018-0315-x]. 
Strategies to interpret both ML and DL models are rapidly emerging, and the literature on the topic is growing at an exponential rate [@arxiv:2001.02522].
Therefore, instead of recommending specific methods for either DL-specific or general-purpose model interpretation, we suggest consulting [@url:https://christophm.github.io/interpretable-ml-book/] which is freely available and continually updated.

In conclusion, analyze your DL models to identify how their inputs are interacting as they generate their predictions, especially when your models are being used for critical decision making.
