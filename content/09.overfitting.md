## Tip 7: Address deep neural networks' increased tendency to overfit the dataset {#overfitting}

Overfitting is a challenge inherent to machine learning in general, and is one of the most significant challenges you'll face when applying deep learning specifically.
Overfitting occurs when a model fits patterns in the training data so closely that it is including non-generalizable noise or non-scientifically relevant perturbations in the relationships it is learning.
In other words, it is as if the model is memorizing patterns in the specific data set it is training on rather than learning general relationships that hold across similar datasets.
This subtle distinction is made clearer by seeing what happens when a model is tested on data to which it was not exposed during training: just as a student who memorizes exam materials struggles to correctly answer questions for which they have not studied, a machine learning model that has overfit to its training data will perform poorly on unseen test data.
Deep learning models are particularly susceptible to overfitting due to their relatively large number of parameters and associated representational capacity.
Just as a smarter student may have greater potential for memorization, deep learning models seem more prone to overfitting than traditional machine learning models.

![A visual example of overfitting and failure to generalize. While a high-degree polynomial achieves high accuracy on its training data, it performs poorly on data with specificities that have not been seen before. That is, the model has learned the training dataset specifically rather than learning a generalizable pattern that represents data of this type. In contrast, a simple linear regression works well on both datasets. The greater representational capacity of the polynomial is analogous to using a larger or deeper neural network.](images/overfitting.png){#fig:overfitting-fig}

In general, one of the most effective ways to combat overfitting is to detect it in the first place.
One way to do this is to split the main dataset being worked on into three parts: a training set, a tuning set (also commonly called a validation set in the machine learning literature), and a test set.
These three partitions allow us to optimize models by iterating between model learning on the training set and hyperparameter evaluation on the tuning set without affecting the final model assessment on the test set.
That is, the data used for testing should be "locked away" and used only once to evaluate the final model after all training and tuning steps are completed.
A researcher can then use the model's performance on the unseen test data as a measure of how overfit (i.e. nongeneralizable) the model is.
This type of approach is necessary for evaluating the generalizability of models without the biases that can arise from learning and testing on the same data [@arxiv:1811.12808; @doi:10.1162/089976698300017197].
While a slight drop in performance from the training set to the test set is normal, a significant drop is a clear sign of overfitting (see Figure @fig:overfitting-fig for a visual demonstration of an overfit model that performs poorly on test data).

If overfitting is an issue, there are a variety of techniques to reduce overfitting, including data augmentation and regularization techniques such as dropout [@url:http://jmlr.csail.mit.edu/papers/v15/srivastava14a.html] and weight decay [@tag:krogh-weight-decay].
Another way to reduce overfitting, as described by Chuang and Keiser, is to identify the baseline level of memorization that is occuring by training on data that has its labels randomly shuffled.
By comparing the model performance with the shuffled data to that achieved with the actual data [@doi:10.1021/acschembio.8b00881], a practitioner can identify overfitting as a model that performs no better on real data, as this suggest that any predictive capacity is not due to data-driven signal.
One important caveat when working with partitioned data is the need to apply transformation and normalization procedures equally to all datasets.
The parameters required for such procedures (for example, quantile normalization, a common standardization method when analyzing gene-expression data) should only be derived from the training data, and not from the tuning or test data.
Additionally, many conventional metrics for classification (e.g. area under the receiver operating characteristic curve or AUROC) have limited utility in cases of extreme class imbalance [@pmid:25738806].
Therefore, model performance should be evaluated with a carefully picked panel of relevant metrics that make minimal assumptions about the composition of the testing data [@doi:10.1021/acs.molpharmaceut.7b00578].

When working with biological and medical data, one must also carefully consider potential sources of bias and/or non-independence when defining training and test sets.
For example, a deep learning model for pneumonia detection in chest X-rays appeared to performed well within the hospitals providing the training data, but then failed to generalize to other hospitals [@doi:10.1371/journal.pmed.1002683].
This resulted from the deep learning model picking up on signal related to which hospital the images were from, and represents a type of artifact or "batch effect" that practitioners must be vigilant towards.
When dealing with sequence data, holding out test data that are evolutionarily related or that share structural homology to the training data can result in overfitting that is hard to detect due to the inherent relatedness of the partitioned data (cite?).
In such situations, simply holding out test data selected from a random partition of the training data can be insufficient.
Again, the best remedy for identifying confounding variables is to [know your data](#know-your-problem) and to test your model on truly independent data.

In essence, practitioners should split data into training, tuning, and single-use testing sets to assess the performance of the model on data that can provide a reliable estimate of its generalization performance.
Futhermore, be cognizant of the danger of skewed or biased data artificially inflating accuracy.
