## Tip 3: Understand the complexities of training deep neural networks {#complexities}

Correctly training deep neural networks is a non-trivial process.
There are many different options and potential pitfalls at every stage.
To get good results you have to expect to train many networks with
different tweaks and modifications.
Despite improved framework ease-of-use and on-demand cloud computing resources
this means DL can be very demanding, requiring significant
infrastructure and patience to complete.
The experimentation inherent to DL is often noisy (requiring repetition) and
represent a significant organizational challenge.
All code, parameters, and results must be carefully corralled using good
coding practices (e.g. version control, continuous integration etc.) in order to 
be effective and interpretable.
This organization is also key to being able to efficiently share your work
and to update your model as new data becomes available. 


Similar to [Tip 4](#baselines), try to start with a relatively smaller network
and increase the size and complexity as needed to prevent wasting time and 
resources. 
Beware of the seemingly trivial choices that are being made implicitly by 
default settings in your framework of choice e.g. choice of optimization algorithm
(adaptive methods are often faster but can lead to worse results [@arxiv:1705.08292]).
These need to be carefully considered and their impacts evaluated (see [Tip 6](#hyperparameters)).
