## Tip 9: Don't over-interpret predictions {#interpretation}

Once we have trained an accurate deep learning model, we often want to use it to deduce relationships and inform scientific findings.
However, in doing this, we need to be careful to correctly interpret the model's predictions.
Given that deep learning models can be difficult to interpret intuitively, there is often a temptation to anthropomorphize the models, and to overinterpret the predictions in indulgent and/or inaccurate ways.
As the classic statistical saying "correlation doesn't mean causation" implies, predictions by deep learning models don't necessarily speak to certain causual relationships.
While we generally know this, and understand that accurately predicting an outcome doesn't imply the learning of any causal mechanism, it can be easy to forget this lesson when the predictions are extremely accurate.
A poignant example of this lesson is from work where authors evaluated the capacities of several models to predict the probability of death for patients with pneumonia admitted to an intensive care unit [@tag:predicting-pneumonia-mortality; @doi:10.1145/2783258.2788613].
Unsurprisingly, the neural network model achieved the best predictive accuracy.
However, after fitting a rule-based model in order to better understand the relationships inherent to their data, the authors discovered that the hospital data implied the rule `HasAsthma(x) => LowerRisk(x)`.
This rule contradicts medical understanding, as having asthma doesn't make pneumonia better!
Nonetheless, this rule was supported by the data, as pneumonia patients with a history of asthma tended to receive more aggressive care.
The neural network had therefore also learned to make predictions according to this rule despite the fact that it has nothing to do with causality or mechanism.
Guiding treatment decisions according to the predictions of the neural network would have been disastrous, even though the neural network had high predictive accuracy.

To trust deep learning models, we must combine knowledge of the training data ([Tip 4](#know-your-problem)) with inspection of the model ([Tip 8](#blackbox)).
To move beyond fitting predictive models and towards the building of an understanding that can inform scientific deduction, we suggest working to disanetangle a model's internal logic by comparing data domains where your model succeeds to those in which they fail. And again, take care not to overinterpret or anthropomorphize your models.
