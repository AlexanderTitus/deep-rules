## Tip 10: Actively consider the ethical implications of your work {#privacy}

While deep learning continues to be a powerful, transformative tool within life sciences research—spanning basic biology and pre-clinical science to varied translational approaches and clinical studies—it is important to comment on some ethics—related considerations.
For instance, despite the fact that deep learning methods are helping to increase medical efficiency through improved diagnostic capability and risk assessment, certain biases may be inadvertently introduced into models related to patient age, race, and gender [@doi:10.1002/hast.977]; as previously mentioned, deep learning practitioners may make use of datasets not representative of diverse populations and patient characteristics [@doi:10.1377/hlthaff.2014.0048], thereby contributing to these problems (please refer to Tip 4).

Therefore, it is important to think thoroughly and cautiously about deep learning applications and its potential impact to persons and society—mindful of possible harms, injuries, injustices, and other types of wrongdoings.
At a minimum, practitioners must ensure that, wherever relevant, their life sciences projects are fully compliant with local research governance/approval policies, legal requirements, institutional review board (IRB) policies, and any other relevant bodies and their standards.
Moreover, we offer below three tangible, action-oriented recommendations to further empower and enrichen deep learning researchers.

First, similarly to how certain teams keep a project-specific or programming-related issue tracker detailing known bugs and other technical issues, practitioners should get into the habit of keeping an active _ethics register_.
Teams can institute what Rachel Thomas of the Center for Applied Data Ethics at the University of San Francisco terms “ethical risk sweeps.”
Teams focus on “periodically scheduling times to really go through what could go wrong and what are the ethical risks.”
After all, “ethics is thinking through what can go wrong before it does and having processes in place around what happens when there are mistakes or errors” [@https://venturebeat.com/2019/10/07/how-to-operationalize-ai-ethics/].
Second, to help foster a conscious ethics-oriented mindset, researchers should consider expanding journal clubs to include scholarly and popular articles detailing real-world ethics issues relevant to in their scientific fields.
This will help researchers to think more holistically and judiciously (that is, with good judgment and sense) about their work and its implications.
Third, we encourage individual- and team-level participation in professional societies and other types of organizations and initiatives related to AI/deep learning and data ethics [@https://ocean.sagepub.com/blog/10-organizations-leading-the-way-in-ethical-ai; @https://www.aies-conference.com/2021/; @http://ethics-artificial-intelligence-conference.mozello.com].
This will encourage a sense of community and intellectual engagement, keeping practitioners abreast of cutting-edge insights and emerging professional standards.

Furthermore, practitioners may encounter datasets that cannot be shared, such as ones for which there would be significant ethical or legal issues associated with their release [@doi:10.1371/journal.pcbi.1005399].
Examples of such data include classified or confidential data, biological data related to trade secrets, medical records, or other personally identifiable information [@doi:10.1038/s41576-020-0257-5].
While deep learning models can capture information-rich abstractions of multiple features of the data during the training process (which represents one of its great strengths), these features may be more prone to leak the data that they were trained over if the model is shared or allowed to be queried with arbitrary inputs [@doi:10.1145/2810103.2813677; @arxiv:1610.05820].
In other words, the complex relationships learned about the input data can potentially be used to infer characteristics about the original dataset.
This means that the strengths that imbue deep learnings with its great predictive capacity also raise the level of risk surrounding data privacy.
Therefore, while there is tremendous promise for deep learning techniques to extract information that cannot readily be captured by traditional methods [@arxiv:1509.09292], it is imperative not to share models trained on sensitive data.
This also holds true for certain traditional machine learning methods that learn by capturing specific details of the full training data (for example, _k_-nearest neighbors models).

Techniques to train deep neural networks without sharing unencrypted access to data are being advanced through implementations of homomorphic encryption, which serves to enable equivalent prediction on data that is encrypted end to end [@doi:10.1371/journal.pcbi.1006454; @arxiv:1811.00778].
Privacy-preserving techniques [@arxiv:1811.04017], such as differential privacy [@doi:10.1145/2976749.2978318; @doi:10.1101/159756; @arxiv:1812.01484], can help to mitigate risks as long as the assumptions underlying these techniques are met.
These methods provide a path towards a future where trained models and their predictions can be shared, but more software development and theoretical advances will be required to make these techniques easy to apply correctly in many settings.
Unless you use these techniques, don't share the weights or arbitrary access to the predictions of models trained on sensitive data.
