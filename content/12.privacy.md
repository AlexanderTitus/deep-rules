## Tip 10: Don't share models trained on sensitive data {#privacy}

Practitioners may encounter datasets that cannot be shared, such as ones for which there would be significant ethical or legal issues associated with release [@doi:10.1371/journal.pcbi.1005399].
Examples of such data include classified or confidential data, biological data related to trade secrets (_e.g._ genomic assay data), medical records, and personally identifiable information [@doi:10.1038/s41576-020-0257-5].
While deep learning models can capture information-rich abstractions of multiple features of the data during the training process (which represents one of its great strengths), these features may be more prone to leak the data that they were trained over if the model is shared or allowed to be queried with arbitrary inputs [@doi:10.1145/2810103.2813677; @arxiv:1610.05820].
In other words, the complex relationships learned about the input data can potentially be used to infer characterists about the original dataset.
This means that the strengths that imbue deep learnings with its great predictive capacity also raise the level of risk surrounding data privacy.
Therefore, while there is tremendous promise for deep learning techniques to extract information that cannot readily be captured by traditional methods [@arxiv:1509.09292], it is imperative not to share models trained on sensitive data.
This also holds true for certain traditional machine learning methods that learn by capturing specific details of the full training data (for example, _k_-nearest neighbors models). 

Techniques to train deep neural networks without sharing unencrypted access to data are being advanced through implementations of homomorphic encryption, which serves to enable equivalent prediction on data that is encrypted end to end [@doi:10.1371/journal.pcbi.1006454; @arxiv:1811.00778].
Privacy preserving techniques [@arxiv:1811.04017], such as differential privacy [@doi:10.1145/2976749.2978318; @doi:10.1101/159756; @arxiv:1812.01484], can help to mitigate risks as long as the assumptions underlying these techniques are met.
These methods provide a path towards a future where trained models and their predictions can be shared, but more software development and theoretical advances will be required to make these techniques easy to apply correctly in many settings.
Until then, don't share models trained on sensitive data.
