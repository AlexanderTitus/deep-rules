[
  {
    "number": "1312.6114v10",
    "version": "10",
    "URL": "https://arxiv.org/abs/1312.6114v10",
    "title": "Auto-Encoding Variational Bayes",
    "issued": {
      "date-parts": [
        [
          2013,
          12,
          20
        ]
      ]
    },
    "author": [
      {
        "literal": "Diederik P Kingma"
      },
      {
        "literal": "Max Welling"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.",
    "type": "report",
    "id": "NLVTJ9Lj"
  },
  {
    "number": "1509.09292v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1509.09292v2",
    "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints",
    "issued": {
      "date-parts": [
        [
          2015,
          9,
          30
        ]
      ]
    },
    "author": [
      {
        "literal": "David Duvenaud"
      },
      {
        "literal": "Dougal Maclaurin"
      },
      {
        "literal": "Jorge Aguilera-Iparraguirre"
      },
      {
        "literal": "Rafael Gómez-Bombarelli"
      },
      {
        "literal": "Timothy Hirzel"
      },
      {
        "literal": "Alán Aspuru-Guzik"
      },
      {
        "literal": "Ryan P. Adams"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "We introduce a convolutional neural network that operates directly on graphs.\nThese networks allow end-to-end learning of prediction pipelines whose inputs\nare graphs of arbitrary size and shape. The architecture we present generalizes\nstandard molecular feature extraction methods based on circular fingerprints.\nWe show that these data-driven features are more interpretable, and have better\npredictive performance on a variety of tasks.",
    "type": "report",
    "id": "UeE0s74F"
  },
  {
    "number": "1803.01271v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1803.01271v2",
    "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks\n  for Sequence Modeling",
    "issued": {
      "date-parts": [
        [
          2018,
          3,
          4
        ]
      ]
    },
    "author": [
      {
        "literal": "Shaojie Bai"
      },
      {
        "literal": "J. Zico Kolter"
      },
      {
        "literal": "Vladlen Koltun"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "For most deep learning practitioners, sequence modeling is synonymous with\nrecurrent networks. Yet recent results indicate that convolutional\narchitectures can outperform recurrent networks on tasks such as audio\nsynthesis and machine translation. Given a new sequence modeling task or\ndataset, which architecture should one use? We conduct a systematic evaluation\nof generic convolutional and recurrent architectures for sequence modeling. The\nmodels are evaluated across a broad range of standard tasks that are commonly\nused to benchmark recurrent networks. Our results indicate that a simple\nconvolutional architecture outperforms canonical recurrent networks such as\nLSTMs across a diverse range of tasks and datasets, while demonstrating longer\neffective memory. We conclude that the common association between sequence\nmodeling and recurrent networks should be reconsidered, and convolutional\nnetworks should be regarded as a natural starting point for sequence modeling\ntasks. To assist related work, we have made code available at\nhttp://github.com/locuslab/TCN .",
    "type": "report",
    "id": "aqgi0yxG"
  },
  {
    "number": "1803.04765v1",
    "version": "1",
    "URL": "https://arxiv.org/abs/1803.04765v1",
    "title": "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust\n  Deep Learning",
    "issued": {
      "date-parts": [
        [
          2018,
          3,
          13
        ]
      ]
    },
    "author": [
      {
        "literal": "Nicolas Papernot"
      },
      {
        "literal": "Patrick McDaniel"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Deep neural networks (DNNs) enable innovative applications of machine\nlearning like image recognition, machine translation, or malware detection.\nHowever, deep learning is often criticized for its lack of robustness in\nadversarial settings (e.g., vulnerability to adversarial inputs) and general\ninability to rationalize its predictions. In this work, we exploit the\nstructure of deep learning to enable new learning-based inference and decision\nstrategies that achieve desirable properties such as robustness and\ninterpretability. We take a first step in this direction and introduce the Deep\nk-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest\nneighbors algorithm with representations of the data learned by each layer of\nthe DNN: a test input is compared to its neighboring training points according\nto the distance that separates them in the representations. We show the labels\nof these neighboring points afford confidence estimates for inputs outside the\nmodel's training manifold, including on malicious inputs like adversarial\nexamples--and therein provides protections against inputs that are outside the\nmodels understanding. This is because the nearest neighbors can be used to\nestimate the nonconformity of, i.e., the lack of support for, a prediction in\nthe training data. The neighbors also constitute human-interpretable\nexplanations of predictions. We evaluate the DkNN algorithm on several\ndatasets, and show the confidence estimates accurately identify inputs outside\nthe model, and that the explanations provided by nearest neighbors are\nintuitive and useful in understanding model failures.",
    "type": "report",
    "id": "uBcf6TJ2"
  },
  {
    "number": "1805.11783v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1805.11783v2",
    "title": "To Trust Or Not To Trust A Classifier",
    "issued": {
      "date-parts": [
        [
          2018,
          5,
          30
        ]
      ]
    },
    "author": [
      {
        "literal": "Heinrich Jiang"
      },
      {
        "literal": "Been Kim"
      },
      {
        "literal": "Melody Y. Guan"
      },
      {
        "literal": "Maya Gupta"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Knowing when a classifier's prediction can be trusted is useful in many\napplications and critical for safely using AI. While the bulk of the effort in\nmachine learning research has been towards improving classifier performance,\nunderstanding when a classifier's predictions should and should not be trusted\nhas received far less attention. The standard approach is to use the\nclassifier's discriminant or confidence score; however, we show there exists an\nalternative that is more effective in many situations. We propose a new score,\ncalled the trust score, which measures the agreement between the classifier and\na modified nearest-neighbor classifier on the testing example. We show\nempirically that high (low) trust scores produce surprisingly high precision at\nidentifying correctly (incorrectly) classified examples, consistently\noutperforming the classifier's confidence score as well as many other\nbaselines. Further, under some mild distributional assumptions, we show that if\nthe trust score for an example is high (low), the classifier will likely agree\n(disagree) with the Bayes-optimal classifier. Our guarantees consist of\nnon-asymptotic rates of statistical consistency under various nonparametric\nsettings and build on recent developments in topological data analysis.",
    "type": "report",
    "id": "2bsGpiQt"
  },
  {
    "number": "1807.00431v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1807.00431v2",
    "title": "Confounding variables can degrade generalization performance of\n  radiological deep learning models",
    "issued": {
      "date-parts": [
        [
          2018,
          7,
          2
        ]
      ]
    },
    "author": [
      {
        "literal": "John R. Zech"
      },
      {
        "literal": "Marcus A. Badgeley"
      },
      {
        "literal": "Manway Liu"
      },
      {
        "literal": "Anthony B. Costa"
      },
      {
        "literal": "Joseph J. Titano"
      },
      {
        "literal": "Eric K. Oermann"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Early results in using convolutional neural networks (CNNs) on x-rays to\ndiagnose disease have been promising, but it has not yet been shown that models\ntrained on x-rays from one hospital or one group of hospitals will work equally\nwell at different hospitals. Before these tools are used for computer-aided\ndiagnosis in real-world clinical settings, we must verify their ability to\ngeneralize across a variety of hospital systems. A cross-sectional design was\nused to train and evaluate pneumonia screening CNNs on 158,323 chest x-rays\nfrom NIH (n=112,120 from 30,805 patients), Mount Sinai (42,396 from 12,904\npatients), and Indiana (n=3,807 from 3,683 patients). In 3 / 5 natural\ncomparisons, performance on chest x-rays from outside hospitals was\nsignificantly lower than on held-out x-rays from the original hospital systems.\nCNNs were able to detect where an x-ray was acquired (hospital system, hospital\ndepartment) with extremely high accuracy and calibrate predictions accordingly.\nThe performance of CNNs in diagnosing diseases on x-rays may reflect not only\ntheir ability to identify disease-specific imaging findings on x-rays, but also\ntheir ability to exploit confounding information. Estimates of CNN performance\nbased on test data from hospital systems used for model training may overstate\ntheir likely real-world performance.",
    "DOI": "10.1371/journal.pmed.1002683",
    "type": "report",
    "id": "FEPLn1Uo"
  },
  {
    "number": "1810.08055v1",
    "version": "1",
    "URL": "https://arxiv.org/abs/1810.08055v1",
    "title": "Ten Simple Rules for Reproducible Research in Jupyter Notebooks",
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          13
        ]
      ]
    },
    "author": [
      {
        "literal": "Adam Rule"
      },
      {
        "literal": "Amanda Birmingham"
      },
      {
        "literal": "Cristal Zuniga"
      },
      {
        "literal": "Ilkay Altintas"
      },
      {
        "literal": "Shih-Cheng Huang"
      },
      {
        "literal": "Rob Knight"
      },
      {
        "literal": "Niema Moshiri"
      },
      {
        "literal": "Mai H. Nguyen"
      },
      {
        "literal": "Sara Brin Rosenthal"
      },
      {
        "literal": "Fernando Pérez"
      },
      {
        "literal": "Peter W. Rose"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Reproducibility of computational studies is a hallmark of scientific\nmethodology. It enables researchers to build with confidence on the methods and\nfindings of others, reuse and extend computational pipelines, and thereby drive\nscientific progress. Since many experimental studies rely on computational\nanalyses, biologists need guidance on how to set up and document reproducible\ndata analyses or simulations.\n  In this paper, we address several questions about reproducibility. For\nexample, what are the technical and non-technical barriers to reproducible\ncomputational studies? What opportunities and challenges do computational\nnotebooks offer to overcome some of these barriers? What tools are available\nand how can they be used effectively?\n  We have developed a set of rules to serve as a guide to scientists with a\nspecific focus on computational notebook systems, such as Jupyter Notebooks,\nwhich have become a tool of choice for many applications. Notebooks combine\ndetailed workflows with narrative text and visualization of results. Combined\nwith software repositories and open source licensing, notebooks are powerful\ntools for transparent, collaborative, reproducible, and reusable data analyses.",
    "type": "report",
    "id": "Tx4vUlOa"
  },
  {
    "number": "1811.00778v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1811.00778v2",
    "title": "The AlexNet Moment for Homomorphic Encryption: HCNN, the First\n  Homomorphic CNN on Encrypted Data with GPUs",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          2
        ]
      ]
    },
    "author": [
      {
        "literal": "Ahmad Al Badawi"
      },
      {
        "literal": "Jin Chao"
      },
      {
        "literal": "Jie Lin"
      },
      {
        "literal": "Chan Fook Mun"
      },
      {
        "literal": "Jun Jie Sim"
      },
      {
        "literal": "Benjamin Hong Meng Tan"
      },
      {
        "literal": "Xiao Nan"
      },
      {
        "literal": "Khin Mi Mi Aung"
      },
      {
        "literal": "Vijay Ramaseshan Chandrasekhar"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Fully homomorphic encryption, with its widely-known feature of computing on\nencrypted data, empowers a wide range of privacy-concerned cloud applications\nincluding deep learning as a service. This comes at a high cost since FHE\nincludes highly-intensive computation that requires enormous computing power.\nAlthough the literature includes a number of proposals to run CNNs on encrypted\ndata, the performance is still far from satisfactory. In this paper, we push\nthe level up and show how to accelerate the performance of running CNNs on\nencrypted data using GPUs. We evaluated a CNN to classify homomorphically the\nMNIST dataset into 10 classes. We used a number of techniques such as\nlow-precision training, unified training and testing network, optimized FHE\nparameters and a very efficient GPU implementation to achieve high performance.\nOur solution achieved high security level (> 128 bit) and high accuracy (99%).\nIn terms of performance, our best results show that we could classify the\nentire testing dataset in 14.105 seconds, with per-image amortized time (1.411\nmilliseconds) 40.41x faster than prior art.",
    "type": "report",
    "id": "3326vtLW"
  },
  {
    "number": "1811.04017v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1811.04017v2",
    "title": "A generic framework for privacy preserving deep learning",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          9
        ]
      ]
    },
    "author": [
      {
        "literal": "Theo Ryffel"
      },
      {
        "literal": "Andrew Trask"
      },
      {
        "literal": "Morten Dahl"
      },
      {
        "literal": "Bobby Wagner"
      },
      {
        "literal": "Jason Mancuso"
      },
      {
        "literal": "Daniel Rueckert"
      },
      {
        "literal": "Jonathan Passerat-Palmbach"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "We detail a new framework for privacy preserving deep learning and discuss\nits assets. The framework puts a premium on ownership and secure processing of\ndata and introduces a valuable representation based on chains of commands and\ntensors. This abstraction allows one to implement complex privacy preserving\nconstructs such as Federated Learning, Secure Multiparty Computation, and\nDifferential Privacy while still exposing a familiar deep learning API to the\nend-user. We report early results on the Boston Housing and Pima Indian\nDiabetes datasets. While the privacy features apart from Differential Privacy\ndo not impact the prediction accuracy, the current implementation of the\nframework introduces a significant overhead in performance, which will be\naddressed at a later stage of the development. We believe this work is an\nimportant milestone introducing the first reliable, general framework for\nprivacy preserving deep learning.",
    "type": "report",
    "id": "1HuQe3Z8X"
  },
  {
    "number": "1811.12808v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1811.12808v2",
    "title": "Model Evaluation, Model Selection, and Algorithm Selection in Machine\n  Learning",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          13
        ]
      ]
    },
    "author": [
      {
        "literal": "Sebastian Raschka"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "The correct use of model evaluation, model selection, and algorithm selection\ntechniques is vital in academic machine learning research as well as in many\nindustrial settings. This article reviews different techniques that can be used\nfor each of these three subtasks and discusses the main advantages and\ndisadvantages of each technique with references to theoretical and empirical\nstudies. Further, recommendations are given to encourage best yet feasible\npractices in research and applications of machine learning. Common methods such\nas the holdout method for model evaluation and selection are covered, which are\nnot recommended when working with small datasets. Different flavors of the\nbootstrap technique are introduced for estimating the uncertainty of\nperformance estimates, as an alternative to confidence intervals via normal\napproximation if bootstrapping is computationally feasible. Common\ncross-validation techniques such as leave-one-out cross-validation and k-fold\ncross-validation are reviewed, the bias-variance trade-off for choosing k is\ndiscussed, and practical tips for the optimal choice of k are given based on\nempirical evidence. Different statistical tests for algorithm comparisons are\npresented, and strategies for dealing with multiple comparisons such as omnibus\ntests and multiple-comparison corrections are discussed. Finally, alternative\nmethods for algorithm selection, such as the combined F-test 5x2\ncross-validation and nested cross-validation, are recommended for comparing\nmachine learning algorithms when datasets are small.",
    "type": "report",
    "id": "1CDx6NYSj"
  },
  {
    "number": "1812.01484v1",
    "version": "1",
    "URL": "https://arxiv.org/abs/1812.01484v1",
    "title": "Privacy-Preserving Distributed Deep Learning for Clinical Data",
    "issued": {
      "date-parts": [
        [
          2018,
          12,
          4
        ]
      ]
    },
    "author": [
      {
        "literal": "Brett K. Beaulieu-Jones"
      },
      {
        "literal": "William Yuan"
      },
      {
        "literal": "Samuel G. Finlayson"
      },
      {
        "literal": "Zhiwei Steven Wu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Deep learning with medical data often requires larger samples sizes than are\navailable at single providers. While data sharing among institutions is\ndesirable to train more accurate and sophisticated models, it can lead to\nsevere privacy concerns due the sensitive nature of the data. This problem has\nmotivated a number of studies on distributed training of neural networks that\ndo not require direct sharing of the training data. However, simple distributed\ntraining does not offer provable privacy guarantees to satisfy technical safe\nstandards and may reveal information about the underlying patients. We present\na method to train neural networks for clinical data in a distributed fashion\nunder differential privacy. We demonstrate these methods on two datasets that\ninclude information from multiple independent sites, the eICU collaborative\nResearch Database and The Cancer Genome Atlas.",
    "type": "report",
    "id": "eJgWbXRz"
  },
  {
    "publisher": "Springer Berlin Heidelberg",
    "DOI": "10.1007/978-3-642-35289-8",
    "type": "book",
    "source": "Crossref",
    "title": "Neural Networks: Tricks of the Trade",
    "container-title": "Lecture Notes in Computer Science",
    "editor": [
      {
        "given": "Grégoire",
        "family": "Montavon"
      },
      {
        "given": "Geneviève B.",
        "family": "Orr"
      },
      {
        "given": "Klaus-Robert",
        "family": "Müller"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "URL": "https://doi.org/gfvtvt",
    "id": "JT3rHKc7"
  },
  {
    "publisher": "Springer Nature",
    "issue": "1",
    "DOI": "10.1007/s10278-016-9914-9",
    "type": "article-journal",
    "page": "95-101",
    "source": "Crossref",
    "title": "High-Throughput Classification of Radiographs Using Deep Convolutional Neural Networks",
    "volume": "30",
    "author": [
      {
        "given": "Alvin",
        "family": "Rajkomar"
      },
      {
        "given": "Sneha",
        "family": "Lingam"
      },
      {
        "given": "Andrew G.",
        "family": "Taylor"
      },
      {
        "given": "Michael",
        "family": "Blum"
      },
      {
        "given": "John",
        "family": "Mongan"
      }
    ],
    "container-title": "Journal of Digital Imaging",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2016,
          10,
          11
        ]
      ]
    },
    "URL": "https://doi.org/gcgk7v",
    "container-title-short": "J Digit Imaging",
    "PMCID": "PMC5267603",
    "PMID": "27730417",
    "id": "x6HXFAS4"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "3",
    "DOI": "10.1007/s11263-015-0816-y",
    "type": "article-journal",
    "page": "211-252",
    "source": "Crossref",
    "title": "ImageNet Large Scale Visual Recognition Challenge",
    "volume": "115",
    "author": [
      {
        "given": "Olga",
        "family": "Russakovsky"
      },
      {
        "given": "Jia",
        "family": "Deng"
      },
      {
        "given": "Hao",
        "family": "Su"
      },
      {
        "given": "Jonathan",
        "family": "Krause"
      },
      {
        "given": "Sanjeev",
        "family": "Satheesh"
      },
      {
        "given": "Sean",
        "family": "Ma"
      },
      {
        "given": "Zhiheng",
        "family": "Huang"
      },
      {
        "given": "Andrej",
        "family": "Karpathy"
      },
      {
        "given": "Aditya",
        "family": "Khosla"
      },
      {
        "given": "Michael",
        "family": "Bernstein"
      },
      {
        "given": "Alexander C.",
        "family": "Berg"
      },
      {
        "given": "Li",
        "family": "Fei-Fei"
      }
    ],
    "container-title": "International Journal of Computer Vision",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2015,
          4,
          11
        ]
      ]
    },
    "URL": "https://doi.org/gcgk7w",
    "container-title-short": "Int J Comput Vis",
    "id": "cBVeXnZx"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "2",
    "DOI": "10.1016/0893-6080(91)90009-t",
    "type": "article-journal",
    "page": "251-257",
    "source": "Crossref",
    "title": "Approximation capabilities of multilayer feedforward networks",
    "volume": "4",
    "author": [
      {
        "given": "Kurt",
        "family": "Hornik"
      }
    ],
    "container-title": "Neural Networks",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "URL": "https://doi.org/dzwxkd",
    "container-title-short": "Neural Networks",
    "id": "1BnILgle7"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "2",
    "DOI": "10.1016/s0933-3657(96)00367-3",
    "type": "article-journal",
    "page": "107-138",
    "source": "Crossref",
    "title": "An evaluation of machine-learning methods for predicting pneumonia mortality",
    "volume": "9",
    "author": [
      {
        "given": "Gregory F.",
        "family": "Cooper"
      },
      {
        "given": "Constantin F.",
        "family": "Aliferis"
      },
      {
        "given": "Richard",
        "family": "Ambrosino"
      },
      {
        "given": "John",
        "family": "Aronis"
      },
      {
        "given": "Bruce G.",
        "family": "Buchanan"
      },
      {
        "given": "Richard",
        "family": "Caruana"
      },
      {
        "given": "Michael J.",
        "family": "Fine"
      },
      {
        "given": "Clark",
        "family": "Glymour"
      },
      {
        "given": "Geoffrey",
        "family": "Gordon"
      },
      {
        "given": "Barbara H.",
        "family": "Hanusa"
      },
      {
        "given": "Janine E.",
        "family": "Janosky"
      },
      {
        "given": "Christopher",
        "family": "Meek"
      },
      {
        "given": "Tom",
        "family": "Mitchell"
      },
      {
        "given": "Thomas",
        "family": "Richardson"
      },
      {
        "given": "Peter",
        "family": "Spirtes"
      }
    ],
    "container-title": "Artificial Intelligence in Medicine",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1997,
          2
        ]
      ]
    },
    "URL": "https://doi.org/b6vnmd",
    "container-title-short": "Artificial Intelligence in Medicine",
    "id": "980FAm5x"
  },
  {
    "publisher": "American Chemical Society (ACS)",
    "issue": "12",
    "DOI": "10.1021/acs.molpharmaceut.7b00578",
    "type": "article-journal",
    "page": "4462-4475",
    "source": "Crossref",
    "title": "Comparison of Deep Learning With Multiple Machine Learning Methods and Metrics Using Diverse Drug Discovery Data Sets",
    "volume": "14",
    "author": [
      {
        "given": "Alexandru",
        "family": "Korotcov"
      },
      {
        "given": "Valery",
        "family": "Tkachenko"
      },
      {
        "given": "Daniel P.",
        "family": "Russo"
      },
      {
        "given": "Sean",
        "family": "Ekins"
      }
    ],
    "container-title": "Molecular Pharmaceutics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2017,
          11,
          13
        ]
      ]
    },
    "URL": "https://doi.org/gcj4p2",
    "container-title-short": "Mol. Pharmaceutics",
    "PMCID": "PMC5741413",
    "PMID": "29096442",
    "id": "rKXyJKNt"
  },
  {
    "publisher": "American Chemical Society (ACS)",
    "issue": "10",
    "DOI": "10.1021/acschembio.8b00881",
    "type": "article-journal",
    "page": "2819-2821",
    "source": "Crossref",
    "title": "Adversarial Controls for Scientific Machine Learning",
    "volume": "13",
    "author": [
      {
        "given": "Kangway V.",
        "family": "Chuang"
      },
      {
        "given": "Michael J.",
        "family": "Keiser"
      }
    ],
    "container-title": "ACS Chemical Biology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          19
        ]
      ]
    },
    "URL": "https://doi.org/gfk9mh",
    "container-title-short": "ACS Chem. Biol.",
    "PMID": "30336670",
    "id": "yqAEYaMg"
  },
  {
    "publisher": "Springer Nature",
    "issue": "1",
    "DOI": "10.1023/a:1010933404324",
    "type": "article-journal",
    "page": "5-32",
    "source": "Crossref",
    "volume": "45",
    "author": [
      {
        "given": "Leo",
        "family": "Breiman"
      }
    ],
    "container-title": "Machine Learning",
    "issued": {
      "date-parts": [
        [
          2001
        ]
      ]
    },
    "URL": "https://doi.org/d8zjwq",
    "id": "Uy4oESDl"
  },
  {
    "publisher": "Springer Nature",
    "issue": "4",
    "DOI": "10.1038/ng1201-365",
    "type": "article-journal",
    "page": "365-371",
    "source": "Crossref",
    "title": "Minimum information about a microarray experiment (MIAME)—toward standards for microarray data",
    "volume": "29",
    "author": [
      {
        "given": "Alvis",
        "family": "Brazma"
      },
      {
        "given": "Pascal",
        "family": "Hingamp"
      },
      {
        "given": "John",
        "family": "Quackenbush"
      },
      {
        "given": "Gavin",
        "family": "Sherlock"
      },
      {
        "given": "Paul",
        "family": "Spellman"
      },
      {
        "given": "Chris",
        "family": "Stoeckert"
      },
      {
        "given": "John",
        "family": "Aach"
      },
      {
        "given": "Wilhelm",
        "family": "Ansorge"
      },
      {
        "given": "Catherine A.",
        "family": "Ball"
      },
      {
        "given": "Helen C.",
        "family": "Causton"
      },
      {
        "given": "Terry",
        "family": "Gaasterland"
      },
      {
        "given": "Patrick",
        "family": "Glenisson"
      },
      {
        "given": "Frank C.P.",
        "family": "Holstege"
      },
      {
        "given": "Irene F.",
        "family": "Kim"
      },
      {
        "given": "Victor",
        "family": "Markowitz"
      },
      {
        "given": "John C.",
        "family": "Matese"
      },
      {
        "given": "Helen",
        "family": "Parkinson"
      },
      {
        "given": "Alan",
        "family": "Robinson"
      },
      {
        "given": "Ugis",
        "family": "Sarkans"
      },
      {
        "given": "Steffen",
        "family": "Schulze-Kremer"
      },
      {
        "given": "Jason",
        "family": "Stewart"
      },
      {
        "given": "Ronald",
        "family": "Taylor"
      },
      {
        "given": "Jaak",
        "family": "Vilo"
      },
      {
        "given": "Martin",
        "family": "Vingron"
      }
    ],
    "container-title": "Nature Genetics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2001,
          12
        ]
      ]
    },
    "URL": "https://doi.org/ck257n",
    "container-title-short": "Nat Genet",
    "PMID": "11726920",
    "id": "YuxbleXb"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "10",
    "DOI": "10.1038/nrg2825",
    "type": "article-journal",
    "page": "733-739",
    "source": "Crossref",
    "title": "Tackling the widespread and critical impact of batch effects in high-throughput data",
    "volume": "11",
    "author": [
      {
        "given": "Jeffrey T.",
        "family": "Leek"
      },
      {
        "given": "Robert B.",
        "family": "Scharpf"
      },
      {
        "given": "Héctor Corrada",
        "family": "Bravo"
      },
      {
        "given": "David",
        "family": "Simcha"
      },
      {
        "given": "Benjamin",
        "family": "Langmead"
      },
      {
        "given": "W. Evan",
        "family": "Johnson"
      },
      {
        "given": "Donald",
        "family": "Geman"
      },
      {
        "given": "Keith",
        "family": "Baggerly"
      },
      {
        "given": "Rafael A.",
        "family": "Irizarry"
      }
    ],
    "container-title": "Nature Reviews Genetics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2010,
          9,
          14
        ]
      ]
    },
    "URL": "https://doi.org/cfr324",
    "container-title-short": "Nat Rev Genet",
    "PMCID": "PMC3880143",
    "PMID": "20838408",
    "id": "mPnIAH38"
  },
  {
    "publisher": "Springer Nature",
    "issue": "1",
    "DOI": "10.1038/s41467-017-02388-1",
    "type": "article-journal",
    "source": "Crossref",
    "title": "VAMPnets for deep learning of molecular kinetics",
    "volume": "9",
    "author": [
      {
        "given": "Andreas",
        "family": "Mardt"
      },
      {
        "given": "Luca",
        "family": "Pasquali"
      },
      {
        "given": "Hao",
        "family": "Wu"
      },
      {
        "given": "Frank",
        "family": "Noé"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          1,
          2
        ]
      ]
    },
    "URL": "https://doi.org/gcvf62",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC5750224",
    "PMID": "29295994",
    "id": "lwg6sPLT"
  },
  {
    "publisher": "Springer Nature",
    "issue": "1",
    "DOI": "10.1038/s41467-018-05378-z",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Deep learning to predict the lab-of-origin of engineered DNA",
    "volume": "9",
    "author": [
      {
        "given": "Alec A. K.",
        "family": "Nielsen"
      },
      {
        "given": "Christopher A.",
        "family": "Voigt"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          8,
          7
        ]
      ]
    },
    "URL": "https://doi.org/gd27sw",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC6081423",
    "PMID": "30087331",
    "id": "WGfstNkj"
  },
  {
    "publisher": "Springer Nature",
    "issue": "10",
    "DOI": "10.1038/s41551-018-0315-x",
    "type": "article-journal",
    "page": "709-710",
    "source": "Crossref",
    "title": "Towards trustable machine learning",
    "volume": "2",
    "container-title": "Nature Biomedical Engineering",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          10
        ]
      ]
    },
    "URL": "https://doi.org/gfw9cn",
    "container-title-short": "Nat Biomed Eng",
    "PMID": "31015650",
    "id": "GdO9NZJH"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "DOI": "10.1038/s41746-018-0029-1",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Scalable and accurate deep learning with electronic health records",
    "volume": "1",
    "author": [
      {
        "given": "Alvin",
        "family": "Rajkomar"
      },
      {
        "given": "Eyal",
        "family": "Oren"
      },
      {
        "given": "Kai",
        "family": "Chen"
      },
      {
        "given": "Andrew M.",
        "family": "Dai"
      },
      {
        "given": "Nissan",
        "family": "Hajaj"
      },
      {
        "given": "Michaela",
        "family": "Hardt"
      },
      {
        "given": "Peter J.",
        "family": "Liu"
      },
      {
        "given": "Xiaobing",
        "family": "Liu"
      },
      {
        "given": "Jake",
        "family": "Marcus"
      },
      {
        "given": "Mimi",
        "family": "Sun"
      },
      {
        "given": "Patrik",
        "family": "Sundberg"
      },
      {
        "given": "Hector",
        "family": "Yee"
      },
      {
        "given": "Kun",
        "family": "Zhang"
      },
      {
        "given": "Yi",
        "family": "Zhang"
      },
      {
        "given": "Gerardo",
        "family": "Flores"
      },
      {
        "given": "Gavin E.",
        "family": "Duggan"
      },
      {
        "given": "Jamie",
        "family": "Irvine"
      },
      {
        "given": "Quoc",
        "family": "Le"
      },
      {
        "given": "Kurt",
        "family": "Litsch"
      },
      {
        "given": "Alexander",
        "family": "Mossin"
      },
      {
        "given": "Justin",
        "family": "Tansuwan"
      },
      {
        "given": "De",
        "family": "Wang"
      },
      {
        "given": "James",
        "family": "Wexler"
      },
      {
        "given": "Jimbo",
        "family": "Wilson"
      },
      {
        "given": "Dana",
        "family": "Ludwig"
      },
      {
        "given": "Samuel L.",
        "family": "Volchenboum"
      },
      {
        "given": "Katherine",
        "family": "Chou"
      },
      {
        "given": "Michael",
        "family": "Pearson"
      },
      {
        "given": "Srinivasan",
        "family": "Madabushi"
      },
      {
        "given": "Nigam H.",
        "family": "Shah"
      },
      {
        "given": "Atul J.",
        "family": "Butte"
      },
      {
        "given": "Michael D.",
        "family": "Howell"
      },
      {
        "given": "Claire",
        "family": "Cui"
      },
      {
        "given": "Greg S.",
        "family": "Corrado"
      },
      {
        "given": "Jeffrey",
        "family": "Dean"
      }
    ],
    "container-title": "npj Digital Medicine",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          5,
          8
        ]
      ]
    },
    "URL": "https://doi.org/gdqcc8",
    "container-title-short": "npj Digital Med",
    "id": "1DssZebFm"
  },
  {
    "publisher": "The Royal Society",
    "issue": "141",
    "DOI": "10.1098/rsif.2017.0387",
    "type": "article-journal",
    "page": "20170387",
    "source": "Crossref",
    "title": "Opportunities and obstacles for deep learning in biology and medicine",
    "volume": "15",
    "author": [
      {
        "given": "Travers",
        "family": "Ching"
      },
      {
        "given": "Daniel S.",
        "family": "Himmelstein"
      },
      {
        "given": "Brett K.",
        "family": "Beaulieu-Jones"
      },
      {
        "given": "Alexandr A.",
        "family": "Kalinin"
      },
      {
        "given": "Brian T.",
        "family": "Do"
      },
      {
        "given": "Gregory P.",
        "family": "Way"
      },
      {
        "given": "Enrico",
        "family": "Ferrero"
      },
      {
        "given": "Paul-Michael",
        "family": "Agapow"
      },
      {
        "given": "Michael",
        "family": "Zietz"
      },
      {
        "given": "Michael M.",
        "family": "Hoffman"
      },
      {
        "given": "Wei",
        "family": "Xie"
      },
      {
        "given": "Gail L.",
        "family": "Rosen"
      },
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Johnny",
        "family": "Israeli"
      },
      {
        "given": "Jack",
        "family": "Lanchantin"
      },
      {
        "given": "Stephen",
        "family": "Woloszynek"
      },
      {
        "given": "Anne E.",
        "family": "Carpenter"
      },
      {
        "given": "Avanti",
        "family": "Shrikumar"
      },
      {
        "given": "Jinbo",
        "family": "Xu"
      },
      {
        "given": "Evan M.",
        "family": "Cofer"
      },
      {
        "given": "Christopher A.",
        "family": "Lavender"
      },
      {
        "given": "Srinivas C.",
        "family": "Turaga"
      },
      {
        "given": "Amr M.",
        "family": "Alexandari"
      },
      {
        "given": "Zhiyong",
        "family": "Lu"
      },
      {
        "given": "David J.",
        "family": "Harris"
      },
      {
        "given": "Dave",
        "family": "DeCaprio"
      },
      {
        "given": "Yanjun",
        "family": "Qi"
      },
      {
        "given": "Anshul",
        "family": "Kundaje"
      },
      {
        "given": "Yifan",
        "family": "Peng"
      },
      {
        "given": "Laura K.",
        "family": "Wiley"
      },
      {
        "given": "Marwin H. S.",
        "family": "Segler"
      },
      {
        "given": "Simina M.",
        "family": "Boca"
      },
      {
        "given": "S. Joshua",
        "family": "Swamidass"
      },
      {
        "given": "Austin",
        "family": "Huang"
      },
      {
        "given": "Anthony",
        "family": "Gitter"
      },
      {
        "given": "Casey S.",
        "family": "Greene"
      }
    ],
    "container-title": "Journal of The Royal Society Interface",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          4
        ]
      ]
    },
    "URL": "https://doi.org/gddkhn",
    "container-title-short": "J. R. Soc. Interface",
    "PMCID": "PMC5938574",
    "PMID": "29618526",
    "id": "PZMP42Ak"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:p>Background: Data sharing accelerates scientific progress but sharing individual level data while preserving patient privacy presents a barrier. Methods and Results: Using pairs of deep neural networks, we generated simulated, synthetic \"participants\" that closely resemble participants of the SPRINT trial. We showed that such paired networks can be trained with differential privacy, a formal privacy framework that limits the likelihood that queries of the synthetic participants' data could identify a real a participant in the trial. Machine-learning predictors built on the synthetic population generalize to the original dataset. This finding suggests that the synthetic data can be shared with others, enabling them to perform hypothesis-generating analyses as though they had the original trial data. Conclusions: Deep neural networks that generate synthetic participants facilitate secondary analyses and reproducible investigation of clinical datasets by enhancing data sharing while preserving participant privacy.</jats:p>",
    "DOI": "10.1101/159756",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Privacy-preserving generative deep neural networks support clinical data sharing",
    "author": [
      {
        "given": "Brett K.",
        "family": "Beaulieu-Jones"
      },
      {
        "given": "Zhiwei Steven",
        "family": "Wu"
      },
      {
        "given": "Chris",
        "family": "Williams"
      },
      {
        "given": "Ran",
        "family": "Lee"
      },
      {
        "given": "Sanjeev P",
        "family": "Bhavnani"
      },
      {
        "given": "James Brian",
        "family": "Byrd"
      },
      {
        "given": "Casey S.",
        "family": "Greene"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          7,
          5
        ]
      ]
    },
    "URL": "https://doi.org/gcnzrn",
    "id": "fbIH12yd"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:p>Advanced machine learning models applied to large-scale genomics datasets hold the promise to be major drivers for genome science. Once trained, such models can serve as a tool to probe the relationships between data modalities, including the effect of genetic variants on phenotype. However, lack of standardization and limited accessibility of trained models have hampered their impact in practice. To address this, we present Kipoi, a collaborative initiative to define standards and to foster reuse of trained models in genomics. Already, the Kipoi repository contains over 2,000 trained models that cover canonical prediction tasks in transcriptional and post-transcriptional gene regulation. The Kipoi model standard grants automated software installation and provides unified interfaces to apply and interpret models. We illustrate Kipoi through canonical use cases, including model benchmarking, transfer learning, variant effect prediction, and building new models from existing ones. By providing a unified framework to archive, share, access, use, and build on models developed by the community, Kipoi will foster the dissemination and use of machine learning models in genomics.</jats:p>",
    "DOI": "10.1101/375345",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Kipoi: accelerating the community exchange and reuse of predictive models for genomics",
    "author": [
      {
        "given": "Ziga",
        "family": "Avsec"
      },
      {
        "given": "Roman",
        "family": "Kreuzhuber"
      },
      {
        "given": "Johnny",
        "family": "Israeli"
      },
      {
        "given": "Nancy",
        "family": "Xu"
      },
      {
        "given": "Jun",
        "family": "Cheng"
      },
      {
        "given": "Avanti",
        "family": "Shrikumar"
      },
      {
        "given": "Abhimanyu",
        "family": "Banerjee"
      },
      {
        "given": "Daniel S",
        "family": "Kim"
      },
      {
        "given": "Lara",
        "family": "Urban"
      },
      {
        "given": "Anshul",
        "family": "Kundaje"
      },
      {
        "given": "Oliver",
        "family": "Stegle"
      },
      {
        "given": "Julien",
        "family": "Gagneur"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          7,
          24
        ]
      ]
    },
    "URL": "https://doi.org/gd24sx",
    "id": "14cVrrqP1"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:p>Single-cell RNA sequencing (scRNA-seq) is a powerful tool to profile the transcriptomes of a large number of individual cells at a high resolution. These data usually contain measurements of gene expression for many genes in thousands or tens of thousands of cells, though some datasets now reach the million-cell mark. Projecting high-dimensional scRNA-seq data into a low dimensional space aids downstream analysis and data visualization. Many recent preprints accomplish this using variational autoencoders (VAE), generative models that learn underlying structure of data by compress it into a constrained, low dimensional space. The low dimensional spaces generated by VAEs have revealed complex patterns and novel biological signals from large-scale gene expression data and drug response predictions. Here, we evaluate a simple VAE approach for gene expression data, Tybalt, by training and measuring its performance on sets of simulated scRNA-seq data. We find a number of counter-intuitive performance features: i.e., deeper neural networks can struggle when datasets contain more observations under some parameter configurations. We show that these methods are highly sensitive to parameter tuning: when tuned, the performance of the Tybalt model, which was not optimized for scRNA-seq data, outperforms other popular dimension reduction approaches - PCA, ZIFA, UMAP and t-SNE. On the other hand, without tuning performance can also be remarkably poor on the same data. Our results should discourage authors and reviewers from relying on self-reported performance comparisons to evaluate the relative value of contributions in this area at this time. Instead, we recommend that attempts to compare or benchmark autoencoder methods for scRNA-seq data be performed by disinterested third parties or by methods developers only on unseen benchmark data that are provided to all participants simultaneously because the potential for performance differences due to unequal parameter tuning is so high.</jats:p>",
    "DOI": "10.1101/385534",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Parameter tuning is a key part of dimensionality reduction via deep variational autoencoders for single cell RNA transcriptomics",
    "author": [
      {
        "given": "Qiwen",
        "family": "Hu"
      },
      {
        "given": "Casey S",
        "family": "Greene"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          8,
          5
        ]
      ]
    },
    "URL": "https://doi.org/gdxxjf",
    "id": "5CsWRjfp"
  },
  {
    "publisher": "IEEE",
    "DOI": "10.1109/cvprw.2014.131",
    "type": "paper-conference",
    "source": "Crossref",
    "title": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition",
    "author": [
      {
        "given": "Ali Sharif",
        "family": "Razavian"
      },
      {
        "given": "Hossein",
        "family": "Azizpour"
      },
      {
        "given": "Josephine",
        "family": "Sullivan"
      },
      {
        "given": "Stefan",
        "family": "Carlsson"
      }
    ],
    "event": "2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
    "container-title": "2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops",
    "issued": {
      "date-parts": [
        [
          2014,
          6
        ]
      ]
    },
    "URL": "https://doi.org/f3np4s",
    "id": "x7a5SM90"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "1",
    "DOI": "10.1109/jbhi.2016.2636665",
    "type": "article-journal",
    "page": "4-21",
    "source": "Crossref",
    "title": "Deep Learning for Health Informatics",
    "volume": "21",
    "author": [
      {
        "given": "Daniele",
        "family": "Ravi"
      },
      {
        "given": "Charence",
        "family": "Wong"
      },
      {
        "given": "Fani",
        "family": "Deligianni"
      },
      {
        "given": "Melissa",
        "family": "Berthelot"
      },
      {
        "given": "Javier",
        "family": "Andreu-Perez"
      },
      {
        "given": "Benny",
        "family": "Lo"
      },
      {
        "given": "Guang-Zhong",
        "family": "Yang"
      }
    ],
    "container-title": "IEEE Journal of Biomedical and Health Informatics",
    "issued": {
      "date-parts": [
        [
          2017,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gfgtzx",
    "container-title-short": "IEEE J. Biomed. Health Inform.",
    "PMID": "28055930",
    "id": "8seWxxzY"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "12",
    "DOI": "10.1109/jproc.2017.2761740",
    "type": "article-journal",
    "page": "2295-2329",
    "source": "Crossref",
    "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey",
    "volume": "105",
    "author": [
      {
        "given": "Vivienne",
        "family": "Sze"
      },
      {
        "given": "Yu-Hsin",
        "family": "Chen"
      },
      {
        "given": "Tien-Ju",
        "family": "Yang"
      },
      {
        "given": "Joel S.",
        "family": "Emer"
      }
    ],
    "container-title": "Proceedings of the IEEE",
    "issued": {
      "date-parts": [
        [
          2017,
          12
        ]
      ]
    },
    "URL": "https://doi.org/gcnp38",
    "container-title-short": "Proc. IEEE",
    "id": "L7EocHX2"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "DOI": "10.1109/tbdata.2016.2573280",
    "type": "article-journal",
    "page": "1-1",
    "source": "Crossref",
    "title": "Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis",
    "author": [
      {
        "given": "Wenlu",
        "family": "Zhang"
      },
      {
        "given": "Rongjian",
        "family": "Li"
      },
      {
        "given": "Tao",
        "family": "Zeng"
      },
      {
        "given": "Qian",
        "family": "Sun"
      },
      {
        "given": "Sudhir",
        "family": "Kumar"
      },
      {
        "given": "Jieping",
        "family": "Ye"
      },
      {
        "given": "Shuiwang",
        "family": "Ji"
      }
    ],
    "container-title": "IEEE Transactions on Big Data",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "URL": "https://doi.org/gfvs28",
    "container-title-short": "IEEE Trans. Big Data",
    "id": "ZwUaSNWa"
  },
  {
    "publisher": "WORLD SCIENTIFIC",
    "DOI": "10.1142/9789814644730_0014",
    "type": "paper-conference",
    "source": "Crossref",
    "title": "UNSUPERVISED FEATURE CONSTRUCTION AND KNOWLEDGE EXTRACTION FROM GENOME-WIDE ASSAYS OF BREAST CANCER WITH DENOISING AUTOENCODERS",
    "author": [
      {
        "given": "JIE",
        "family": "TAN"
      },
      {
        "given": "MATTHEW",
        "family": "UNG"
      },
      {
        "given": "CHAO",
        "family": "CHENG"
      },
      {
        "given": "CASEY S",
        "family": "GREENE"
      }
    ],
    "event": "Proceedings of the Pacific Symposium",
    "container-title": "Biocomputing 2015",
    "issued": {
      "date-parts": [
        [
          2014,
          11
        ]
      ]
    },
    "URL": "https://doi.org/gcgmbs",
    "id": "PBiRSdXv"
  },
  {
    "publisher": "ACM Press",
    "DOI": "10.1145/2783258.2788613",
    "type": "paper-conference",
    "source": "Crossref",
    "title": "Intelligible Models for HealthCare",
    "author": [
      {
        "given": "Rich",
        "family": "Caruana"
      },
      {
        "given": "Yin",
        "family": "Lou"
      },
      {
        "given": "Johannes",
        "family": "Gehrke"
      },
      {
        "given": "Paul",
        "family": "Koch"
      },
      {
        "given": "Marc",
        "family": "Sturm"
      },
      {
        "given": "Noemie",
        "family": "Elhadad"
      }
    ],
    "event": "the 21th ACM SIGKDD International Conference",
    "container-title": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "URL": "https://doi.org/gftgxk",
    "id": "gSmt16Rh"
  },
  {
    "publisher": "ACM Press",
    "DOI": "10.1145/2810103.2813677",
    "type": "paper-conference",
    "source": "Crossref",
    "title": "Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures",
    "author": [
      {
        "given": "Matt",
        "family": "Fredrikson"
      },
      {
        "given": "Somesh",
        "family": "Jha"
      },
      {
        "given": "Thomas",
        "family": "Ristenpart"
      }
    ],
    "event": "the 22nd ACM SIGSAC Conference",
    "container-title": "Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security - CCS '15",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "URL": "https://doi.org/cwdm",
    "id": "zCqhgXvY"
  },
  {
    "publisher": "ACM Press",
    "DOI": "10.1145/2976749.2978318",
    "type": "paper-conference",
    "source": "Crossref",
    "title": "Deep Learning with Differential Privacy",
    "author": [
      {
        "given": "Martin",
        "family": "Abadi"
      },
      {
        "given": "Andy",
        "family": "Chu"
      },
      {
        "given": "Ian",
        "family": "Goodfellow"
      },
      {
        "given": "H. Brendan",
        "family": "McMahan"
      },
      {
        "given": "Ilya",
        "family": "Mironov"
      },
      {
        "given": "Kunal",
        "family": "Talwar"
      },
      {
        "given": "Li",
        "family": "Zhang"
      }
    ],
    "event": "the 2016 ACM SIGSAC Conference",
    "container-title": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security - CCS'16",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "URL": "https://doi.org/gcrnp3",
    "id": "LiCxcgZp"
  },
  {
    "publisher": "MIT Press - Journals",
    "issue": "7",
    "DOI": "10.1162/089976698300017197",
    "type": "article-journal",
    "page": "1895-1923",
    "source": "Crossref",
    "title": "Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms",
    "volume": "10",
    "author": [
      {
        "given": "Thomas G.",
        "family": "Dietterich"
      }
    ],
    "container-title": "Neural Computation",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1998,
          10
        ]
      ]
    },
    "URL": "https://doi.org/fqc9w5",
    "container-title-short": "Neural Computation",
    "id": "hJQdIoO3"
  },
  {
    "publisher": "Springer Nature",
    "issue": "1",
    "DOI": "10.1186/s13040-017-0155-3",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Ten quick tips for machine learning in computational biology",
    "volume": "10",
    "author": [
      {
        "given": "Davide",
        "family": "Chicco"
      }
    ],
    "container-title": "BioData Mining",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2017,
          12
        ]
      ]
    },
    "URL": "https://doi.org/gdb9wr",
    "container-title-short": "BioData Mining",
    "PMCID": "PMC5721660",
    "PMID": "29234465",
    "id": "p4Nl5If0"
  },
  {
    "publisher": "Springer Nature",
    "issue": "1",
    "DOI": "10.1186/s13321-017-0226-y",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Deep-learning: investigating deep neural networks hyper-parameters and comparison of performance to shallow methods for modeling bioactivity data",
    "volume": "9",
    "author": [
      {
        "given": "Alexios",
        "family": "Koutsoukas"
      },
      {
        "given": "Keith J.",
        "family": "Monaghan"
      },
      {
        "given": "Xiaoli",
        "family": "Li"
      },
      {
        "given": "Jun",
        "family": "Huan"
      }
    ],
    "container-title": "Journal of Cheminformatics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2017,
          6,
          28
        ]
      ]
    },
    "URL": "https://doi.org/gfwv4d",
    "container-title-short": "J Cheminform",
    "PMCID": "PMC5489441",
    "PMID": "29086090",
    "id": "19zfIm033"
  },
  {
    "publisher": "Public Library of Science (PLoS)",
    "issue": "10",
    "DOI": "10.1371/journal.pcbi.1003285",
    "type": "article-journal",
    "page": "e1003285",
    "source": "Crossref",
    "title": "Ten Simple Rules for Reproducible Computational Research",
    "volume": "9",
    "author": [
      {
        "given": "Geir Kjetil",
        "family": "Sandve"
      },
      {
        "given": "Anton",
        "family": "Nekrutenko"
      },
      {
        "given": "James",
        "family": "Taylor"
      },
      {
        "given": "Eivind",
        "family": "Hovig"
      }
    ],
    "container-title": "PLoS Computational Biology",
    "language": "en",
    "editor": [
      {
        "given": "Philip E.",
        "family": "Bourne"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2013,
          10,
          24
        ]
      ]
    },
    "URL": "https://doi.org/pjb",
    "container-title-short": "PLoS Comput Biol",
    "PMCID": "PMC3812051",
    "PMID": "24204232",
    "id": "Pf3steOn"
  },
  {
    "publisher": "Public Library of Science (PLoS)",
    "issue": "7",
    "DOI": "10.1371/journal.pcbi.1004947",
    "type": "article-journal",
    "page": "e1004947",
    "source": "Crossref",
    "title": "Ten Simple Rules for Taking Advantage of Git and GitHub",
    "volume": "12",
    "author": [
      {
        "given": "Yasset",
        "family": "Perez-Riverol"
      },
      {
        "given": "Laurent",
        "family": "Gatto"
      },
      {
        "given": "Rui",
        "family": "Wang"
      },
      {
        "given": "Timo",
        "family": "Sachsenberg"
      },
      {
        "given": "Julian",
        "family": "Uszkoreit"
      },
      {
        "given": "Felipe da Veiga",
        "family": "Leprevost"
      },
      {
        "given": "Christian",
        "family": "Fufezan"
      },
      {
        "given": "Tobias",
        "family": "Ternent"
      },
      {
        "given": "Stephen J.",
        "family": "Eglen"
      },
      {
        "given": "Daniel S.",
        "family": "Katz"
      },
      {
        "given": "Tom J.",
        "family": "Pollard"
      },
      {
        "given": "Alexander",
        "family": "Konovalov"
      },
      {
        "given": "Robert M.",
        "family": "Flight"
      },
      {
        "given": "Kai",
        "family": "Blin"
      },
      {
        "given": "Juan Antonio",
        "family": "Vizcaíno"
      }
    ],
    "container-title": "PLOS Computational Biology",
    "language": "en",
    "editor": [
      {
        "given": "Scott",
        "family": "Markel"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016,
          7,
          14
        ]
      ]
    },
    "URL": "https://doi.org/gbrb39",
    "container-title-short": "PLoS Comput Biol",
    "PMCID": "PMC4945047",
    "PMID": "27415786",
    "id": "kEX5dgzK"
  },
  {
    "publisher": "Public Library of Science (PLoS)",
    "issue": "3",
    "DOI": "10.1371/journal.pcbi.1005399",
    "type": "article-journal",
    "page": "e1005399",
    "source": "Crossref",
    "title": "Ten simple rules for responsible big data research",
    "volume": "13",
    "author": [
      {
        "given": "Matthew",
        "family": "Zook"
      },
      {
        "given": "Solon",
        "family": "Barocas"
      },
      {
        "given": "danah",
        "family": "boyd"
      },
      {
        "given": "Kate",
        "family": "Crawford"
      },
      {
        "given": "Emily",
        "family": "Keller"
      },
      {
        "given": "Seeta Peña",
        "family": "Gangadharan"
      },
      {
        "given": "Alyssa",
        "family": "Goodman"
      },
      {
        "given": "Rachelle",
        "family": "Hollander"
      },
      {
        "given": "Barbara A.",
        "family": "Koenig"
      },
      {
        "given": "Jacob",
        "family": "Metcalf"
      },
      {
        "given": "Arvind",
        "family": "Narayanan"
      },
      {
        "given": "Alondra",
        "family": "Nelson"
      },
      {
        "given": "Frank",
        "family": "Pasquale"
      }
    ],
    "container-title": "PLOS Computational Biology",
    "language": "en",
    "editor": [
      {
        "given": "Fran",
        "family": "Lewitter"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          3,
          30
        ]
      ]
    },
    "URL": "https://doi.org/gdqfcn",
    "container-title-short": "PLoS Comput Biol",
    "PMCID": "PMC5373508",
    "PMID": "28358831",
    "id": "uXPlMpfq"
  },
  {
    "publisher": "Public Library of Science (PLoS)",
    "issue": "9",
    "DOI": "10.1371/journal.pcbi.1006454",
    "type": "article-journal",
    "page": "e1006454",
    "source": "Crossref",
    "title": "SIG-DB: Leveraging homomorphic encryption to securely interrogate privately held genomic databases",
    "volume": "14",
    "author": [
      {
        "given": "Alexander J.",
        "family": "Titus"
      },
      {
        "given": "Audrey",
        "family": "Flower"
      },
      {
        "given": "Patrick",
        "family": "Hagerty"
      },
      {
        "given": "Paul",
        "family": "Gamble"
      },
      {
        "given": "Charlie",
        "family": "Lewis"
      },
      {
        "given": "Todd",
        "family": "Stavish"
      },
      {
        "given": "Kevin P.",
        "family": "O’Connell"
      },
      {
        "given": "Greg",
        "family": "Shipley"
      },
      {
        "given": "Stephanie M.",
        "family": "Rogers"
      }
    ],
    "container-title": "PLOS Computational Biology",
    "language": "en",
    "editor": [
      {
        "given": "Scott",
        "family": "Markel"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          9,
          4
        ]
      ]
    },
    "URL": "https://doi.org/gd6xd5",
    "container-title-short": "PLoS Comput Biol",
    "PMCID": "PMC6138421",
    "PMID": "30180163",
    "id": "me326jb9"
  },
  {
    "title": "The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets.",
    "volume": "10",
    "issue": "3",
    "page": "e0118432",
    "container-title": "PloS one",
    "container-title-short": "PLoS ONE",
    "ISSN": "1932-6203",
    "issued": {
      "date-parts": [
        [
          2015,
          3,
          4
        ]
      ]
    },
    "author": [
      {
        "given": "Takaya",
        "family": "Saito"
      },
      {
        "given": "Marc",
        "family": "Rehmsmeier"
      }
    ],
    "PMID": "25738806",
    "PMCID": "PMC4349800",
    "DOI": "10.1371/journal.pone.0118432",
    "abstract": "Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets. ",
    "URL": "https://www.ncbi.nlm.nih.gov/pubmed/25738806",
    "type": "article-journal",
    "id": "u86hHJ9b"
  },
  {
    "id": "wgOFUxdw",
    "type": "article-journal",
    "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    "container-title": "J. Mach. Learn. Res.",
    "page": "1929–1958",
    "volume": "15",
    "issue": "1",
    "source": "ACM Digital Library",
    "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
    "URL": "http://dl.acm.org/citation.cfm?id=2627435.2670313",
    "ISSN": "1532-4435",
    "shortTitle": "Dropout",
    "author": [
      {
        "family": "Srivastava",
        "given": "Nitish"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      },
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Salakhutdinov",
        "given": "Ruslan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014",
          1
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          6,
          12
        ]
      ]
    }
  },
  {
    "id": "eR3C2hhK",
    "type": "paper-conference",
    "title": "A Simple Weight Decay Can Improve Generalization",
    "container-title": "Proceedings of the 4th International Conference on Neural Information Processing Systems",
    "collection-title": "NIPS'91",
    "publisher": "Morgan Kaufmann Publishers Inc.",
    "publisher-place": "San Francisco, CA, USA",
    "page": "950–957",
    "source": "ACM Digital Library",
    "event-place": "San Francisco, CA, USA",
    "abstract": "It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.",
    "URL": "http://dl.acm.org/citation.cfm?id=2986916.2987033",
    "ISBN": "9781558602229",
    "note": "event-place: Denver, Colorado",
    "author": [
      {
        "family": "Krogh",
        "given": "Anders"
      },
      {
        "family": "Hertz",
        "given": "John A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1991"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          6,
          12
        ]
      ]
    }
  },
  {
    "id": "R1RpVu06",
    "type": "article-journal",
    "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    "container-title": "Journal of Machine Learning Research",
    "page": "1929-1958",
    "volume": "15",
    "source": "Journal of Machine Learning Research",
    "abstract": "Deep neural nets with a large number of parameters are very\npowerful machine learning systems. However, overfitting is a\nserious problem in such networks. Large networks are also slow\nto use, making it difficult to deal with overfitting by\ncombining the predictions of many different large neural nets at\ntest time. Dropout is a technique for addressing this problem.\nThe key idea is to randomly drop units (along with their\nconnections) from the neural network during training. This\nprevents units from co-adapting too much. During training,\ndropout samples from an exponential number of different\n“thinned” networks. At test time, it is easy to approximate the\neffect of averaging the predictions of all these thinned\nnetworks by simply using a single unthinned network that has\nsmaller weights. This significantly reduces overfitting and\ngives major improvements over other regularization methods. We\nshow that dropout improves the performance of neural networks on\nsupervised learning tasks in vision, speech recognition,\ndocument classification and computational biology, obtaining\nstate-of-the-art results on many benchmark data sets.",
    "URL": "http://jmlr.org/papers/v15/srivastava14a.html",
    "shortTitle": "Dropout",
    "author": [
      {
        "family": "Srivastava",
        "given": "Nitish"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      },
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Salakhutdinov",
        "given": "Ruslan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          6,
          12
        ]
      ]
    }
  },
  {
    "id": "nvwiZALT",
    "type": "article-journal",
    "title": "Interpreting Neural-network Connection Weights",
    "container-title": "AI Expert",
    "page": "46–51",
    "volume": "6",
    "issue": "4",
    "source": "ACM Digital Library",
    "URL": "http://dl.acm.org/citation.cfm?id=129449.129452",
    "ISSN": "0888-3785",
    "author": [
      {
        "family": "Garson",
        "given": "G. David"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1991",
          4
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          6,
          12
        ]
      ]
    }
  },
  {
    "id": "4oKcgKmU",
    "type": "paper-conference",
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "container-title": "Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37",
    "collection-title": "ICML'15",
    "publisher": "JMLR.org",
    "page": "448–456",
    "source": "ACM Digital Library",
    "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",
    "URL": "http://dl.acm.org/citation.cfm?id=3045118.3045167",
    "note": "event-place: Lille, France",
    "shortTitle": "Batch Normalization",
    "author": [
      {
        "family": "Ioffe",
        "given": "Sergey"
      },
      {
        "family": "Szegedy",
        "given": "Christian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2015"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          6,
          12
        ]
      ]
    }
  },
  {
    "type": "webpage",
    "URL": "https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility",
    "title": "Deep Learning SDK Documentation",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          1
        ]
      ]
    },
    "author": [
      {
        "literal": "NVIDIA"
      }
    ],
    "id": "1GSwNJdl7"
  },
  {
    "type": "webpage",
    "URL": "https://github.com/Benjamin-Lee/deep-rules",
    "title": "Benjamin-Lee/deep-rules GitHub repository",
    "container-title": "GitHub",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "author": [
      {
        "given": "Benjamin",
        "family": "Lee"
      }
    ],
    "id": "ysdRl4lj"
  },
  {
    "id": "1GGGHdsew",
    "type": "webpage",
    "title": "Open collaborative writing with Manubot",
    "URL": "https://greenelab.github.io/meta-review/",
    "language": "en-US",
    "author": [
      {
        "family": "Himmelstein",
        "given": "Daniel S."
      },
      {
        "family": "Rubinetti",
        "given": "Vincent"
      },
      {
        "family": "Slochower",
        "given": "David R."
      },
      {
        "family": "Hu",
        "given": "Dongbo"
      },
      {
        "family": "Malladi",
        "given": "Venkat S."
      },
      {
        "family": "Greene",
        "given": "Casey S."
      },
      {
        "family": "Gitter",
        "given": "Anthony"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2019",
          6,
          11
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          6,
          12
        ]
      ]
    }
  },
  {
    "id": "enhj7VT6",
    "type": "chapter",
    "title": "How transferable are features in deep neural networks?",
    "container-title": "Advances in Neural Information Processing Systems 27",
    "publisher": "Curran Associates, Inc.",
    "page": "3320–3328",
    "source": "Neural Information Processing Systems",
    "URL": "http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf",
    "author": [
      {
        "family": "Yosinski",
        "given": "Jason"
      },
      {
        "family": "Clune",
        "given": "Jeff"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      },
      {
        "family": "Lipson",
        "given": "Hod"
      }
    ],
    "editor": [
      {
        "family": "Ghahramani",
        "given": "Z."
      },
      {
        "family": "Welling",
        "given": "M."
      },
      {
        "family": "Cortes",
        "given": "C."
      },
      {
        "family": "Lawrence",
        "given": "N. D."
      },
      {
        "family": "Weinberger",
        "given": "K. Q."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          6,
          12
        ]
      ]
    }
  },
  {
    "id": "mIx19cpn",
    "type": "chapter",
    "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning",
    "container-title": "Advances in Neural Information Processing Systems 30",
    "publisher": "Curran Associates, Inc.",
    "page": "4148–4158",
    "source": "Neural Information Processing Systems",
    "URL": "http://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf",
    "author": [
      {
        "family": "Wilson",
        "given": "Ashia C"
      },
      {
        "family": "Roelofs",
        "given": "Rebecca"
      },
      {
        "family": "Stern",
        "given": "Mitchell"
      },
      {
        "family": "Srebro",
        "given": "Nati"
      },
      {
        "family": "Recht",
        "given": "Benjamin"
      }
    ],
    "editor": [
      {
        "family": "Guyon",
        "given": "I."
      },
      {
        "family": "Luxburg",
        "given": "U. V."
      },
      {
        "family": "Bengio",
        "given": "S."
      },
      {
        "family": "Wallach",
        "given": "H."
      },
      {
        "family": "Fergus",
        "given": "R."
      },
      {
        "family": "Vishwanathan",
        "given": "S."
      },
      {
        "family": "Garnett",
        "given": "R."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          6,
          12
        ]
      ]
    }
  }
]
