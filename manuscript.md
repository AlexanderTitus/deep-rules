---
author-meta:
- Benjamin D. Lee
- Alexander J. Titus
- Kun-Hsing Yu
- Marc G. Chevrette
- Paul Allen Stewart
- Evan M. Cofer
- Sebastian Raschka
- Finlay Maguire
- Benjamin J. Lengerich
- Alexandr A. Kalinin
- Anthony Gitter
- Casey S. Greene
- Simina M. Boca
- Timothy J. Triche, Jr.
- Thiago Britto-Borges
- Elana J. Fertig
- Michael D. Kessler
- Alexandra J. Lee
- Beth Signal
bibliography:
- content/manual-references.json
date-meta: '2020-10-14'
header-includes: '<!--

  Manubot generated metadata rendered from header-includes-template.html.

  Suggest improvements at https://github.com/manubot/manubot/blob/master/manubot/process/header-includes-template.html

  -->

  <meta name="dc.format" content="text/html" />

  <meta name="dc.title" content="Ten Quick Tips for Deep Learning in Biology" />

  <meta name="citation_title" content="Ten Quick Tips for Deep Learning in Biology" />

  <meta property="og:title" content="Ten Quick Tips for Deep Learning in Biology" />

  <meta property="twitter:title" content="Ten Quick Tips for Deep Learning in Biology" />

  <meta name="dc.date" content="2020-10-14" />

  <meta name="citation_publication_date" content="2020-10-14" />

  <meta name="dc.language" content="en-US" />

  <meta name="citation_language" content="en-US" />

  <meta name="dc.relation.ispartof" content="Manubot" />

  <meta name="dc.publisher" content="Manubot" />

  <meta name="citation_journal_title" content="Manubot" />

  <meta name="citation_technical_report_institution" content="Manubot" />

  <meta name="citation_author" content="Benjamin D. Lee" />

  <meta name="citation_author_institution" content="Lab41, In-Q-Tel" />

  <meta name="citation_author_institution" content="School of Engineering and Applied Sciences, Harvard University" />

  <meta name="citation_author_institution" content="Department of Genetics, Harvard Medical School" />

  <meta name="citation_author_orcid" content="0000-0002-7133-8397" />

  <meta name="citation_author" content="Alexander J. Titus" />

  <meta name="citation_author_institution" content="Titus Analytics" />

  <meta name="citation_author_orcid" content="0000-0002-0145-9564" />

  <meta name="citation_author" content="Kun-Hsing Yu" />

  <meta name="citation_author_institution" content="Department of Biomedical Informatics, Harvard Medical School" />

  <meta name="citation_author_orcid" content="0000-0001-9892-8218" />

  <meta name="citation_author" content="Marc G. Chevrette" />

  <meta name="citation_author_institution" content="Wisconsin Institute for Discovery and Department of Plant Pathology, University of Wisconsin-Madison" />

  <meta name="citation_author_orcid" content="0000-0002-7209-0717" />

  <meta name="twitter:creator" content="@wildtypeMC" />

  <meta name="citation_author" content="Paul Allen Stewart" />

  <meta name="citation_author_institution" content="Biostatistics and Bioinformatics Shared Resource, Moffitt Cancer Center" />

  <meta name="citation_author_orcid" content="0000-0003-0882-308X" />

  <meta name="citation_author" content="Evan M. Cofer" />

  <meta name="citation_author_institution" content="Lewis-Sigler Institute for Integrative Genomics, Princeton University" />

  <meta name="citation_author_institution" content="Graduate Program in Quantitative and Computational Biology, Princeton University" />

  <meta name="citation_author_orcid" content="0000-0003-3877-0433" />

  <meta name="citation_author" content="Sebastian Raschka" />

  <meta name="citation_author_institution" content="Department of Statistics, University of Wisconsin-Madison" />

  <meta name="citation_author_orcid" content="0000-0001-6989-4493" />

  <meta name="citation_author" content="Finlay Maguire" />

  <meta name="citation_author_institution" content="Faculty of Computer Science, Dalhousie University" />

  <meta name="citation_author_orcid" content="0000-0002-1203-9514" />

  <meta name="citation_author" content="Benjamin J. Lengerich" />

  <meta name="citation_author_institution" content="Computer Science Department, Carnegie Mellon University" />

  <meta name="citation_author_orcid" content="0000-0001-8690-9554" />

  <meta name="citation_author" content="Alexandr A. Kalinin" />

  <meta name="citation_author_institution" content="Department of Computational Medicine and Bioinformatics, University of Michigan" />

  <meta name="citation_author_orcid" content="0000-0003-4563-3226" />

  <meta name="citation_author" content="Anthony Gitter" />

  <meta name="citation_author_institution" content="Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison" />

  <meta name="citation_author_institution" content="Morgridge Institute for Research" />

  <meta name="citation_author_orcid" content="0000-0002-5324-9833" />

  <meta name="twitter:creator" content="@anthonygitter" />

  <meta name="citation_author" content="Casey S. Greene" />

  <meta name="citation_author_institution" content="Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania" />

  <meta name="citation_author_orcid" content="0000-0001-8713-9213" />

  <meta name="citation_author" content="Simina M. Boca" />

  <meta name="citation_author_institution" content="Innovation Center for Biomedical Informatics, Georgetown University Medical Center" />

  <meta name="citation_author_institution" content="Department of Oncology, Georgetown University Medical Center" />

  <meta name="citation_author_institution" content="Department of Biostatistics, Bioinformatics and Biomathematics, Georgetown University Medical Center" />

  <meta name="citation_author_institution" content="Cancer Prevention and Control Program, Lombardi Comprehensive Cancer Center" />

  <meta name="citation_author_orcid" content="0000-0002-1400-3398" />

  <meta name="citation_author" content="Timothy J. Triche, Jr." />

  <meta name="citation_author_institution" content="Center for Epigenetics, Van Andel Research Institute" />

  <meta name="citation_author_institution" content="Department of Translational Genomics, Keck School of Medicine, University of Southern California" />

  <meta name="citation_author_orcid" content="0000-0001-5665-946X" />

  <meta name="citation_author" content="Thiago Britto-Borges" />

  <meta name="citation_author_institution" content="Section of Bioinformatics and Systems Cardiology, Department of Internal Medicine III and Klaus Tschira Institute for Integrative Computational Cardiology, University Hospital Heidelberg" />

  <meta name="citation_author_orcid" content="0000-0002-6218-4429" />

  <meta name="citation_author" content="Elana J. Fertig" />

  <meta name="citation_author_institution" content="Department of Oncology, Department of Biomedical Engineering, Department of Applied Mathematics and Statistics, Johns Hopkins University" />

  <meta name="citation_author_orcid" content="0000-0003-3204-342X" />

  <meta name="citation_author" content="Michael D. Kessler" />

  <meta name="citation_author_institution" content="Department of Oncology, Johns Hopkins University" />

  <meta name="citation_author_orcid" content="0000-0003-1258-5221" />

  <meta name="citation_author" content="Alexandra J. Lee" />

  <meta name="citation_author_institution" content="Genomics and Computational Biology Graduate Program, University of Pennsylvania" />

  <meta name="citation_author_institution" content="Department of Systems Pharmacology and Translational Therapeutics, University of Pennsylvania" />

  <meta name="citation_author_orcid" content="0000-0002-0208-3730" />

  <meta name="citation_author" content="Beth Signal" />

  <meta name="citation_author_institution" content="Climate Change Cluster, University of Technology Sydney" />

  <meta name="citation_author_orcid" content="None" />

  <link rel="canonical" href="https://Benjamin-Lee.github.io/deep-rules/" />

  <meta property="og:url" content="https://Benjamin-Lee.github.io/deep-rules/" />

  <meta property="twitter:url" content="https://Benjamin-Lee.github.io/deep-rules/" />

  <meta name="citation_fulltext_html_url" content="https://Benjamin-Lee.github.io/deep-rules/" />

  <meta name="citation_pdf_url" content="https://Benjamin-Lee.github.io/deep-rules/manuscript.pdf" />

  <link rel="alternate" type="application/pdf" href="https://Benjamin-Lee.github.io/deep-rules/manuscript.pdf" />

  <link rel="alternate" type="text/html" href="https://Benjamin-Lee.github.io/deep-rules/v/0e3ebf52293f5f89fa588cf039b2a9a82a676cf2/" />

  <meta name="manubot_html_url_versioned" content="https://Benjamin-Lee.github.io/deep-rules/v/0e3ebf52293f5f89fa588cf039b2a9a82a676cf2/" />

  <meta name="manubot_pdf_url_versioned" content="https://Benjamin-Lee.github.io/deep-rules/v/0e3ebf52293f5f89fa588cf039b2a9a82a676cf2/manuscript.pdf" />

  <meta property="og:type" content="article" />

  <meta property="twitter:card" content="summary_large_image" />

  <meta property="og:image" content="https://github.com/Benjamin-Lee/deep-rules/raw/0e3ebf52293f5f89fa588cf039b2a9a82a676cf2/thumbnail.png" />

  <meta property="twitter:image" content="https://github.com/Benjamin-Lee/deep-rules/raw/0e3ebf52293f5f89fa588cf039b2a9a82a676cf2/thumbnail.png" />

  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />

  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />

  <meta name="theme-color" content="#ad1457" />

  <!-- end Manubot generated metadata -->'
keywords:
- quick tips
- machine learning
- deep learning
- artificial intelligence
lang: en-US
manubot-clear-requests-cache: false
manubot-output-bibliography: output/references.json
manubot-output-citekeys: output/citations.tsv
manubot-requests-cache-path: ci/cache/requests-cache
title: Ten Quick Tips for Deep Learning in Biology
...






<small><em>
This manuscript
([permalink](https://Benjamin-Lee.github.io/deep-rules/v/0e3ebf52293f5f89fa588cf039b2a9a82a676cf2/))
was automatically generated
from [Benjamin-Lee/deep-rules@0e3ebf5](https://github.com/Benjamin-Lee/deep-rules/tree/0e3ebf52293f5f89fa588cf039b2a9a82a676cf2)
on October 14, 2020.
</em></small>

## Authors
Please note the current author order is chronological and does not reflect the final order.



+ **Benjamin D. Lee**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-7133-8397](https://orcid.org/0000-0002-7133-8397)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [Benjamin-Lee](https://github.com/Benjamin-Lee)<br>
  <small>
     Lab41, In-Q-Tel; School of Engineering and Applied Sciences, Harvard University; Department of Genetics, Harvard Medical School
  </small>

+ **Alexander J. Titus**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-0145-9564](https://orcid.org/0000-0002-0145-9564)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [AlexanderTitus](https://github.com/AlexanderTitus)<br>
  <small>
     Titus Analytics
  </small>

+ **Kun-Hsing Yu**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-9892-8218](https://orcid.org/0000-0001-9892-8218)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [khyu](https://github.com/khyu)<br>
  <small>
     Department of Biomedical Informatics, Harvard Medical School
  </small>

+ **Marc G. Chevrette**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-7209-0717](https://orcid.org/0000-0002-7209-0717)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [chevrm](https://github.com/chevrm)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [wildtypeMC](https://twitter.com/wildtypeMC)<br>
  <small>
     Wisconsin Institute for Discovery and Department of Plant Pathology, University of Wisconsin-Madison
  </small>

+ **Paul Allen Stewart**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-0882-308X](https://orcid.org/0000-0003-0882-308X)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [pstew](https://github.com/pstew)<br>
  <small>
     Biostatistics and Bioinformatics Shared Resource, Moffitt Cancer Center
  </small>

+ **Evan M. Cofer**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-3877-0433](https://orcid.org/0000-0003-3877-0433)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [evancofer](https://github.com/evancofer)<br>
  <small>
     Lewis-Sigler Institute for Integrative Genomics, Princeton University; Graduate Program in Quantitative and Computational Biology, Princeton University
  </small>

+ **Sebastian Raschka**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-6989-4493](https://orcid.org/0000-0001-6989-4493)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [rasbt](https://github.com/rasbt)<br>
  <small>
     Department of Statistics, University of Wisconsin-Madison
  </small>

+ **Finlay Maguire**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-1203-9514](https://orcid.org/0000-0002-1203-9514)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [fmaguire](https://github.com/fmaguire)<br>
  <small>
     Faculty of Computer Science, Dalhousie University
  </small>

+ **Benjamin J. Lengerich**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-8690-9554](https://orcid.org/0000-0001-8690-9554)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [blengerich](https://github.com/blengerich)<br>
  <small>
     Computer Science Department, Carnegie Mellon University
  </small>

+ **Alexandr A. Kalinin**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-4563-3226](https://orcid.org/0000-0003-4563-3226)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [alxndrkalinin](https://github.com/alxndrkalinin)<br>
  <small>
     Department of Computational Medicine and Bioinformatics, University of Michigan
  </small>

+ **Anthony Gitter**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-5324-9833](https://orcid.org/0000-0002-5324-9833)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [agitter](https://github.com/agitter)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [anthonygitter](https://twitter.com/anthonygitter)<br>
  <small>
     Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison; Morgridge Institute for Research
  </small>

+ **Casey S. Greene**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-8713-9213](https://orcid.org/0000-0001-8713-9213)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [cgreene](https://github.com/cgreene)<br>
  <small>
     Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania
  </small>

+ **Simina M. Boca**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-1400-3398](https://orcid.org/0000-0002-1400-3398)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [SiminaB](https://github.com/SiminaB)<br>
  <small>
     Innovation Center for Biomedical Informatics, Georgetown University Medical Center; Department of Oncology, Georgetown University Medical Center; Department of Biostatistics, Bioinformatics and Biomathematics, Georgetown University Medical Center; Cancer Prevention and Control Program, Lombardi Comprehensive Cancer Center
  </small>

+ **Timothy J. Triche, Jr.**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-5665-946X](https://orcid.org/0000-0001-5665-946X)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [ttriche](https://github.com/ttriche)<br>
  <small>
     Center for Epigenetics, Van Andel Research Institute; Department of Translational Genomics, Keck School of Medicine, University of Southern California
  </small>

+ **Thiago Britto-Borges**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-6218-4429](https://orcid.org/0000-0002-6218-4429)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [tbrittoborges](https://github.com/tbrittoborges)<br>
  <small>
     Section of Bioinformatics and Systems Cardiology, Department of Internal Medicine III and Klaus Tschira Institute for Integrative Computational Cardiology, University Hospital Heidelberg
  </small>

+ **Elana J. Fertig**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-3204-342X](https://orcid.org/0000-0003-3204-342X)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [ejfertig](https://github.com/ejfertig)<br>
  <small>
     Department of Oncology, Department of Biomedical Engineering, Department of Applied Mathematics and Statistics, Johns Hopkins University
  </small>

+ **Michael D. Kessler**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0003-1258-5221](https://orcid.org/0000-0003-1258-5221)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [mdkessler](https://github.com/mdkessler)<br>
  <small>
     Department of Oncology, Johns Hopkins University
  </small>

+ **Alexandra J. Lee**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-0208-3730](https://orcid.org/0000-0002-0208-3730)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [ajlee21](https://github.com/ajlee21)<br>
  <small>
     Genomics and Computational Biology Graduate Program, University of Pennsylvania; Department of Systems Pharmacology and Translational Therapeutics, University of Pennsylvania
  </small>

+ **Beth Signal**<br>
    · ![GitHub icon](images/github.svg){.inline_icon}
    [betsig](https://github.com/betsig)<br>
  <small>
     Climate Change Cluster, University of Technology Sydney
  </small>



## Introduction {#intro}

Deep learning is a subfield of machine learning focusing on artificial neural networks with many layers.
These methods are increasingly being used for the analysis of biological data [@doi:10.1098/rsif.2017.0387].
In many cases, novel biological insights have been revealed through careful evaluation of deep learning methods ranging from predicting protein-drug binding kinetics [@doi:10.1038/s41467-017-02388-1] to identifying the lab-of-origin of synthetic DNA [@doi:10.1038/s41467-018-05378-z].
However, for researchers and students entirely new to this area and those experienced in using classical machine learning methods (for example, linear regression), using deep learning correctly can be a daunting task.
Furthermore, the lack of concise recommendations for biological applications of deep learning poses an additional challenge for newcomers wishing to apply state-of-the-art deep learning in their research.
Since deep learning is an active and specialized research area, detailed resources are rapidly rendered obsolete, and only a few resources articulate general deep learning best practices to the scientific community broadly and the biological community specifically.
To address this issue, we solicited input from a community of researchers with varied biological and deep learning interests to write this manuscript collaboratively using the GitHub version control platform [@url:https://github.com/Benjamin-Lee/deep-rules] and Manubot [@doi:10.1371/journal.pcbi.1007128].

Through the course of our discussions, several themes became clear: the importance of understanding and applying machine learning fundamentals [@doi:10.1186/s13040-017-0155-3] as a baseline for utilizing deep learning, the necessity for extensive model comparisons with careful evaluation, and the need for critical thought in interpreting results generated by means of deep learning, among others.
The major similarities between deep learning and traditional computational methods also became apparent.
Although deep learning is a distinct subfield of machine learning, it is still a subfield.
It is subject to the many limitations inherent to machine learning, and many best practices for machine learning also apply to deep learning.
In addition, as with all computational methods, deep learning should be applied in a systematic manner that is reproducible and rigorously tested.

Ultimately, the tips we collate range from high-level guidance to the implementation of best practices.
It is our hope that they will provide actionable, deep learning-specific advice for both new and experienced deep learning practitioners alike who would like to employ deep learning in biological research.
By increasing the accessibility of deep learning for applications in biological research, we aim to improve the overall quality and reporting of deep learning in the literature, enabling more researchers to utilize these state-of-the art modeling techniques.


## Tip 1: Decide whether deep learning is appropriate for your problem {#appropriate}

In recent years, the number of publications implementing deep learning in biology have risen tremendously.
Given deep learning's usefulness across a range of scientific questions and data modalities, it may appear that it is a panacea for modeling problems.
Indeed, neural networks are universal function approximators, meaning that they are in principle capable of learning any function [@doi:10.1007/BF02551274; @tag:hornik-approximation].
If deep learning is so powerful and popular, why would one ever not choose to use it?

The reason is simple: deep learning is not suited to every situation in reality.
Training deep learning models requires a significant amount of data, computing power, and expertise.
In some areas of biology where data collection is thoroughly automated, such as DNA sequencing, large amounts of quality data may be available.
For other areas which rely on manual data collection, there may not be enough data to effectively train models.
Though there are methods to increase the amount of training data, such as data augmentation (in which existing data is slightly manipulated to yield "new" samples) and weak supervision (in which simple labeling heuristics are combined to produce noisy, probabilistic labels) [@arxiv:1605.07723v3], these methods cannot overcome a complete shortage of data.
In the context of supervised classification, deep learning should be considered for datasets with at least one hundred samples per class [@arxiv:1511.06348] as a rule of thumb, though in all cases it is best suited to cases when datasets contain orders of magnitude more samples.

Furthermore, training deep learning models can be very demanding, often requiring extensive computing infrastructure and patience to achieve state-of-the-art performance [@doi:10.1109/JPROC.2017.2761740].
In some deep learning contexts, such as generating human-like text, state-of-the-art models have over one hundred billion parameters [@arxiv:2005.14165].
Training such large models from scratch can be a costly and time-consuming undertaking [@arxiv:1906.02243].
Luckily, most deep learning research in biology will not require nearly as much computation, though it usually requires more than can be done feasibly on an individual consumer-grade device.
Specialized hardware such as discrete graphics processing units (GPUs) or custom deep learning accelerators can dramatically reduce the time and cost required to train models, but this hardware is not universally accessible.
Currently, both GPU- and deep learning-optimized accelerator-equipped servers can be rented from cloud providers, though working with these servers adds additional cost and complexity.
As deep learning becomes more popular, these accelerators are likely to be more broadly available (for example, recent-generation iPhones already have such hardware).
In contrast, traditional machine learning training can often be done on a laptop (or even a \$5 computer [@arxiv:1809.00238]) in seconds to minutes.

Beyond the necessity for greater data and computational capacity in deep learning, building and training deep learning models generally requires more expertise than traditional machine learning models.
Currently, there are several competing programming frameworks for deep learning such as Tensorflow [@arxiv:1603.04467] and PyTorch [@arxiv:1912.01703].
These frameworks allow users to create and deploy entirely novel model architectures and are widely used in deep learning research as well as in industrial applications.
This flexibility combined with the rapid development of the deep learning field has resulted in large, complex frameworks that can be daunting to new users.
For readers new to software development but experienced in biology, gaining computational skills while interfacing with such complex industrial-grade tools can be a challenge.
An advantage of machine learning over deep learning is that currently there are more tools capable of automating the model selection and training process.
Automated machine learning (AutoML) tools such as TPOT [@doi:10.1007/978-3-319-31204-0_9], which is capable of using genetic programming to optimize machine learning pipelines, and Turi Create [@https://github.com/apple/turicreate], a task-oriented machine learning and deep learning framework which automatically tests multiple machine learning models when training, allow users to achieve competitive performance with only a few lines of code.
Luckily, there are efforts underway to reduce the expertise required to build and use deep learning models.
Indeed, both TPOT and Turi Create, as well as other tools such as AutoKeras [@arxiv:1806.10282], are capable of abstracting away much of the programming required for "standard" deep learning tasks.
Projects such as Keras [@https://keras.io], a high-level interface for TensorFlow, make it relatively straightforward to design and test custom deep learning architectures.
In the future, projects such as these are likely to bring deep learning experimentation within reach to even more researchers.

There are some types of problems in which using deep learning is strongly indicated over machine learning.
Assuming a sufficient quantity of quality data is available, applications such as computer vision and natural language processing are likely to benefit from deep learning.
In fact, these areas were the first to see significant breakthroughs through the application of deep learning [@doi:10.1145/3065386] during the recent deep learning revolution."
For example, Ferreira et al. used deep learning to recognize individual birds from images [@doi:10.1111/2041-210X.13436].
This problem was historically difficult but, by combining automatic data collection using RFID tags with data augmentation and transfer learning (explained in [Tip 5](#architecture)), the authors were able to use deep learning achieve 90% accuracy in several species.
Other areas include generative models, in which new samples are able to be created based on the training data, and reinforcement learning, in which agents are trained to interact with their environments.
In general, before using deep learning, investigate whether similar problems (including analogous ones in other domains) have been solved successfully using deep learning.

Depending on the amount and the nature of the available data, as well as the task to be performed, deep learning may not always be able to outperform conventional methods.
As an illustration, Rajkomar et al. [@doi:10.1038/s41746-018-0029-1] found that simpler baseline models achieved performance comparable with that of deep learning in a number of clinical prediction tasks using electronic health records, which may be a surprise to many.
Another example is provided by Koutsoukas et al., who benchmarked several traditional machine learning approaches against deep neural networks for modeling bioactivity data on moderately sized datasets [@doi:10.1186/s13321-017-0226-y].
The researchers found that while well tuned deep learning approaches generally tend to outperform conventional classifiers, simple methods such as Naive Bayes classification tend to outperform deep learning as the noise in the dataset increases.
Similarly, Chen et al. [@doi:s41746-019-0122-0] tested deep learning and a variety of traditional machine learning methods such as logistic regression and random forests on five different clinical datasets, finding that the non deep learning methods matched or exceeded the accuracy of the deep learning model in all cases while requiring an order of magnitude less training time.

In conclusion, deep learning is a tool and, like any other tool, must be used after consideration of its strengths and weaknesses for the problem at hand.
Once settled upon deep learning as a potential solution, practitioners should follow the scientific method and compare its performance to traditional methods, as we will see next.


## Tip 2: Use traditional methods to establish performance baselines {#baselines}

Deep learning requires practitioners to consider a larger number and variety of tuning parameters (that is, algorithmic settings) than more traditional machine learning methods.
These settings are often called hyperparameters, and their extensiveness can make it easy to fall into the trap of performing an unnecessarily convoluted analysis.
Hence, before applying deep learning to a given problem, we highly recommend implementing a simpler model with fewer hyperparameters at the beginning of each study.
Such models include logistic regression, random forests, k-nearest neighbors, naive Bayes, and support vector machines, and using them can help to establish baseline performance expectations.
While performance baselines available from existing literature can also serve as helpful guides, an implementation of a simpler model that uses the same software framework as planned for deep learning can greatly help with assessing the correctness of data processing steps, performance evaluation pipelines, resource requirement estimates, and computational performance estimates.
Furthermore, in some cases, it can even be useful to combine simpler baseline models with deep neural networks, as such hybrid models can improve generalization performance, model interpretability, and confidence estimation [@arxiv:1803.04765; @arxiv:1805.11783].

However, it is important to gauge the relative effectiveness of baseline and deep learning models by comparing them with established tools (for example, bioinformatics pipelines or image analysis workflows), as conventional methods (machine learning based or not) can potentially perform equivalently to or better than newer machine or deep learning methods.
While this seems unintuitive, it can in fact be the case when the available data are of limited size and/or atypical in nature.
For example, Rajkomar et al. [@doi:10.1038/s41746-018-0029-1] found that simpler baseline models achieved performance comparable with that of deep learning in a number of clinical prediction tasks using electronic health records.
Another example is provided by Koutsoukas et al., who benchmarked several traditional machine learning approaches against deep neural networks for modeling bioactivity data on moderately sized datasets [@doi:10.1186/s13321-017-0226-y].
The researchers found that while well tuned deep learning approaches generally tend to outperform conventional classifiers, simpler conventional methods such as Naive Bayes classification tend to outperform deep learning as the noise in the dataset increases.

Another potential pitfall arises from comparing the performance of baseline conventional models trained with default settings with the performance of deep learning models that have undergone rigorous tuning and optimization.
Since conventional off-the-shelf machine learning algorithms (for example, support vector machines and random forests) are also likely to benefit from hyperparameter tuning, such incongruity prevents the comparison of equally optimized models and can lead to false conclusions about model efficacy.
Hu and Greene [@doi:10.1142/9789813279827_0033] discuss this under the umbrella of what they call the "Continental Breakfast Included" effect, and they describe how the unequal tuning of hyperparameters across different learning algorithms can especially skew evaluation when the performance of an algorithm varies substantially with modest changes to its hyperparameters.
Therefore, practitioners should tune the settings of both traditional machine and deep learning-based methods before making claims about relative performance differences, as performance comparisons among machine learning and deep learning models are only informative when the models are equally well optimized.

To sum this tip up, practitioners are encouraged to create and fully tune several traditional models and standard pipelines before implementing a deep learning model.


## Tip 3: Understand the complexities of training deep neural networks {#complexities}

Correctly training deep neural networks is a non-trivial process.
There are many different options and potential pitfalls at every stage.
To get good results, you must expect to train many networks with a range of different parameter and hyperparameter settings.
Deep learning can be very demanding, often requiring extensive computing infrastructure and patience to achieve state-of-the-art performance [@doi:10.1109/JPROC.2017.2761740].
The experimentation inherent to deep learning is often noisy (requiring repetition) and represents a significant organizational challenge.
All code, random seeds, parameters, and results must be carefully corralled using general good coding practices (for example, version control [@doi:10.1371/journal.pcbi.1004947], continuous integration etc.) in order to be effective and interpretable.
This organization is also key to being able to efficiently share and reproduce your work [@doi:10.1371/journal.pcbi.1003285; @arxiv:1810.08055] as well as to update your model as new data becomes available.

One specific reproducibility pitfall that is often missed in deep learning applications is the default use of non-deterministic algorithms by CUDA/CuDNN backends when using GPUs.
Making this process reproducible is distinct from setting random seeds, which will primarily affect pseudorandom deterministic procedures such as shuffling and initialization, and requires explicitly specifying the use of deterministic algorithms in your deep learning library [@url:https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility].

Similar to [Tip 4](#baselines), try to start with a relatively small network and increase the size and complexity as needed to prevent wasting time and resources.
Beware of the seemingly trivial choices that are being made implicitly by default settings in your framework of choice. For example, choice of optimization algorithm (adaptive methods often lead to faster convergence during training but may lead to worse generalization performance on independent datasets [@url:https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning]).
These need to be carefully considered and their impacts evaluated (see [Tip 6](#hyperparameters)).

In short, use smaller and simpler networks to enable faster prototyping and follow general software development best practices to maximize reproducibility.


## Tip 4: Know your data and your question {#know-your-problem}

Having a well defined scientific question and a clear analysis plan is crucial for carrying out a successful deep learning project.
Just like it would be inadvisable to step foot in a laboratory and begin experiments without having a defined endpoint, a deep learning project should not be undertaken without preparation.
Foremost, it is important to assess if a dataset exists that can answer the biological question of interest for the given deep learning model; obtaining said data and associated metadata and reviewing the study protocol should be pursued as early on in the project as possible.
A publication or resource might purportedly offer a dataset that seems to be a good fit to test your hypothesis, but the act of obtaining it can reveal numerous problems.
It may be unstructured when it is supposed to be structured, crucial metadata such as sample stratification are missing, or the usable sample size is different than what is reported.
Data collection should be documented or a data collection protocol should be created and specified in the project documentation.
Information such as the resource used, the date downloaded, and the version of the dataset, if any, will help minimize operational confusion and will allow for transparency during the publication process.

Once the dataset is obtained, it is easy to begin analysis without a good understanding of the study design, namely why the data was collected and how.
Metadata has been standardized in many fields and can help with this (for example, see [@doi:10.1038/ng1201-365]), but if at all possible, seek out a subject matter expert who has experience with this type of data.
Receiving first-hand knowledge of the “gotchas" of a dataset will minimize the amount of guesswork and increase the success rate of a deep learning project.
For example, if the main reason why the data was collected was to test the impact of an intervention, then it may be the case that a randomized controlled trial was performed.
However, it is not always possible to perform a randomized trial for ethical or practical reasons.
Therefore, an observational study design is often considered, with the data either prospectively or retrospectively collected.
In order to ensure similar distributions of important characteristics across study groups in the absence of randomization, individuals may be matched based on age, gender, or weight.
Study designs will often have different assumptions and caveats, and these cannot be ignored during a data analysis.
Many datasets are now passively collected or do not have a specific design, but even in this case it is important to know how individuals or samples were treated.
Samples originating from the same study site, oversampling of ethnic groups or zip codes, and sample processing differences are all sources of variation that need to be accounted for.

In all cases, investigators should consider the extent to which the outcome of interest is likely to be predictable from the input data and begin by thoroughly inspecting the input data.
Data exploration with unsupervised learning and data visualization can reveal the biases and technical artifacts in these datasets, providing a critical first step to assessing data quality before any deep learning model is applied.
In some cases, these analyses can identify biases from known technical artifacts or sample processing which can be corrected through preprocessing techniques to support more accurate application of deep leaning models for subsequent prediction or feature identification problems from those datasets.

Systematic biases, which can be induced by confounding variables, for example, can lead to artifacts or so-called "batch effects."
As a consequence, models may learn to rely on correlations that are irrelevant in the scientific context of the study and may result in misguided predictions and misleading conclusions [@doi:10.1038/nrg2825].
Other study design considerations that should not be overlooked include knowing whether a study involves biological or technical replicates or both.
For example, are some samples collected from the same individuals at different time points?
Are those time points before and after some treatment?
If one assumes that all the samples are independent but that is in fact not the case, a variety of issues may arise, including having a lower effective sample size than expected.

In general, deep learning has an increased tendency for overfitting compared to classical methods, due to the large number of parameters being estimated, making issues of adequate sample size even more important (see [Tip 7](#overfitting)).
For a large dataset overfitting may not be a concern, but the modeling power of deep learning may lead to more spurious correlations and thus incorrect interpretation of results (see [Tip 9](#interpretation)).
It is important to note that molecular or imaging datasets often require appropriate clinical or demographic data to support robust analyses; this must always be balanced with the need to protect patient privacy (see [Tip 10](#privacy)).
Looking at these annotations can also clarify the study design (for example, by seeing if all the individuals are adolescents or women) or at least help the analyst employing deep learning to know what questions to ask.

Data simulation is a powerful approach to develop an understanding of how data and analytical methods interact.
In data simulation, a model is used to learn the true distribution of a training set for the purpose of creating new data points.
Often, researchers may perform simulations under some assumptions about the data generating process to identify useful model architectures and hyperparameters.
Simulated datasets can be used to verify the correctness of a model’s implementation.
To accurately test the performance of the model, it is important that simulated datasets be generated for a range of parameters.
For example, varying the parameters to violate the model's assumptions can test the sensitivity of the model's performance.
Parameter tuning the simulation can help researchers identify the key features that drive method performance.
In other cases, neural networks can be used to simulate data to better understand how to structure analyses.
For example, it is possible to study how analytical strategies cope with varying number of noise sources by using neural networks to simulate genome-wide data [@doi:10.1101/2020.05.03.066597].
Simulating data from assumptions about the data generating distribution can help to debug or characterize deep learning models, and deep learning models can also simulate data in cases where it is hard to make reasonable assumptions from first principles.

Basically, thoroughly study your data and ensure that you understand its context and peculiarities _before_ jumping into deep learning.


## Tip 5: Choose an appropriate data representation and neural network architecture {#architecture}

While certain best practices have been established by the research community [@doi:10.1007/978-3-642-35289-8], architecture design choices remain largely problem-specific and are vastly empirical efforts requiring extensive experimentation.
Furthermore, as deep learning is a quickly evolving field, many recommendations are often short-lived, and are frequently replaced by newer insights supported by recent empirical results.
This is further complicated by the fact that many recommendations do not generalize well across different problems and datasets.
Therefore, unfortunately, choosing how to represent your data and design your architecture is closer to an art than a science.
That said, there are some general principles that are useful to follow when experimenting.

First and foremost, use your knowledge of the available data and your question (see [Tip 4](#know-your-problem)) to inform your data representation and architectural design choices.
For example, if your dataset is an array of measurements with no natural ordering of inputs (such as gene expression data), multilayer perceptrons (MLPs) may be effective.
These are the most basic type of neural network, and they are able to learn complex non-linear relationships across the input data despite their relative simplicity.
Similarly, if your dataset is comprised of images, convolutional neural networks (CNNs) are a good choice because they emphasize local structures and adjacency within the data.
CNNs may also be a good choice for learning on sequences, as recent empirical evidence suggests that they can outperform canonical sequence learning techniques such as recurrent neural networks (RNNs) and the closely related long short-term memory (LSTM) networks [@arxiv:1803.01271].

Deep learning models typically benefit from increasing the amount of labeled data with which to train on.
Large amounts of data help to avoid overfitting (see [Tip 7](#overfitting)), and increase the likelihood of achieving top performance on a given task.
In the event that there is not enough data available to train your model, consider using transfer learning.
In transfer learning, a model whose weights were generated by training on another dataset is used as the starting point for training [@tag:Yosinski2014].
Transfer learning is most useful when the pre-training and target datasets are of similar nature [@tag:Yosinski2014].
For this reason, it is important to search for similar datasets that are already available.
These can potentially be used to increase the size of the training set or for pre-training and subsequent fine-tuning on the target data.
However, even when this assumption does not hold, transferring features still can still improve model performance compared with random feature initialization.
For example Rojkomar et al. showed advantages of ImageNet-pretraining [@doi:10.1007/s11263-015-0816-y] for a model that is applied to grayscale medical image classification [@doi:10.1007/s10278-016-9914-9].
In addition, or as an alternative to pre-training models on larger datasets for transfer learning yourself, you may also be able to obtain pre-trained models from public repositories, such as Kipoi [@doi:10.1101/375345] for genomics models.
Moreover, learned features can be helpful even when a pre-training task is different from a target task [@doi:10.1109/CVPRW.2014.131].
Another related approach is multi-task learning, which consists of simultaneously training a network for multiple separate tasks that share features.
In fact, multi-task learning can be used separately or even in combination with transfer learning [@doi:10.1109/TBDATA.2016.2573280].

This tip can be distilled into two main action points: first, base your network's architecture on your knowledge of the problem and, second, take advantage of similar existing data or pre-trained deep learning models.


## Tip 6: Tune your hyperparameters extensively and systematically {#hyperparameters}

Multi-layer neural networks can approximate arbitrary continuous functions, given at least one hidden layer, a non-linear activation function, and a large number of hidden units [@tag:hornik-approximation].
The same theory applies to deeper architectures, which require an exponentially smaller number of hidden units to approximate functions with the same complexity as neural networks with only one hidden layer.
The flexibility of neural networks to approximate arbitrary, continuous functions as well as the overall trend towards deeper architectures with an increasing number of hidden units and learnable weight parameters (the so-called increasing "capacity" of neural networks) allows for solving more and more complex problems but also poses additional challenges during model training.
You should expect to systematically evaluate the impact of numerous hyperparameters when you aim to apply deep neural networks to new data or challenges.
Hyperparameters are typically manifested in the choice of optimization algorithms, learning rate, activation functions, number of hidden layers and hidden units, size of the training batches, weight initialization schemes, and also seeds for pseudo-random number generators used for dataset shuffling and weight initialization.
Moreover, additional hyperparameters are introduced common techniques that facilitate the training of deeper architectures, such as norm penalties (typically in the form of $L^2$ regularization), Dropout [@tag:srivastava-dropout], and Batch Normalization [@tag:ioffe-batchnorm], which can reduce the effect of the so-called vanishing or exploding gradient problem when working with deep neural networks.

This flexibility also makes it difficult to evaluate the extent to which neural network methods are well suited to solving a task.
We discussed how the Continental Breakfast Included effect could affect methods developers seeking to compare techniques in [Tip 2](#baselines).
This effect also has implications for those seeking to use existing deep learning methods because performance estimates from deep neural networks are often provided after tuning.
The implication of this effect on users of deep neural networks is that attaining performance numbers that match those reported in publications is likely to require a relatively large input of human and computation time for hyperparameter optimization.

To get the best performance of your model, be sure to systematically optimize your hyperparameters on your tuning dataset, introduced in the next section.


## Tip 7: Address deep neural networks' increased tendency to overfit the dataset {#overfitting}

Overfitting is one of the most significant dangers you'll face in deep learning (and traditional machine learning).
Put simply, overfitting occurs when a model fits patterns in the training data too closely, includes noise or non-scientifically relevant perturbations, or in the most extreme case, simply memorizes patterns in the training set.
This subtle distinction is made clearer by seeing what happens when a model is tested on data to which it was not exposed during training: just as a student who memorizes exam materials struggles to correctly answer questions for which they have not studied, a machine learning model that has overfit to its training data will perform poorly on unseen test data.
Deep learning models are particularly susceptible to overfitting due to their relatively large number of parameters and associated representational capacity.
To continue the student analogy, a smarter student has greater potential for memorization than average one and thus may be more inclined to memorize.

![A visual example of overfitting and failure to generalize. While a high-degree polynomial gets high accuracy on its training data, it performs poorly on data unlike that which it has seen before. In contrast, a simple linear regression works well on both datasets. The greater representational capacity of the polynomial is analogous to using a larger or deeper neural network.](images/overfitting.png){#fig:overfitting-fig}

To evaluate deep supervised learning models, they should be trained, tuned, and tested on non-overlapping datasets.
The data used for testing should be locked and only used one-time for evaluating the final model after all tuning steps are completed.
Using a test set more than once will lead to biased estimates of the generalization performance [@arxiv:1811.12808; @doi:10.1162/089976698300017197].
While transformation and normalization procedures need to be applied equally to all datasets, the parameters required for such procedures (for example, quantile normalization, a common standardization method when analyzing gene-expression data) should only be derived from training data, not tuning and test data, to keep the latter two independent.
Additionally, many conventional metrics for classification (e.g. area under the receiver operating characteristic curve or AUROC) have limited utility in cases of extreme class imbalance [@pmid:25738806].
Model performance should be evaluated with a carefully picked panel of relevant metrics that make minimal assumptions about the composition of the testing data [@doi:10.1021/acs.molpharmaceut.7b00578], with particular consideration given to metrics that are most directly applicable to the task at hand.

The simplest way to combat overfitting is to detect it.
This can be done by splitting the dataset into three parts: a training set, a tuning set (also commonly called a validation set in the machine learning literature), and a test set.
By exposing the model solely to the training data during fitting, a researcher can use the model's performance on the unseen test data to measure the amount of overfitting.
While a slight drop in performance from the training set to the test set is normal, a significant drop is a clear sign of overfitting (see Figure @fig:overfitting-fig for a visual demonstration of an overfit model that performs poorly on test data).
In addition, there are a variety of techniques to reduce overfitting during training including data augmentation and regularization techniques such as dropout [@url:http://jmlr.csail.mit.edu/papers/v15/srivastava14a.html] and weight decay [@tag:krogh-weight-decay].
Another way, as described by Chuang and Keiser, is to identify the baseline level of memorization of the network by training on the data with the labels randomly shuffled and to see if the model performs better on the actual data [@doi:10.1021/acschembio.8b00881].
If the model performs no better on real data than randomly scrambled data, then the performance of the model can be attributed to overfitting.

Additionally, in biology and medicine it is critical to consider independence when defining training and test sets.
For example, a deep learning model for pneumonia detection in chest X-rays performed well but failed to generalize to outside hospitals because they were able to detect which hospital the image was from and exploited this information when making predictions [@doi:10.1371/journal.pmed.1002683].
Similarly, when dealing with sequence data, holding out data that are evolutionarily related or share structural homology to the training data can result in overfitting.
In these cases, simply holding out test data selected from a random partition of the training data is insufficient.
The best remedy for confounding variables is to [know your data](#know-your-problem) and to test your model on truly independent data.

In essence, practitioners should split data into training, tuning, and single-use testing sets to assess the performance of the model on data that can provide a reliable estimate of its generalization performance.
Futhermore, be cognizant of the danger of skewed or biased data artificially inflating accuracy.


## Tip 8: Your deep learning models can be more transparent {#blackbox}

Model interpretability is a broad concept.
In certain cases, the goal behind interpretation is to understand the underlying data generating processes while in other cases the goal is to understand why a model made the prediction that it did for a specific example or set of examples.
In much of the machine learning literature, including in our guidelines, the concept of model interpretability refers to the ability to identify the discriminative features that influence or sway the predictions.
machine learning models vary widely in terms of interpretability: some are fully transparent while others are considered to be "black-boxes" that make predictions with little ability to examine why.
Logistic regression and decision tree models are generally considered interpretable, while deep neural networks are often considered among the most difficult to interpret because they can have many parameters and non-linear relationships. 

Model interpretability is particularly important in biomedicine, where subsequent decision making often requires human input.
For example, while prediction rules can be derived from high-throughput molecular datasets, most affordable clinical tests rely on lower dimensional measurements of a limited number of biomarkers.
Selecting those biomarkers to support decision making is an important modeling and interpretation challenge.
Many authors attribute a lower uptake of deep learning tools in healthcare to interpretability challenges [@doi:10.1109/JBHI.2016.2636665; @doi:10.1038/s41551-018-0315-x]. 
Strategies to interpret both machine learning and deep learning models are rapidly emerging, and the literature on the topic is growing at an exponential rate [@arxiv:2001.02522].
Therefore, instead of recommending specific methods for either deep learning-specific or general-purpose model interpretation, we suggest consulting [@url:https://christophm.github.io/interpretable-ml-book/] which is freely available and continually updated.

Model interpretation is an open, active area of research.
It is becoming more feasible to interpret models with many parameters and non-linear relationships, but in many cases simpler models remain substantially easier to interpret than more complex ones.
When deciding on a machine learning approach and model architecture, consider an interpretability versus accuracy tradeoff.
A challenge in considering this tradeoff is that the extent to which one trades interpretability for accuracy depends on the problem itself.
When the features provided to the model are already highly relevant to the task at hand, a simpler, interpretable model that gives up only a little performance when compared to a very complex one more useful in many settings.
On the other hand, if features must be combined in complex ways to be meaningful for the task, the performance difference of a model capable of capturing that structure may outweigh the interpretability costs.
An appropriate choice can only be made after careful consideration, which often includes estimating the performance of a simple, linear model that serves as a [baseline](#baselines).
In cases where models are learned from high-throughput datasets, a small subset of features in the dataset may be strongly correlated with the complex combination of the larger feature set defined from the deep learning model.
In this case, this more limited number of features can themselves be used in the subsequent simplified model to further enhance interpretability of the model.
This feature reduction can be essential to defining biomarker panels that enable clinical applications.


## Tip 9: Don't over-interpret predictions {#interpretation}

Once we have trained an accurate deep learning model, we often want to use it to deduce scientific findings.
In doing so, we need to take care to correctly interpret the model's predictions.
Because deep models can be difficult to interpret intuitively, there is a temptation to anthropomorphize the models.
We must resist this temptation.

A common saying in statistics classes is "correlation doesn't imply causality".
While we know that accurately predicting an outcome doesn't imply learning the causal mechanism, it can be easy to forget this lesson when the predictions are extremely accurate.
A poignant example of this lesson is [@tag:predicting-pneumonia-mortality; @doi:10.1145/2783258.2788613].
In this study, the authors evaluated the capacities of several models to predict the probability of death for patients admitted to an intensive care unit with pneumonia.
Unsurprisingly, the neural network model achieved the best predictive accuracy.
However, after fitting a rule-based model, the authors discovered that the hospital data implied the rule `HasAsthma(x) => LowerRisk(x)`.
This rule contradicts medical understanding - having asthma doesn't make pneumonia better!
This rule was supported by the data (pneumonia patients with a history of asthma tended to receive more aggressive care), so the neural network also learned to make predictions according to this rule.
Guiding treatment decisions according to the predictions of the neural network would have been disastrous, even though the neural network had high predictive accuracy.

To trust deep learning models, we must combine knowledge of the training data ([Tip 4](#know-your-problem)) with inspection of the model ([Tip 8](#blackbox)).
To move beyond fitting predictive models toward building understanding and deducing scientific conclusions, probe data domains where your model succeeds and contrast them with domains where your model fails in order to identify your model's internal logic, taking care to avoid overinterpreting or anthropomorphizing the model.


## Tip 10: Don't share models trained on sensitive data {#privacy}

Practitioners may encounter datasets that cannot be shared, such as ones for which there would be significant ethical or legal issues associated with release [@doi:10.1371/journal.pcbi.1005399].
Examples of such data include classified, confidential, trade secret biological data as well as medical records, certain genomic assays, and personally identifiable information [@doi:10.1038/s41576-020-0257-5].
One of the greatest opportunities for deep learning in biology is the ability for these techniques to extract information that cannot readily be captured by traditional methods [@arxiv:1509.09292].
The representation learning of the deep learning models can capture information-rich abstractions of multiple features of the data during the training process.
However, these features may be more prone to leak the data that they were trained over if the model is shared or allowed to be queried with arbitrary inputs.
Thus, with both deep learning and certain traditional machine learning methods (for example, _k_-nearest neighbors models, which learn by memorizing the full training data), it is imperative not to share models trained on sensitive data.

Techniques to train deep neural networks without sharing unencrypted access to data are being advanced through implementations of homomorphic encryption [@doi:10.1371/journal.pcbi.1006454; @arxiv:1811.00778], but adversarial training techniques such as model inversion attacks can be used to exploit model predictions to recover recognizable images of people's faces used for training [@doi:10.1145/2810103.2813677].
Privacy preserving techniques [@arxiv:1811.04017], such as differential privacy [@doi:10.1145/2976749.2978318; @doi:10.1101/159756; @arxiv:1812.01484], can help to mitigate risks as long as the assumptions underlying these techniques are met.
These techniques provide a path towards a future where models can be shared, but more software development and theoretical advances will be required to make these techniques easy to apply in many settings.
Until then, don't share models trained on sensitive data.


## Conclusion {#conclusion}

Deep learning techniques have the potential for wide use in biology, meeting or exceeding the performance of both humans and the current state-of-the art algorithms in a variety of tasks.
Beyond simply achieving good predictive performance, deep learning has the potential to generate novel biological insights that could assist the progress of fundamental research.
To realize this potential, the use of deep learning as a research tool must be approached as any other tool would be: scientifically and thoughtfully.
We hope that our tips will serve as a starting point for the discussion of best practices for deep learning as they apply to biology, not as an ending point.


## Acknowledgements {#acknowledgements}

The authors would like the thank Daniel Himmelstein and the developers of Manubot for creating the software that enabled the collaborative composition of this manuscript.
We would also like to thank [**TODO**: insert the names of the contributors who don't meet the standards for authorship] for their contributions to the discussions that comprised the initial stage of the drafting process.


## References {.page_break_before}

<!-- Explicitly insert bibliography here -->
<div id="refs"></div>


[@tag:srivastava-dropout]: http://dl.acm.org/citation.cfm?id=2670313
[@tag:ioffe-batchnorm]: https://dl.acm.org/citation.cfm?id=3045118.3045167
[@tag:garson-interpreting]: https://dl.acm.org/citation.cfm?id=129452
[@tag:hornik-approximation]: doi:10.1016/0893-6080(91)90009-T
[@tag:krogh-weight-decay]: http://dl.acm.org/citation.cfm?id=2986916.2987033
[@tag:predicting-pneumonia-mortality]: doi:10.1016/S0933-3657(96)00367-3
[@tag:Yosinski2014]: https://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks
