## Tip 10: Don't share models trained on sensitive data {#privacy}

One of the greatest opportunities for deep learning in biology is the ability for deep learning techniques to incorporate representation learning to extract information that can not readily be captured by traditional methods [@arxiv:1509.09292].
The abundance of features for each training example means that the representation learning of the deep learning models can capture information-rich abstractions of data during the training process.
Therefore with both deep learning and traditional machine learning models (_e.g._ _k_-nearest neighbors models, which learn by memorizing the full training data), it is imperative not to share models trained on sensitive data.
Applying deep learning to images of cats from the internet does not pose significant ethical, legal, or privacy problems; this is not the case when dealing with classified, confidential, trade secret, or other types of biological data that cannot be shared.

Techniques to train deep neural networks without sharing unencrypted access to data are being advanced through implementations of homomorphic encryption [@doi:10.1371/journal.pcbi.1006454; @arxiv:1811.00778].
However, adversarial training techniques such as model inversion attacks can be used to exploit model predictions to recover recognizable images of people's faces used for training [@doi:10.1145/2810103.2813677].
These risks are more significant in deep learning than traditional machine learning because models have more representational capacity.
The large number of model weights, even in a relatively small network, allow deep learning to model high-dimensional non-linear relationships among features.
Such models can learn more nuanced features of specific data, but these features may be so specific that they may reveal the underlying sensitive data.
When training deep learning models on sensitive data, using privacy preserving techniques [@arxiv:1811.04017], such as differential privacy [@doi:10.1145/2976749.2978318; @doi:10.1101/159756; @arxiv:1812.01484], can help to mitigate risks as long as the assumptions underlying these techniques are met.
