## Tip 4: Know your data and your question {#know-your-problem}

Having a well-defined question, a hypothesis to test based on that question, and a clear analysis plan for proving the hypothesis are all crucial for carrying out a successful deep learning project.
Just like it would be inadvisable to step foot in a laboratory and begin experiments without having a defined endpoint, a deep learning project should not be undertaken without preparation.
Foremost, it is important to assess if a dataset exists that can prove or disprove the hypothesis, and obtaining said data and associated meta-data should be pursued as early on in the project as possible.
A publication or resource might purportedly offer data that seems to be a good fit to test your hypothesis, but the act of obtaining the data can reveal numerous problems such as the data is unstructured when it is supposed to be structured, crucial meta-data such as sample stratification is missing, or the usable sample size is different than what is reported.
Data collection should be documented or a data collection protocol should be created and specified in the project documentation.
Information such as the resource used, the date downloaded, and the version of the dataset, if any, will help minimize operational confusion and will allow for transparency during the publication process.
Once the data is obtained, it is easy to begin analyzing data without a good understanding of the study design, namely why the data were collected and how.
Having appropriate meta-data and a comprehensive data dictionary is essential for any analysis, including one that involves deep learning.
If at all possible, seek out a subject matter expert who has experience with this type of data.
Receiving first-hand knowledge of the â€œgotchas" of a dataset will minimize the amount of guesswork and increase  the success rate of the deep learning project.
For example, if the main reason why the data were collected was to test the impact of an intervention, then it may be the case that a randomized controlled trial was performed.
However, if the standard of care requires an intervention, such as chemotherapy for cancer, then it would be unethical to randomize a patient into a placebo arm.
Therefore, the design of the study may have been an observational one, either prospective or retrospective and may also incorporate some amount of matching.
This matching means that cases and controls may have been selected so that the age range, gender, or weight distribution is similar.
All of these designs have different assumptions and caveats, which cannot be ignored during a data analysis.
Many datasets are now passively collected or do not have a specific design, but even in this case it is important to know how individuals or samples were treated.
Samples originating from the same study site, oversampling of ethnic groups or zip codes, and sample processing differences are all sources of variation that need to be accounted for.

Systematic biases can lead to artifacts or "batch effects," which mean that instead of finding correlates with an outcome or grouping of interest, the investigator may find correlates with variables that are not of interest and obtain misleading results [@doi:10.
1038/nrg2825].
Other study design considerations that should not be overlooked include knowing whether a study involves biological or technical replicates or both.
For example, are some samples collected from the same individuals at different time points? Are those time points before and after some treatment?If one assumes that all the samples are independent but that is in fact not the case, a variety of issues may arise, including having a lower effective sample size than expected.

In general, deep learning has an increased tendency for overfitting, compared to classical methods, due to the large number of parameters being estimated, making issues of adequate sample size even more important (see [Tip 7](#overfitting)).
For a large dataset, overfitting may not be a concern, but the modeling power of deep learning may lead to more spurious correlations and thus incorrect interpretation of results (see [Tip 9](#interpretation)).
Finally, it is important to note that with the exception of very specific cases of unsupervised data analysis, it is generally the case that a molecular or imaging dataset does not have much value without appropriate clinical or demographic data; this must always be balanced with the need to protect patient privacy (see [Tip 10](#privacy)).
Looking at these data can also clarify the study design (for example, by seeing if all the individuals are adolescents or women) or at least help the analyst employing deep learning to know what questions to ask.
