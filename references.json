[
  {
    "number": "1312.6114v10",
    "version": "10",
    "URL": "https://arxiv.org/abs/1312.6114v10",
    "title": "Auto-Encoding Variational Bayes",
    "issued": {
      "date-parts": [
        [
          2013,
          12,
          20
        ]
      ]
    },
    "author": [
      {
        "literal": "Diederik P Kingma"
      },
      {
        "literal": "Max Welling"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.",
    "type": "report",
    "id": "NLVTJ9Lj"
  },
  {
    "number": "1509.09292v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1509.09292v2",
    "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints",
    "issued": {
      "date-parts": [
        [
          2015,
          9,
          30
        ]
      ]
    },
    "author": [
      {
        "literal": "David Duvenaud"
      },
      {
        "literal": "Dougal Maclaurin"
      },
      {
        "literal": "Jorge Aguilera-Iparraguirre"
      },
      {
        "literal": "Rafael Gómez-Bombarelli"
      },
      {
        "literal": "Timothy Hirzel"
      },
      {
        "literal": "Alán Aspuru-Guzik"
      },
      {
        "literal": "Ryan P. Adams"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "We introduce a convolutional neural network that operates directly on graphs.\nThese networks allow end-to-end learning of prediction pipelines whose inputs\nare graphs of arbitrary size and shape. The architecture we present generalizes\nstandard molecular feature extraction methods based on circular fingerprints.\nWe show that these data-driven features are more interpretable, and have better\npredictive performance on a variety of tasks.",
    "type": "report",
    "id": "UeE0s74F"
  },
  {
    "number": "1803.04765v1",
    "version": "1",
    "URL": "https://arxiv.org/abs/1803.04765v1",
    "title": "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust\n  Deep Learning",
    "issued": {
      "date-parts": [
        [
          2018,
          3,
          13
        ]
      ]
    },
    "author": [
      {
        "literal": "Nicolas Papernot"
      },
      {
        "literal": "Patrick McDaniel"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Deep neural networks (DNNs) enable innovative applications of machine\nlearning like image recognition, machine translation, or malware detection.\nHowever, deep learning is often criticized for its lack of robustness in\nadversarial settings (e.g., vulnerability to adversarial inputs) and general\ninability to rationalize its predictions. In this work, we exploit the\nstructure of deep learning to enable new learning-based inference and decision\nstrategies that achieve desirable properties such as robustness and\ninterpretability. We take a first step in this direction and introduce the Deep\nk-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest\nneighbors algorithm with representations of the data learned by each layer of\nthe DNN: a test input is compared to its neighboring training points according\nto the distance that separates them in the representations. We show the labels\nof these neighboring points afford confidence estimates for inputs outside the\nmodel's training manifold, including on malicious inputs like adversarial\nexamples--and therein provides protections against inputs that are outside the\nmodels understanding. This is because the nearest neighbors can be used to\nestimate the nonconformity of, i.e., the lack of support for, a prediction in\nthe training data. The neighbors also constitute human-interpretable\nexplanations of predictions. We evaluate the DkNN algorithm on several\ndatasets, and show the confidence estimates accurately identify inputs outside\nthe model, and that the explanations provided by nearest neighbors are\nintuitive and useful in understanding model failures.",
    "type": "report",
    "id": "uBcf6TJ2"
  },
  {
    "number": "1805.11783v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1805.11783v2",
    "title": "To Trust Or Not To Trust A Classifier",
    "issued": {
      "date-parts": [
        [
          2018,
          5,
          30
        ]
      ]
    },
    "author": [
      {
        "literal": "Heinrich Jiang"
      },
      {
        "literal": "Been Kim"
      },
      {
        "literal": "Melody Y. Guan"
      },
      {
        "literal": "Maya Gupta"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Knowing when a classifier's prediction can be trusted is useful in many\napplications and critical for safely using AI. While the bulk of the effort in\nmachine learning research has been towards improving classifier performance,\nunderstanding when a classifier's predictions should and should not be trusted\nhas received far less attention. The standard approach is to use the\nclassifier's discriminant or confidence score; however, we show there exists an\nalternative that is more effective in many situations. We propose a new score,\ncalled the trust score, which measures the agreement between the classifier and\na modified nearest-neighbor classifier on the testing example. We show\nempirically that high (low) trust scores produce surprisingly high precision at\nidentifying correctly (incorrectly) classified examples, consistently\noutperforming the classifier's confidence score as well as many other\nbaselines. Further, under some mild distributional assumptions, we show that if\nthe trust score for an example is high (low), the classifier will likely agree\n(disagree) with the Bayes-optimal classifier. Our guarantees consist of\nnon-asymptotic rates of statistical consistency under various nonparametric\nsettings and build on recent developments in topological data analysis.",
    "type": "report",
    "id": "2bsGpiQt"
  },
  {
    "number": "1807.00431v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1807.00431v2",
    "title": "Confounding variables can degrade generalization performance of\n  radiological deep learning models",
    "issued": {
      "date-parts": [
        [
          2018,
          7,
          2
        ]
      ]
    },
    "author": [
      {
        "literal": "John R. Zech"
      },
      {
        "literal": "Marcus A. Badgeley"
      },
      {
        "literal": "Manway Liu"
      },
      {
        "literal": "Anthony B. Costa"
      },
      {
        "literal": "Joseph J. Titano"
      },
      {
        "literal": "Eric K. Oermann"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Early results in using convolutional neural networks (CNNs) on x-rays to\ndiagnose disease have been promising, but it has not yet been shown that models\ntrained on x-rays from one hospital or one group of hospitals will work equally\nwell at different hospitals. Before these tools are used for computer-aided\ndiagnosis in real-world clinical settings, we must verify their ability to\ngeneralize across a variety of hospital systems. A cross-sectional design was\nused to train and evaluate pneumonia screening CNNs on 158,323 chest x-rays\nfrom NIH (n=112,120 from 30,805 patients), Mount Sinai (42,396 from 12,904\npatients), and Indiana (n=3,807 from 3,683 patients). In 3 / 5 natural\ncomparisons, performance on chest x-rays from outside hospitals was\nsignificantly lower than on held-out x-rays from the original hospital systems.\nCNNs were able to detect where an x-ray was acquired (hospital system, hospital\ndepartment) with extremely high accuracy and calibrate predictions accordingly.\nThe performance of CNNs in diagnosing diseases on x-rays may reflect not only\ntheir ability to identify disease-specific imaging findings on x-rays, but also\ntheir ability to exploit confounding information. Estimates of CNN performance\nbased on test data from hospital systems used for model training may overstate\ntheir likely real-world performance.",
    "type": "report",
    "id": "FEPLn1Uo"
  },
  {
    "number": "1811.00778v1",
    "version": "1",
    "URL": "https://arxiv.org/abs/1811.00778v1",
    "title": "The AlexNet Moment for Homomorphic Encryption: HCNN, the First\n  Homomorphic CNN on Encrypted Data with GPUs",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          2
        ]
      ]
    },
    "author": [
      {
        "literal": "Ahmad Al Badawi"
      },
      {
        "literal": "Jin Chao"
      },
      {
        "literal": "Jie Lin"
      },
      {
        "literal": "Chan Fook Mun"
      },
      {
        "literal": "Sim Jun Jie"
      },
      {
        "literal": "Benjamin Hong Meng Tan"
      },
      {
        "literal": "Xiao Nan"
      },
      {
        "literal": "Khin Mi Mi Aung"
      },
      {
        "literal": "Vijay Ramaseshan Chandrasekhar"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Fully homomorphic encryption, with its widely-known feature of computing on\nencrypted data, empowers a wide range of privacy-concerned cloud applications\nincluding deep learning as a service. This comes at a high cost since FHE\nincludes highly-intensive computation that requires enormous computing power.\nAlthough the literature includes a number of proposals to run CNNs on encrypted\ndata, the performance is still far from satisfactory. In this paper, we push\nthe level up and show how to accelerate the performance of running CNNs on\nencrypted data using GPUs. We evaluated a CNN to classify homomorphically the\nMNIST dataset into 10 classes. We used a number of techniques such as\nlow-precision training, unified training and testing network, optimized FHE\nparameters and a very efficient GPU implementation to achieve high performance.\nOur solution achieved high security level (> 128 bit) and high accuracy (99%).\nIn terms of performance, our best results show that we could classify the\nentire testing dataset in 14.105 seconds, with per-image amortized time (1.411\nmilliseconds) 40.41x faster than prior art.",
    "type": "report",
    "id": "3326vtLW"
  },
  {
    "number": "1811.04017v2",
    "version": "2",
    "URL": "https://arxiv.org/abs/1811.04017v2",
    "title": "A generic framework for privacy preserving deep learning",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          9
        ]
      ]
    },
    "author": [
      {
        "literal": "Theo Ryffel"
      },
      {
        "literal": "Andrew Trask"
      },
      {
        "literal": "Morten Dahl"
      },
      {
        "literal": "Bobby Wagner"
      },
      {
        "literal": "Jason Mancuso"
      },
      {
        "literal": "Daniel Rueckert"
      },
      {
        "literal": "Jonathan Passerat-Palmbach"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "We detail a new framework for privacy preserving deep learning and discuss\nits assets. The framework puts a premium on ownership and secure processing of\ndata and introduces a valuable representation based on chains of commands and\ntensors. This abstraction allows one to implement complex privacy preserving\nconstructs such as Federated Learning, Secure Multiparty Computation, and\nDifferential Privacy while still exposing a familiar deep learning API to the\nend-user. We report early results on the Boston Housing and Pima Indian\nDiabetes datasets. While the privacy features apart from Differential Privacy\ndo not impact the prediction accuracy, the current implementation of the\nframework introduces a significant overhead in performance, which will be\naddressed at a later stage of the development. We believe this work is an\nimportant milestone introducing the first reliable, general framework for\nprivacy preserving deep learning.",
    "type": "report",
    "id": "1HuQe3Z8X"
  },
  {
    "number": "1812.01484v1",
    "version": "1",
    "URL": "https://arxiv.org/abs/1812.01484v1",
    "title": "Privacy-Preserving Distributed Deep Learning for Clinical Data",
    "issued": {
      "date-parts": [
        [
          2018,
          12,
          4
        ]
      ]
    },
    "author": [
      {
        "literal": "Brett K. Beaulieu-Jones"
      },
      {
        "literal": "William Yuan"
      },
      {
        "literal": "Samuel G. Finlayson"
      },
      {
        "literal": "Zhiwei Steven Wu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "abstract": "Deep learning with medical data often requires larger samples sizes than are\navailable at single providers. While data sharing among institutions is\ndesirable to train more accurate and sophisticated models, it can lead to\nsevere privacy concerns due the sensitive nature of the data. This problem has\nmotivated a number of studies on distributed training of neural networks that\ndo not require direct sharing of the training data. However, simple distributed\ntraining does not offer provable privacy guarantees to satisfy technical safe\nstandards and may reveal information about the underlying patients. We present\na method to train neural networks for clinical data in a distributed fashion\nunder differential privacy. We demonstrate these methods on two datasets that\ninclude information from multiple independent sites, the eICU collaborative\nResearch Database and The Cancer Genome Atlas.",
    "type": "report",
    "id": "eJgWbXRz"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "2",
    "DOI": "10.1016/0893-6080(91)90009-t",
    "type": "article-journal",
    "page": "251-257",
    "source": "Crossref",
    "title": "Approximation capabilities of multilayer feedforward networks",
    "volume": "4",
    "author": [
      {
        "given": "Kurt",
        "family": "Hornik"
      }
    ],
    "container-title": "Neural Networks",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "URL": "https://doi.org/dzwxkd",
    "container-title-short": "Neural Networks",
    "id": "1BnILgle7"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "2",
    "DOI": "10.1016/s0933-3657(96)00367-3",
    "type": "article-journal",
    "page": "107-138",
    "source": "Crossref",
    "title": "An evaluation of machine-learning methods for predicting pneumonia mortality",
    "volume": "9",
    "author": [
      {
        "given": "Gregory F.",
        "family": "Cooper"
      },
      {
        "given": "Constantin F.",
        "family": "Aliferis"
      },
      {
        "given": "Richard",
        "family": "Ambrosino"
      },
      {
        "given": "John",
        "family": "Aronis"
      },
      {
        "given": "Bruce G.",
        "family": "Buchanan"
      },
      {
        "given": "Richard",
        "family": "Caruana"
      },
      {
        "given": "Michael J.",
        "family": "Fine"
      },
      {
        "given": "Clark",
        "family": "Glymour"
      },
      {
        "given": "Geoffrey",
        "family": "Gordon"
      },
      {
        "given": "Barbara H.",
        "family": "Hanusa"
      },
      {
        "given": "Janine E.",
        "family": "Janosky"
      },
      {
        "given": "Christopher",
        "family": "Meek"
      },
      {
        "given": "Tom",
        "family": "Mitchell"
      },
      {
        "given": "Thomas",
        "family": "Richardson"
      },
      {
        "given": "Peter",
        "family": "Spirtes"
      }
    ],
    "container-title": "Artificial Intelligence in Medicine",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1997,
          2
        ]
      ]
    },
    "URL": "https://doi.org/b6vnmd",
    "container-title-short": "Artificial Intelligence in Medicine",
    "id": "980FAm5x"
  },
  {
    "publisher": "American Chemical Society (ACS)",
    "issue": "12",
    "DOI": "10.1021/acs.molpharmaceut.7b00578",
    "type": "article-journal",
    "page": "4462-4475",
    "source": "Crossref",
    "title": "Comparison of Deep Learning With Multiple Machine Learning Methods and Metrics Using Diverse Drug Discovery Data Sets",
    "volume": "14",
    "author": [
      {
        "given": "Alexandru",
        "family": "Korotcov"
      },
      {
        "given": "Valery",
        "family": "Tkachenko"
      },
      {
        "given": "Daniel P.",
        "family": "Russo"
      },
      {
        "given": "Sean",
        "family": "Ekins"
      }
    ],
    "container-title": "Molecular Pharmaceutics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2017,
          11,
          13
        ]
      ]
    },
    "URL": "https://doi.org/gcj4p2",
    "container-title-short": "Mol. Pharmaceutics",
    "PMCID": "PMC5741413",
    "PMID": "29096442",
    "id": "rKXyJKNt"
  },
  {
    "publisher": "American Chemical Society (ACS)",
    "issue": "10",
    "DOI": "10.1021/acschembio.8b00881",
    "type": "article-journal",
    "page": "2819-2821",
    "source": "Crossref",
    "title": "Adversarial Controls for Scientific Machine Learning",
    "volume": "13",
    "author": [
      {
        "given": "Kangway V.",
        "family": "Chuang"
      },
      {
        "given": "Michael J.",
        "family": "Keiser"
      }
    ],
    "container-title": "ACS Chemical Biology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          19
        ]
      ]
    },
    "URL": "https://doi.org/gfk9mh",
    "container-title-short": "ACS Chem. Biol.",
    "PMID": "30336670",
    "id": "yqAEYaMg"
  },
  {
    "publisher": "Springer Nature",
    "issue": "10",
    "DOI": "10.1038/nrg2825",
    "type": "article-journal",
    "page": "733-739",
    "source": "Crossref",
    "title": "Tackling the widespread and critical impact of batch effects in high-throughput data",
    "volume": "11",
    "author": [
      {
        "given": "Jeffrey T.",
        "family": "Leek"
      },
      {
        "given": "Robert B.",
        "family": "Scharpf"
      },
      {
        "given": "Héctor Corrada",
        "family": "Bravo"
      },
      {
        "given": "David",
        "family": "Simcha"
      },
      {
        "given": "Benjamin",
        "family": "Langmead"
      },
      {
        "given": "W. Evan",
        "family": "Johnson"
      },
      {
        "given": "Donald",
        "family": "Geman"
      },
      {
        "given": "Keith",
        "family": "Baggerly"
      },
      {
        "given": "Rafael A.",
        "family": "Irizarry"
      }
    ],
    "container-title": "Nature Reviews Genetics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2010,
          9,
          14
        ]
      ]
    },
    "URL": "https://doi.org/cfr324",
    "container-title-short": "Nat Rev Genet",
    "PMCID": "PMC3880143",
    "PMID": "20838408",
    "id": "mPnIAH38"
  },
  {
    "publisher": "Springer Nature America, Inc",
    "issue": "1",
    "DOI": "10.1038/s41467-017-02388-1",
    "type": "article-journal",
    "source": "Crossref",
    "title": "VAMPnets for deep learning of molecular kinetics",
    "volume": "9",
    "author": [
      {
        "given": "Andreas",
        "family": "Mardt"
      },
      {
        "given": "Luca",
        "family": "Pasquali"
      },
      {
        "given": "Hao",
        "family": "Wu"
      },
      {
        "given": "Frank",
        "family": "Noé"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          1,
          2
        ]
      ]
    },
    "URL": "https://doi.org/gcvf62",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC5750224",
    "PMID": "29295994",
    "id": "lwg6sPLT"
  },
  {
    "publisher": "Springer Nature America, Inc",
    "issue": "1",
    "DOI": "10.1038/s41467-018-05378-z",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Deep learning to predict the lab-of-origin of engineered DNA",
    "volume": "9",
    "author": [
      {
        "given": "Alec A. K.",
        "family": "Nielsen"
      },
      {
        "given": "Christopher A.",
        "family": "Voigt"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          8,
          7
        ]
      ]
    },
    "URL": "https://doi.org/gd27sw",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC6081423",
    "PMID": "30087331",
    "id": "WGfstNkj"
  },
  {
    "publisher": "Springer Nature",
    "issue": "1",
    "DOI": "10.1038/s41746-018-0029-1",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Scalable and accurate deep learning with electronic health records",
    "volume": "1",
    "author": [
      {
        "given": "Alvin",
        "family": "Rajkomar"
      },
      {
        "given": "Eyal",
        "family": "Oren"
      },
      {
        "given": "Kai",
        "family": "Chen"
      },
      {
        "given": "Andrew M.",
        "family": "Dai"
      },
      {
        "given": "Nissan",
        "family": "Hajaj"
      },
      {
        "given": "Michaela",
        "family": "Hardt"
      },
      {
        "given": "Peter J.",
        "family": "Liu"
      },
      {
        "given": "Xiaobing",
        "family": "Liu"
      },
      {
        "given": "Jake",
        "family": "Marcus"
      },
      {
        "given": "Mimi",
        "family": "Sun"
      },
      {
        "given": "Patrik",
        "family": "Sundberg"
      },
      {
        "given": "Hector",
        "family": "Yee"
      },
      {
        "given": "Kun",
        "family": "Zhang"
      },
      {
        "given": "Yi",
        "family": "Zhang"
      },
      {
        "given": "Gerardo",
        "family": "Flores"
      },
      {
        "given": "Gavin E.",
        "family": "Duggan"
      },
      {
        "given": "Jamie",
        "family": "Irvine"
      },
      {
        "given": "Quoc",
        "family": "Le"
      },
      {
        "given": "Kurt",
        "family": "Litsch"
      },
      {
        "given": "Alexander",
        "family": "Mossin"
      },
      {
        "given": "Justin",
        "family": "Tansuwan"
      },
      {
        "given": "De",
        "family": "Wang"
      },
      {
        "given": "James",
        "family": "Wexler"
      },
      {
        "given": "Jimbo",
        "family": "Wilson"
      },
      {
        "given": "Dana",
        "family": "Ludwig"
      },
      {
        "given": "Samuel L.",
        "family": "Volchenboum"
      },
      {
        "given": "Katherine",
        "family": "Chou"
      },
      {
        "given": "Michael",
        "family": "Pearson"
      },
      {
        "given": "Srinivasan",
        "family": "Madabushi"
      },
      {
        "given": "Nigam H.",
        "family": "Shah"
      },
      {
        "given": "Atul J.",
        "family": "Butte"
      },
      {
        "given": "Michael D.",
        "family": "Howell"
      },
      {
        "given": "Claire",
        "family": "Cui"
      },
      {
        "given": "Greg S.",
        "family": "Corrado"
      },
      {
        "given": "Jeffrey",
        "family": "Dean"
      }
    ],
    "container-title": "npj Digital Medicine",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          5,
          8
        ]
      ]
    },
    "URL": "https://doi.org/gdqcc8",
    "container-title-short": "npj Digital Med",
    "id": "1DssZebFm"
  },
  {
    "publisher": "The Royal Society",
    "issue": "141",
    "DOI": "10.1098/rsif.2017.0387",
    "type": "article-journal",
    "page": "20170387",
    "source": "Crossref",
    "title": "Opportunities and obstacles for deep learning in biology and medicine",
    "volume": "15",
    "author": [
      {
        "given": "Travers",
        "family": "Ching"
      },
      {
        "given": "Daniel S.",
        "family": "Himmelstein"
      },
      {
        "given": "Brett K.",
        "family": "Beaulieu-Jones"
      },
      {
        "given": "Alexandr A.",
        "family": "Kalinin"
      },
      {
        "given": "Brian T.",
        "family": "Do"
      },
      {
        "given": "Gregory P.",
        "family": "Way"
      },
      {
        "given": "Enrico",
        "family": "Ferrero"
      },
      {
        "given": "Paul-Michael",
        "family": "Agapow"
      },
      {
        "given": "Michael",
        "family": "Zietz"
      },
      {
        "given": "Michael M.",
        "family": "Hoffman"
      },
      {
        "given": "Wei",
        "family": "Xie"
      },
      {
        "given": "Gail L.",
        "family": "Rosen"
      },
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Johnny",
        "family": "Israeli"
      },
      {
        "given": "Jack",
        "family": "Lanchantin"
      },
      {
        "given": "Stephen",
        "family": "Woloszynek"
      },
      {
        "given": "Anne E.",
        "family": "Carpenter"
      },
      {
        "given": "Avanti",
        "family": "Shrikumar"
      },
      {
        "given": "Jinbo",
        "family": "Xu"
      },
      {
        "given": "Evan M.",
        "family": "Cofer"
      },
      {
        "given": "Christopher A.",
        "family": "Lavender"
      },
      {
        "given": "Srinivas C.",
        "family": "Turaga"
      },
      {
        "given": "Amr M.",
        "family": "Alexandari"
      },
      {
        "given": "Zhiyong",
        "family": "Lu"
      },
      {
        "given": "David J.",
        "family": "Harris"
      },
      {
        "given": "Dave",
        "family": "DeCaprio"
      },
      {
        "given": "Yanjun",
        "family": "Qi"
      },
      {
        "given": "Anshul",
        "family": "Kundaje"
      },
      {
        "given": "Yifan",
        "family": "Peng"
      },
      {
        "given": "Laura K.",
        "family": "Wiley"
      },
      {
        "given": "Marwin H. S.",
        "family": "Segler"
      },
      {
        "given": "Simina M.",
        "family": "Boca"
      },
      {
        "given": "S. Joshua",
        "family": "Swamidass"
      },
      {
        "given": "Austin",
        "family": "Huang"
      },
      {
        "given": "Anthony",
        "family": "Gitter"
      },
      {
        "given": "Casey S.",
        "family": "Greene"
      }
    ],
    "container-title": "Journal of The Royal Society Interface",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          4
        ]
      ]
    },
    "URL": "https://doi.org/gddkhn",
    "container-title-short": "J. R. Soc. Interface",
    "PMCID": "PMC5938574",
    "PMID": "29618526",
    "id": "PZMP42Ak"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:p>Single-cell RNA sequencing (scRNA-seq) is a powerful tool to profile the transcriptomes of a large number of individual cells at a high resolution. These data usually contain measurements of gene expression for many genes in thousands or tens of thousands of cells, though some datasets now reach the million-cell mark. Projecting high-dimensional scRNA-seq data into a low dimensional space aids downstream analysis and data visualization. Many recent preprints accomplish this using variational autoencoders (VAE), generative models that learn underlying structure of data by compress it into a constrained, low dimensional space. The low dimensional spaces generated by VAEs have revealed complex patterns and novel biological signals from large-scale gene expression data and drug response predictions. Here, we evaluate a simple VAE approach for gene expression data, Tybalt, by training and measuring its performance on sets of simulated scRNA-seq data. We find a number of counter-intuitive performance features: i.e., deeper neural networks can struggle when datasets contain more observations under some parameter configurations. We show that these methods are highly sensitive to parameter tuning: when tuned, the performance of the Tybalt model, which was not optimized for scRNA-seq data, outperforms other popular dimension reduction approaches - PCA, ZIFA, UMAP and t-SNE. On the other hand, without tuning performance can also be remarkably poor on the same data. Our results should discourage authors and reviewers from relying on self-reported performance comparisons to evaluate the relative value of contributions in this area at this time. Instead, we recommend that attempts to compare or benchmark autoencoder methods for scRNA-seq data be performed by disinterested third parties or by methods developers only on unseen benchmark data that are provided to all participants simultaneously because the potential for performance differences due to unequal parameter tuning is so high.</jats:p>",
    "DOI": "10.1101/385534",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Parameter tuning is a key part of dimensionality reduction via deep variational autoencoders for single cell RNA transcriptomics",
    "author": [
      {
        "given": "Qiwen",
        "family": "Hu"
      },
      {
        "given": "Casey S",
        "family": "Greene"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          8,
          5
        ]
      ]
    },
    "URL": "https://doi.org/gdxxjf",
    "id": "5CsWRjfp"
  },
  {
    "publisher": "ACM Press",
    "DOI": "10.1145/2783258.2788613",
    "type": "paper-conference",
    "source": "Crossref",
    "title": "Intelligible Models for HealthCare",
    "author": [
      {
        "given": "Rich",
        "family": "Caruana"
      },
      {
        "given": "Yin",
        "family": "Lou"
      },
      {
        "given": "Johannes",
        "family": "Gehrke"
      },
      {
        "given": "Paul",
        "family": "Koch"
      },
      {
        "given": "Marc",
        "family": "Sturm"
      },
      {
        "given": "Noemie",
        "family": "Elhadad"
      }
    ],
    "event": "the 21th ACM SIGKDD International Conference",
    "container-title": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "URL": "https://doi.org/gftgxk",
    "id": "gSmt16Rh"
  },
  {
    "publisher": "ACM Press",
    "DOI": "10.1145/2810103.2813677",
    "type": "paper-conference",
    "source": "Crossref",
    "title": "Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures",
    "author": [
      {
        "given": "Matt",
        "family": "Fredrikson"
      },
      {
        "given": "Somesh",
        "family": "Jha"
      },
      {
        "given": "Thomas",
        "family": "Ristenpart"
      }
    ],
    "event": "the 22nd ACM SIGSAC Conference",
    "container-title": "Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security - CCS '15",
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "URL": "https://doi.org/cwdm",
    "id": "zCqhgXvY"
  },
  {
    "publisher": "ACM Press",
    "DOI": "10.1145/2976749.2978318",
    "type": "paper-conference",
    "source": "Crossref",
    "title": "Deep Learning with Differential Privacy",
    "author": [
      {
        "given": "Martin",
        "family": "Abadi"
      },
      {
        "given": "Andy",
        "family": "Chu"
      },
      {
        "given": "Ian",
        "family": "Goodfellow"
      },
      {
        "given": "H. Brendan",
        "family": "McMahan"
      },
      {
        "given": "Ilya",
        "family": "Mironov"
      },
      {
        "given": "Kunal",
        "family": "Talwar"
      },
      {
        "given": "Li",
        "family": "Zhang"
      }
    ],
    "event": "the 2016 ACM SIGSAC Conference",
    "container-title": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security - CCS'16",
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "URL": "https://doi.org/gcrnp3",
    "id": "LiCxcgZp"
  },
  {
    "publisher": "Springer Nature",
    "issue": "1",
    "DOI": "10.1186/s13040-017-0155-3",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Ten quick tips for machine learning in computational biology",
    "volume": "10",
    "author": [
      {
        "given": "Davide",
        "family": "Chicco"
      }
    ],
    "container-title": "BioData Mining",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2017,
          12
        ]
      ]
    },
    "URL": "https://doi.org/gdb9wr",
    "container-title-short": "BioData Mining",
    "PMCID": "PMC5721660",
    "PMID": "29234465",
    "id": "p4Nl5If0"
  },
  {
    "publisher": "Public Library of Science (PLoS)",
    "issue": "9",
    "DOI": "10.1371/journal.pcbi.1006454",
    "type": "article-journal",
    "page": "e1006454",
    "source": "Crossref",
    "title": "SIG-DB: Leveraging homomorphic encryption to securely interrogate privately held genomic databases",
    "volume": "14",
    "author": [
      {
        "given": "Alexander J.",
        "family": "Titus"
      },
      {
        "given": "Audrey",
        "family": "Flower"
      },
      {
        "given": "Patrick",
        "family": "Hagerty"
      },
      {
        "given": "Paul",
        "family": "Gamble"
      },
      {
        "given": "Charlie",
        "family": "Lewis"
      },
      {
        "given": "Todd",
        "family": "Stavish"
      },
      {
        "given": "Kevin P.",
        "family": "O’Connell"
      },
      {
        "given": "Greg",
        "family": "Shipley"
      },
      {
        "given": "Stephanie M.",
        "family": "Rogers"
      }
    ],
    "container-title": "PLOS Computational Biology",
    "language": "en",
    "editor": [
      {
        "given": "Scott",
        "family": "Markel"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          9,
          4
        ]
      ]
    },
    "URL": "https://doi.org/gd6xd5",
    "container-title-short": "PLoS Comput Biol",
    "PMCID": "PMC6138421",
    "PMID": "30180163",
    "id": "me326jb9"
  },
  {
    "title": "The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets.",
    "volume": "10",
    "issue": "3",
    "page": "e0118432",
    "container-title": "PloS one",
    "container-title-short": "PLoS ONE",
    "ISSN": "1932-6203",
    "issued": {
      "date-parts": [
        [
          2015,
          3,
          4
        ]
      ]
    },
    "author": [
      {
        "given": "Takaya",
        "family": "Saito"
      },
      {
        "given": "Marc",
        "family": "Rehmsmeier"
      }
    ],
    "PMID": "25738806",
    "PMCID": "PMC4349800",
    "DOI": "10.1371/journal.pone.0118432",
    "abstract": "Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets. ",
    "URL": "https://www.ncbi.nlm.nih.gov/pubmed/25738806",
    "type": "article-journal",
    "id": "u86hHJ9b"
  },
  {
    "type": "paper-conference",
    "title": "A Simple Weight Decay Can Improve Generalization",
    "container-title": "Proceedings of the 4th International Conference on Neural Information Processing Systems",
    "collection-title": "NIPS'91",
    "publisher": "Morgan Kaufmann Publishers Inc.",
    "publisher-place": "San Francisco, CA, USA",
    "page": "950–957",
    "source": "ACM Digital Library",
    "event-place": "San Francisco, CA, USA",
    "abstract": "It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.",
    "URL": "http://dl.acm.org/citation.cfm?id=2986916.2987033",
    "ISBN": "9781558602229",
    "author": [
      {
        "family": "Krogh",
        "given": "Anders"
      },
      {
        "family": "Hertz",
        "given": "John A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1991"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          2,
          8
        ]
      ]
    },
    "id": "eR3C2hhK"
  },
  {
    "type": "article-journal",
    "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    "container-title": "Journal of Machine Learning Research",
    "page": "1929-1958",
    "volume": "15",
    "source": "Journal of Machine Learning Research",
    "abstract": "Deep neural nets with a large number of parameters are very\npowerful machine learning systems. However, overfitting is a\nserious problem in such networks. Large networks are also slow\nto use, making it difficult to deal with overfitting by\ncombining the predictions of many different large neural nets at\ntest time. Dropout is a technique for addressing this problem.\nThe key idea is to randomly drop units (along with their\nconnections) from the neural network during training. This\nprevents units from co-adapting too much. During training,\ndropout samples from an exponential number of different\n“thinned” networks. At test time, it is easy to approximate the\neffect of averaging the predictions of all these thinned\nnetworks by simply using a single unthinned network that has\nsmaller weights. This significantly reduces overfitting and\ngives major improvements over other regularization methods. We\nshow that dropout improves the performance of neural networks on\nsupervised learning tasks in vision, speech recognition,\ndocument classification and computational biology, obtaining\nstate-of-the-art results on many benchmark data sets.",
    "URL": "http://jmlr.org/papers/v15/srivastava14a.html",
    "shortTitle": "Dropout",
    "author": [
      {
        "family": "Srivastava",
        "given": "Nitish"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      },
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Salakhutdinov",
        "given": "Ruslan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          2,
          8
        ]
      ]
    },
    "id": "R1RpVu06"
  },
  {
    "type": "book",
    "title": "Ten Quick Tips for Deep Learning in Biology. Contribute to Benjamin-Lee/deep-rules development by creating an account on GitHub",
    "genre": "Jupyter Notebook",
    "source": "GitHub",
    "URL": "https://github.com/Benjamin-Lee/deep-rules",
    "note": "original-date: 2018-10-09T02:10:24Z",
    "author": [
      {
        "family": "Lee",
        "given": "Benjamin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2019",
          2,
          8
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          2,
          8
        ]
      ]
    },
    "id": "ysdRl4lj"
  },
  {
    "type": "webpage",
    "title": "Open collaborative writing with Manubot",
    "URL": "https://greenelab.github.io/meta-review/",
    "language": "en-US",
    "author": [
      {
        "family": "Himmelstein",
        "given": "Daniel S."
      },
      {
        "family": "Slochower",
        "given": "David R."
      },
      {
        "family": "Malladi",
        "given": "Venkat S."
      },
      {
        "family": "Greene",
        "given": "Casey S."
      },
      {
        "family": "Gitter",
        "given": "Anthony"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2019",
          2,
          8
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2019",
          2,
          8
        ]
      ]
    },
    "id": "1GGGHdsew"
  }
]
