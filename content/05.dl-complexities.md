## Tip 3: Understand the complexities of training deep neural networks {#complexities}

Correctly training deep neural networks is a non-trivial process.
There are many different options and potential pitfalls at every stage.
To get good results you have to expect to train many networks with different tweaks and modifications.
Despite improved framework ease-of-use and on-demand cloud computing resources this means DL can be very demanding, requiring significant infrastructure and patience to complete.
The experimentation inherent to DL is often noisy (requiring repetition) and represents a significant organizational challenge.
All code, parameters, and results must be carefully corralled using good coding practices (for example, version control, continuous integration etc.) in order to be effective and interpretable.
This organization is also key to being able to efficiently share your work and to update your model as new data becomes available. 


Similar to [Tip 4](#baselines), try to start with a relatively smaller network and increase the size and complexity as needed to prevent wasting time and resources. 
Beware of the seemingly trivial choices that are being made implicitly by default settings in your framework of choice e.g. choice of optimization algorithm (adaptive methods often lead to faster convergence during training but may lead to worse generalization performance on independent datasets [@url:https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning]).
These need to be carefully considered and their impacts evaluated (see [Tip 6](#hyperparameters)).
