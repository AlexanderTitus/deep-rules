## Rule 10: Don't share models trained on sensitive data

One of the greatest opportunities for deep learning in biology is the high dimensional nature of much of the biological data collected today. This abundance of features for each sample means that the models can learn an enormous amount about the data during the training process, and therefore it is important not to share models trained on sensitive data. Applying deep learning to images of cats from the internet does not pose a significant privacy problems, but in the field of human health, this starts to approach privacy concerns. Adversarial training techniques, such as model inversion attacks, can be used to exploit model predictions to recover recognizable images of people's faces used for training [@doi:10.1145/2810103.2813677]. These risks are higher in deep learning compared to traditional machine learning techniques because of the greater representational capacity of the models. This enhanced modeling capacity allows the model to learn more robust and nuanced features of specific data, leading to the chance of revealing the underlying sensitive data. When you train deep learning models on sensitive data, be sure not to share the model weights directly, and use techniques such as differential privacy [@doi:10.1145/2976749.2978318, @arXiv:1811.07216] and homomorphic encryption [@doi:10.1371/journal.pcbi.1006454, @arXiv:1811.00778]  to protect sensitive data.

