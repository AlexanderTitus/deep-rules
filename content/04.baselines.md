## Tip 2: Use traditional methods to establish performance baselines {#baselines}

Since deep learning requires practitioners to consider a larger number and variety of tuning parameters compared to more traditional methods, it is easy to fall into the trap of performing an unnecessarily convoluted analysis. 
Hence, before applying deep learning to a given problem, we highly recommend implementing a simple model at the beginning of each study, to establish adequate performance baselines.
More concretely, a researcher may wish to implement and fit a logistic regression model using the same software framework planned for DL first to evaluate the classification performance.
Establishing such a performance baseline may help researchers with assessing the complexity of the task at hand as well as potential technical challenges surrounding data processing-related steps, which can prove useful for implementing more complex DL architectures later on.
The utility of this approach is evidenced by the recent development of hybrid models that combine DL and simpler models to improve generalization performance, model interpretability, and confidence estimation [@arxiv:1803.04765; @arxiv:1805.11783].
Depending on the amount and the nature of the available data, as well as the task to be performed, deep learning may not always be able to outperform conventional methods.
As an illustration, Rajkomar et al. [@doi:10.1038/s41746-018-0029-1] found that simpler baseline models achieved performance comparable with that of DL in a number of clinical prediction tasks using electronic health records, which may be a surprise to many. 
Another example is provided by Koutsoukas et al., who benchmarked several traditional machine learning approaches against deep neural networks for modeling bioactivity data and found that while well-tuned deep learning approaches tend to outperform conventional classifiers on a moderately sized dataset, simple methods such as Naive Bayes classification tend to outperform deep learning as the noise in the dataset increases [@doi:10.1186/s13321-017-0226-y].

It is worth noting that conventional off-the-shelf machine learning algorithms (e.g., support vector machines and random forests) are also likely to benefit from parameter tuning.
It can be tempting to train baseline models with these conventional methods using default parameters, which may provide acceptable but not stellar performance, but then tune the settings for DL algorithms to further optimize performance.
Hu and Greene [@doi:10.1101/385534] discuss a "Continental Breakfast Included" effect by which unequal hyperparameter tuning for different learning algorithms skews the evaluation of these methods, especially when the performance of an algorithm varies substantially with modest changes to its hyperparameters.
Those wishing to compare of different learning algorithms should tune the parameters of traditional and DL-based methods for predictive performance optimization before making claims about relative performance differences.
The performance comparison among DL models and many other ML approaches is informative only when the models are similarly well-tuned.
