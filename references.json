[
  {
    "id": "UeE0s74F",
    "URL": "https://arxiv.org/abs/1509.09292",
    "number": "1509.09292",
    "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints",
    "issued": {
      "date-parts": [
        [
          2015,
          11,
          4
        ]
      ]
    },
    "author": [
      {
        "given": "David",
        "family": "Duvenaud"
      },
      {
        "given": "Dougal",
        "family": "Maclaurin"
      },
      {
        "given": "Jorge",
        "family": "Aguilera-Iparraguirre"
      },
      {
        "given": "Rafael",
        "family": "Gómez-Bombarelli"
      },
      {
        "given": "Timothy",
        "family": "Hirzel"
      },
      {
        "given": "Alán",
        "family": "Aspuru-Guzik"
      },
      {
        "given": "Ryan P.",
        "family": "Adams"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks. ",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1509.09292"
  },
  {
    "id": "iAeJlSAZ",
    "URL": "https://arxiv.org/abs/1511.06348",
    "number": "1511.06348",
    "title": "How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?",
    "issued": {
      "date-parts": [
        [
          2016,
          1,
          11
        ]
      ]
    },
    "author": [
      {
        "given": "Junghwan",
        "family": "Cho"
      },
      {
        "given": "Kyewook",
        "family": "Lee"
      },
      {
        "given": "Ellie",
        "family": "Shin"
      },
      {
        "given": "Garry",
        "family": "Choy"
      },
      {
        "given": "Synho",
        "family": "Do"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1511.06348"
  },
  {
    "id": "hOeUlCvS",
    "URL": "https://arxiv.org/abs/1603.04467",
    "number": "1603.04467",
    "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
    "issued": {
      "date-parts": [
        [
          2016,
          3,
          17
        ]
      ]
    },
    "author": [
      {
        "given": "Martín",
        "family": "Abadi"
      },
      {
        "given": "Ashish",
        "family": "Agarwal"
      },
      {
        "given": "Paul",
        "family": "Barham"
      },
      {
        "given": "Eugene",
        "family": "Brevdo"
      },
      {
        "given": "Zhifeng",
        "family": "Chen"
      },
      {
        "given": "Craig",
        "family": "Citro"
      },
      {
        "given": "Greg S.",
        "family": "Corrado"
      },
      {
        "given": "Andy",
        "family": "Davis"
      },
      {
        "given": "Jeffrey",
        "family": "Dean"
      },
      {
        "given": "Matthieu",
        "family": "Devin"
      },
      {
        "given": "Sanjay",
        "family": "Ghemawat"
      },
      {
        "given": "Ian",
        "family": "Goodfellow"
      },
      {
        "given": "Andrew",
        "family": "Harp"
      },
      {
        "given": "Geoffrey",
        "family": "Irving"
      },
      {
        "given": "Michael",
        "family": "Isard"
      },
      {
        "given": "Yangqing",
        "family": "Jia"
      },
      {
        "given": "Rafal",
        "family": "Jozefowicz"
      },
      {
        "given": "Lukasz",
        "family": "Kaiser"
      },
      {
        "given": "Manjunath",
        "family": "Kudlur"
      },
      {
        "given": "Josh",
        "family": "Levenberg"
      },
      {
        "given": "Dan",
        "family": "Mane"
      },
      {
        "given": "Rajat",
        "family": "Monga"
      },
      {
        "given": "Sherry",
        "family": "Moore"
      },
      {
        "given": "Derek",
        "family": "Murray"
      },
      {
        "given": "Chris",
        "family": "Olah"
      },
      {
        "given": "Mike",
        "family": "Schuster"
      },
      {
        "given": "Jonathon",
        "family": "Shlens"
      },
      {
        "given": "Benoit",
        "family": "Steiner"
      },
      {
        "given": "Ilya",
        "family": "Sutskever"
      },
      {
        "given": "Kunal",
        "family": "Talwar"
      },
      {
        "given": "Paul",
        "family": "Tucker"
      },
      {
        "given": "Vincent",
        "family": "Vanhoucke"
      },
      {
        "given": "Vijay",
        "family": "Vasudevan"
      },
      {
        "given": "Fernanda",
        "family": "Viegas"
      },
      {
        "given": "Oriol",
        "family": "Vinyals"
      },
      {
        "given": "Pete",
        "family": "Warden"
      },
      {
        "given": "Martin",
        "family": "Wattenberg"
      },
      {
        "given": "Martin",
        "family": "Wicke"
      },
      {
        "given": "Yuan",
        "family": "Yu"
      },
      {
        "given": "Xiaoqiang",
        "family": "Zheng"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1603.04467"
  },
  {
    "id": "15N3fu3hC",
    "URL": "https://arxiv.org/abs/1605.07723v3",
    "number": "1605.07723v3",
    "version": "v3",
    "title": "Data Programming: Creating Large Training Sets, Quickly",
    "issued": {
      "date-parts": [
        [
          2016,
          5,
          25
        ]
      ]
    },
    "author": [
      {
        "literal": "Alexander Ratner"
      },
      {
        "literal": "Christopher De Sa"
      },
      {
        "literal": "Sen Wu"
      },
      {
        "literal": "Daniel Selsam"
      },
      {
        "literal": "Christopher Ré"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can \"denoise\" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1605.07723v3"
  },
  {
    "id": "aqgi0yxG",
    "URL": "https://arxiv.org/abs/1803.01271",
    "number": "1803.01271",
    "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
    "issued": {
      "date-parts": [
        [
          2018,
          4,
          20
        ]
      ]
    },
    "author": [
      {
        "given": "Shaojie",
        "family": "Bai"
      },
      {
        "given": "J. Zico",
        "family": "Kolter"
      },
      {
        "given": "Vladlen",
        "family": "Koltun"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN . ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1803.01271"
  },
  {
    "id": "uBcf6TJ2",
    "URL": "https://arxiv.org/abs/1803.04765",
    "number": "1803.04765",
    "title": "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",
    "issued": {
      "date-parts": [
        [
          2018,
          3,
          14
        ]
      ]
    },
    "author": [
      {
        "given": "Nicolas",
        "family": "Papernot"
      },
      {
        "given": "Patrick",
        "family": "McDaniel"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1803.04765"
  },
  {
    "id": "2bsGpiQt",
    "URL": "https://arxiv.org/abs/1805.11783",
    "number": "1805.11783",
    "title": "To Trust Or Not To Trust A Classifier",
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          30
        ]
      ]
    },
    "author": [
      {
        "given": "Heinrich",
        "family": "Jiang"
      },
      {
        "given": "Been",
        "family": "Kim"
      },
      {
        "given": "Melody Y.",
        "family": "Guan"
      },
      {
        "given": "Maya",
        "family": "Gupta"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1805.11783"
  },
  {
    "id": "ra8TSEHY",
    "URL": "https://arxiv.org/abs/1806.10282",
    "number": "1806.10282",
    "title": "Auto-Keras: An Efficient Neural Architecture Search System",
    "issued": {
      "date-parts": [
        [
          2019,
          3,
          27
        ]
      ]
    },
    "author": [
      {
        "given": "Haifeng",
        "family": "Jin"
      },
      {
        "given": "Qingquan",
        "family": "Song"
      },
      {
        "given": "Xia",
        "family": "Hu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1806.10282"
  },
  {
    "id": "E2JcoiqW",
    "URL": "https://arxiv.org/abs/1809.00238",
    "number": "1809.00238",
    "title": "A Machine Learning Driven IoT Solution for Noise Classification in Smart Cities",
    "issued": {
      "date-parts": [
        [
          2018,
          9,
          5
        ]
      ]
    },
    "author": [
      {
        "given": "Yasser",
        "family": "Alsouda"
      },
      {
        "given": "Sabri",
        "family": "Pllana"
      },
      {
        "given": "Arianit",
        "family": "Kurti"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  We present a machine learning based method for noise classification using a low-power and inexpensive IoT unit. We use Mel-frequency cepstral coefficients for audio feature extraction and supervised classification algorithms (that is, support vector machine and k-nearest neighbors) for noise classification. We evaluate our approach experimentally with a dataset of about 3000 sound samples grouped in eight sound classes (such as, car horn, jackhammer, or street music). We explore the parameter space of support vector machine and k-nearest neighbors algorithms to estimate the optimal parameter values for classification of sound samples in the dataset under study. We achieve a noise classification accuracy in the range 85% -- 100%. Training and testing of our k-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less than a second for a dataset with features of more than 3000 sound samples. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1809.00238"
  },
  {
    "id": "Tx4vUlOa",
    "URL": "https://arxiv.org/abs/1810.08055",
    "number": "1810.08055",
    "title": "Ten Simple Rules for Reproducible Research in Jupyter Notebooks",
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          19
        ]
      ]
    },
    "author": [
      {
        "given": "Adam",
        "family": "Rule"
      },
      {
        "given": "Amanda",
        "family": "Birmingham"
      },
      {
        "given": "Cristal",
        "family": "Zuniga"
      },
      {
        "given": "Ilkay",
        "family": "Altintas"
      },
      {
        "given": "Shih-Cheng",
        "family": "Huang"
      },
      {
        "given": "Rob",
        "family": "Knight"
      },
      {
        "given": "Niema",
        "family": "Moshiri"
      },
      {
        "given": "Mai H.",
        "family": "Nguyen"
      },
      {
        "given": "Sara Brin",
        "family": "Rosenthal"
      },
      {
        "given": "Fernando",
        "family": "Pérez"
      },
      {
        "given": "Peter W.",
        "family": "Rose"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Reproducibility of computational studies is a hallmark of scientific methodology. It enables researchers to build with confidence on the methods and findings of others, reuse and extend computational pipelines, and thereby drive scientific progress. Since many experimental studies rely on computational analyses, biologists need guidance on how to set up and document reproducible data analyses or simulations.\n  In this paper, we address several questions about reproducibility. For example, what are the technical and non-technical barriers to reproducible computational studies? What opportunities and challenges do computational notebooks offer to overcome some of these barriers? What tools are available and how can they be used effectively?\n  We have developed a set of rules to serve as a guide to scientists with a specific focus on computational notebook systems, such as Jupyter Notebooks, which have become a tool of choice for many applications. Notebooks combine detailed workflows with narrative text and visualization of results. Combined with software repositories and open source licensing, notebooks are powerful tools for transparent, collaborative, reproducible, and reusable data analyses. ",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1810.08055"
  },
  {
    "id": "3326vtLW",
    "URL": "https://arxiv.org/abs/1811.00778",
    "number": "1811.00778",
    "title": "Towards the AlexNet Moment for Homomorphic Encryption: HCNN, theFirst Homomorphic CNN on Encrypted Data with GPUs",
    "issued": {
      "date-parts": [
        [
          2020,
          8,
          20
        ]
      ]
    },
    "author": [
      {
        "given": "Ahmad Al",
        "family": "Badawi"
      },
      {
        "given": "Jin",
        "family": "Chao"
      },
      {
        "given": "Jie",
        "family": "Lin"
      },
      {
        "given": "Chan Fook",
        "family": "Mun"
      },
      {
        "given": "Jun Jie",
        "family": "Sim"
      },
      {
        "given": "Benjamin Hong Meng",
        "family": "Tan"
      },
      {
        "given": "Xiao",
        "family": "Nan"
      },
      {
        "given": "Khin Mi Mi",
        "family": "Aung"
      },
      {
        "given": "Vijay Ramaseshan",
        "family": "Chandrasekhar"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep Learning as a Service (DLaaS) stands as a promising solution for cloud-based inference applications. In this setting, the cloud has a pre-learned model whereas the user has samples on which she wants to run the model. The biggest concern with DLaaS is user privacy if the input samples are sensitive data. We provide here an efficient privacy-preserving system by employing high-end technologies such as Fully Homomorphic Encryption (FHE), Convolutional Neural Networks (CNNs) and Graphics Processing Units (GPUs). FHE, with its widely-known feature of computing on encrypted data, empowers a wide range of privacy-concerned applications. This comes at high cost as it requires enormous computing power. In this paper, we show how to accelerate the performance of running CNNs on encrypted data with GPUs. We evaluated two CNNs to classify homomorphically the MNIST and CIFAR-10 datasets. Our solution achieved a sufficient security level (> 80 bit) and reasonable classification accuracy (99%) and (77.55%) for MNIST and CIFAR-10, respectively. In terms of latency, we could classify an image in 5.16 seconds and 304.43 seconds for MNIST and CIFAR-10, respectively. Our system can also classify a batch of images (> 8,000) without extra overhead. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1811.00778"
  },
  {
    "id": "1HuQe3Z8X",
    "URL": "https://arxiv.org/abs/1811.04017",
    "number": "1811.04017",
    "title": "A generic framework for privacy preserving deep learning",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          14
        ]
      ]
    },
    "author": [
      {
        "given": "Theo",
        "family": "Ryffel"
      },
      {
        "given": "Andrew",
        "family": "Trask"
      },
      {
        "given": "Morten",
        "family": "Dahl"
      },
      {
        "given": "Bobby",
        "family": "Wagner"
      },
      {
        "given": "Jason",
        "family": "Mancuso"
      },
      {
        "given": "Daniel",
        "family": "Rueckert"
      },
      {
        "given": "Jonathan",
        "family": "Passerat-Palmbach"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1811.04017"
  },
  {
    "id": "1CDx6NYSj",
    "URL": "https://arxiv.org/abs/1811.12808",
    "number": "1811.12808",
    "title": "Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning",
    "issued": {
      "date-parts": [
        [
          2018,
          12,
          4
        ]
      ]
    },
    "author": [
      {
        "given": "Sebastian",
        "family": "Raschka"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1811.12808"
  },
  {
    "id": "eJgWbXRz",
    "URL": "https://arxiv.org/abs/1812.01484",
    "number": "1812.01484",
    "title": "Privacy-Preserving Distributed Deep Learning for Clinical Data",
    "issued": {
      "date-parts": [
        [
          2018,
          12,
          5
        ]
      ]
    },
    "author": [
      {
        "given": "Brett K.",
        "family": "Beaulieu-Jones"
      },
      {
        "given": "William",
        "family": "Yuan"
      },
      {
        "given": "Samuel G.",
        "family": "Finlayson"
      },
      {
        "given": "Zhiwei Steven",
        "family": "Wu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep learning with medical data often requires larger samples sizes than are available at single providers. While data sharing among institutions is desirable to train more accurate and sophisticated models, it can lead to severe privacy concerns due the sensitive nature of the data. This problem has motivated a number of studies on distributed training of neural networks that do not require direct sharing of the training data. However, simple distributed training does not offer provable privacy guarantees to satisfy technical safe standards and may reveal information about the underlying patients. We present a method to train neural networks for clinical data in a distributed fashion under differential privacy. We demonstrate these methods on two datasets that include information from multiple independent sites, the eICU collaborative Research Database and The Cancer Genome Atlas. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1812.01484"
  },
  {
    "id": "1CnZlKOVj",
    "URL": "https://arxiv.org/abs/1906.02243",
    "number": "1906.02243",
    "title": "Energy and Policy Considerations for Deep Learning in NLP",
    "issued": {
      "date-parts": [
        [
          2019,
          6,
          7
        ]
      ]
    },
    "author": [
      {
        "given": "Emma",
        "family": "Strubell"
      },
      {
        "given": "Ananya",
        "family": "Ganesh"
      },
      {
        "given": "Andrew",
        "family": "McCallum"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1906.02243"
  },
  {
    "id": "iTP4h1rX",
    "URL": "https://arxiv.org/abs/1912.01703",
    "number": "1912.01703",
    "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
    "issued": {
      "date-parts": [
        [
          2019,
          12,
          5
        ]
      ]
    },
    "author": [
      {
        "given": "Adam",
        "family": "Paszke"
      },
      {
        "given": "Sam",
        "family": "Gross"
      },
      {
        "given": "Francisco",
        "family": "Massa"
      },
      {
        "given": "Adam",
        "family": "Lerer"
      },
      {
        "given": "James",
        "family": "Bradbury"
      },
      {
        "given": "Gregory",
        "family": "Chanan"
      },
      {
        "given": "Trevor",
        "family": "Killeen"
      },
      {
        "given": "Zeming",
        "family": "Lin"
      },
      {
        "given": "Natalia",
        "family": "Gimelshein"
      },
      {
        "given": "Luca",
        "family": "Antiga"
      },
      {
        "given": "Alban",
        "family": "Desmaison"
      },
      {
        "given": "Andreas",
        "family": "Köpf"
      },
      {
        "given": "Edward",
        "family": "Yang"
      },
      {
        "given": "Zach",
        "family": "DeVito"
      },
      {
        "given": "Martin",
        "family": "Raison"
      },
      {
        "given": "Alykhan",
        "family": "Tejani"
      },
      {
        "given": "Sasank",
        "family": "Chilamkurthy"
      },
      {
        "given": "Benoit",
        "family": "Steiner"
      },
      {
        "given": "Lu",
        "family": "Fang"
      },
      {
        "given": "Junjie",
        "family": "Bai"
      },
      {
        "given": "Soumith",
        "family": "Chintala"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\n  In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\n  We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:1912.01703"
  },
  {
    "id": "cRG2FGOV",
    "URL": "https://arxiv.org/abs/2001.02522",
    "number": "2001.02522",
    "title": "On Interpretability of Artificial Neural Networks",
    "issued": {
      "date-parts": [
        [
          2020,
          1,
          9
        ]
      ]
    },
    "author": [
      {
        "given": "Fenglei",
        "family": "Fan"
      },
      {
        "given": "Jinjun",
        "family": "Xiong"
      },
      {
        "given": "Ge",
        "family": "Wang"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep learning has achieved great successes in many important areas to dealing with text, images, video, graphs, and so on. However, the black-box nature of deep artificial neural networks has become the primary obstacle to their public acceptance and wide popularity in critical applications such as diagnosis and therapy. Due to the huge potential of deep learning, interpreting neural networks has become one of the most critical research directions. In this paper, we systematically review recent studies in understanding the mechanism of neural networks and shed light on some future directions of interpretability research (This work is still in progress). ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:2001.02522"
  },
  {
    "id": "bYOaJHMe",
    "URL": "https://arxiv.org/abs/2005.14165",
    "number": "2005.14165",
    "title": "Language Models are Few-Shot Learners",
    "issued": {
      "date-parts": [
        [
          2020,
          7,
          24
        ]
      ]
    },
    "author": [
      {
        "given": "Tom B.",
        "family": "Brown"
      },
      {
        "given": "Benjamin",
        "family": "Mann"
      },
      {
        "given": "Nick",
        "family": "Ryder"
      },
      {
        "given": "Melanie",
        "family": "Subbiah"
      },
      {
        "given": "Jared",
        "family": "Kaplan"
      },
      {
        "given": "Prafulla",
        "family": "Dhariwal"
      },
      {
        "given": "Arvind",
        "family": "Neelakantan"
      },
      {
        "given": "Pranav",
        "family": "Shyam"
      },
      {
        "given": "Girish",
        "family": "Sastry"
      },
      {
        "given": "Amanda",
        "family": "Askell"
      },
      {
        "given": "Sandhini",
        "family": "Agarwal"
      },
      {
        "given": "Ariel",
        "family": "Herbert-Voss"
      },
      {
        "given": "Gretchen",
        "family": "Krueger"
      },
      {
        "given": "Tom",
        "family": "Henighan"
      },
      {
        "given": "Rewon",
        "family": "Child"
      },
      {
        "given": "Aditya",
        "family": "Ramesh"
      },
      {
        "given": "Daniel M.",
        "family": "Ziegler"
      },
      {
        "given": "Jeffrey",
        "family": "Wu"
      },
      {
        "given": "Clemens",
        "family": "Winter"
      },
      {
        "given": "Christopher",
        "family": "Hesse"
      },
      {
        "given": "Mark",
        "family": "Chen"
      },
      {
        "given": "Eric",
        "family": "Sigler"
      },
      {
        "given": "Mateusz",
        "family": "Litwin"
      },
      {
        "given": "Scott",
        "family": "Gray"
      },
      {
        "given": "Benjamin",
        "family": "Chess"
      },
      {
        "given": "Jack",
        "family": "Clark"
      },
      {
        "given": "Christopher",
        "family": "Berner"
      },
      {
        "given": "Sam",
        "family": "McCandlish"
      },
      {
        "given": "Alec",
        "family": "Radford"
      },
      {
        "given": "Ilya",
        "family": "Sutskever"
      },
      {
        "given": "Dario",
        "family": "Amodei"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: arxiv:2005.14165"
  },
  {
    "type": "chapter",
    "id": "12tC5JTV6",
    "author": [
      {
        "family": "Olson",
        "given": "Randal S."
      },
      {
        "family": "Urbanowicz",
        "given": "Ryan J."
      },
      {
        "family": "Andrews",
        "given": "Peter C."
      },
      {
        "family": "Lavender",
        "given": "Nicole A."
      },
      {
        "family": "Kidd",
        "given": "La Creis"
      },
      {
        "family": "Moore",
        "given": "Jason H."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "container-title": "Lecture Notes in Computer Science",
    "DOI": "10.1007/978-3-319-31204-0_9",
    "publisher": "Springer International Publishing",
    "title": "Automating Biomedical Data Science Through Tree-Based Pipeline Optimization",
    "URL": "https://doi.org/ggfptv",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1007/978-3-319-31204-0_9"
  },
  {
    "type": "book",
    "id": "JT3rHKc7",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "container-title": "Lecture Notes in Computer Science",
    "DOI": "10.1007/978-3-642-35289-8",
    "volume": "7700",
    "publisher": "Springer Berlin Heidelberg",
    "title": "Neural Networks: Tricks of the Trade",
    "URL": "https://doi.org/gfvtvt",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1007/978-3-642-35289-8"
  },
  {
    "type": "article-journal",
    "id": "xwsS0Nlg",
    "author": [
      {
        "family": "Cybenko",
        "given": "G."
      }
    ],
    "issued": {
      "date-parts": [
        [
          1989,
          12
        ]
      ]
    },
    "container-title": "Mathematics of Control, Signals, and Systems",
    "DOI": "10.1007/bf02551274",
    "volume": "2",
    "issue": "4",
    "page": "303-314",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Approximation by superpositions of a sigmoidal function",
    "URL": "https://doi.org/dp3968",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1007/bf02551274"
  },
  {
    "type": "article-journal",
    "id": "x6HXFAS4",
    "author": [
      {
        "family": "Rajkomar",
        "given": "Alvin"
      },
      {
        "family": "Lingam",
        "given": "Sneha"
      },
      {
        "family": "Taylor",
        "given": "Andrew G."
      },
      {
        "family": "Blum",
        "given": "Michael"
      },
      {
        "family": "Mongan",
        "given": "John"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016,
          10,
          11
        ]
      ]
    },
    "container-title": "Journal of Digital Imaging",
    "DOI": "10.1007/s10278-016-9914-9",
    "volume": "30",
    "issue": "1",
    "page": "95-101",
    "publisher": "Springer Science and Business Media LLC",
    "title": "High-Throughput Classification of Radiographs Using Deep Convolutional Neural Networks",
    "URL": "https://doi.org/gcgk7v",
    "PMCID": "PMC5267603",
    "PMID": "27730417",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1007/s10278-016-9914-9"
  },
  {
    "type": "article-journal",
    "id": "cBVeXnZx",
    "author": [
      {
        "family": "Russakovsky",
        "given": "Olga"
      },
      {
        "family": "Deng",
        "given": "Jia"
      },
      {
        "family": "Su",
        "given": "Hao"
      },
      {
        "family": "Krause",
        "given": "Jonathan"
      },
      {
        "family": "Satheesh",
        "given": "Sanjeev"
      },
      {
        "family": "Ma",
        "given": "Sean"
      },
      {
        "family": "Huang",
        "given": "Zhiheng"
      },
      {
        "family": "Karpathy",
        "given": "Andrej"
      },
      {
        "family": "Khosla",
        "given": "Aditya"
      },
      {
        "family": "Bernstein",
        "given": "Michael"
      },
      {
        "family": "Berg",
        "given": "Alexander C."
      },
      {
        "family": "Fei-Fei",
        "given": "Li"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2015,
          4,
          11
        ]
      ]
    },
    "container-title": "International Journal of Computer Vision",
    "DOI": "10.1007/s11263-015-0816-y",
    "volume": "115",
    "issue": "3",
    "page": "211-252",
    "publisher": "Springer Science and Business Media LLC",
    "title": "ImageNet Large Scale Visual Recognition Challenge",
    "URL": "https://doi.org/gcgk7w",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1007/s11263-015-0816-y"
  },
  {
    "type": "article-journal",
    "id": "1BnILgle7",
    "author": [
      {
        "family": "Hornik",
        "given": "Kurt"
      }
    ],
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "container-title": "Neural Networks",
    "DOI": "10.1016/0893-6080(91)90009-t",
    "volume": "4",
    "issue": "2",
    "page": "251-257",
    "publisher": "Elsevier BV",
    "title": "Approximation capabilities of multilayer feedforward networks",
    "URL": "https://doi.org/dzwxkd",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1016/0893-6080(91)90009-t"
  },
  {
    "type": "article-journal",
    "id": "980FAm5x",
    "author": [
      {
        "family": "Cooper",
        "given": "Gregory F."
      },
      {
        "family": "Aliferis",
        "given": "Constantin F."
      },
      {
        "family": "Ambrosino",
        "given": "Richard"
      },
      {
        "family": "Aronis",
        "given": "John"
      },
      {
        "family": "Buchanan",
        "given": "Bruce G."
      },
      {
        "family": "Caruana",
        "given": "Richard"
      },
      {
        "family": "Fine",
        "given": "Michael J."
      },
      {
        "family": "Glymour",
        "given": "Clark"
      },
      {
        "family": "Gordon",
        "given": "Geoffrey"
      },
      {
        "family": "Hanusa",
        "given": "Barbara H."
      },
      {
        "family": "Janosky",
        "given": "Janine E."
      },
      {
        "family": "Meek",
        "given": "Christopher"
      },
      {
        "family": "Mitchell",
        "given": "Tom"
      },
      {
        "family": "Richardson",
        "given": "Thomas"
      },
      {
        "family": "Spirtes",
        "given": "Peter"
      }
    ],
    "issued": {
      "date-parts": [
        [
          1997,
          2
        ]
      ]
    },
    "container-title": "Artificial Intelligence in Medicine",
    "DOI": "10.1016/s0933-3657(96)00367-3",
    "volume": "9",
    "issue": "2",
    "page": "107-138",
    "publisher": "Elsevier BV",
    "title": "An evaluation of machine-learning methods for predicting pneumonia mortality",
    "URL": "https://doi.org/b6vnmd",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1016/s0933-3657(96)00367-3"
  },
  {
    "type": "article-journal",
    "id": "rKXyJKNt",
    "author": [
      {
        "family": "Korotcov",
        "given": "Alexandru"
      },
      {
        "family": "Tkachenko",
        "given": "Valery"
      },
      {
        "family": "Russo",
        "given": "Daniel P."
      },
      {
        "family": "Ekins",
        "given": "Sean"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          11,
          13
        ]
      ]
    },
    "container-title": "Molecular Pharmaceutics",
    "DOI": "10.1021/acs.molpharmaceut.7b00578",
    "volume": "14",
    "issue": "12",
    "page": "4462-4475",
    "publisher": "American Chemical Society (ACS)",
    "title": "Comparison of Deep Learning With Multiple Machine Learning Methods and Metrics Using Diverse Drug Discovery Data Sets",
    "URL": "https://doi.org/gcj4p2",
    "PMCID": "PMC5741413",
    "PMID": "29096442",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1021/acs.molpharmaceut.7b00578"
  },
  {
    "type": "article-journal",
    "id": "yqAEYaMg",
    "author": [
      {
        "family": "Chuang",
        "given": "Kangway V."
      },
      {
        "family": "Keiser",
        "given": "Michael J."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          19
        ]
      ]
    },
    "container-title": "ACS Chemical Biology",
    "DOI": "10.1021/acschembio.8b00881",
    "volume": "13",
    "issue": "10",
    "page": "2819-2821",
    "publisher": "American Chemical Society (ACS)",
    "title": "Adversarial Controls for Scientific Machine Learning",
    "URL": "https://doi.org/gfk9mh",
    "PMID": "30336670",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1021/acschembio.8b00881"
  },
  {
    "type": "article-journal",
    "id": "YuxbleXb",
    "author": [
      {
        "family": "Brazma",
        "given": "Alvis"
      },
      {
        "family": "Hingamp",
        "given": "Pascal"
      },
      {
        "family": "Quackenbush",
        "given": "John"
      },
      {
        "family": "Sherlock",
        "given": "Gavin"
      },
      {
        "family": "Spellman",
        "given": "Paul"
      },
      {
        "family": "Stoeckert",
        "given": "Chris"
      },
      {
        "family": "Aach",
        "given": "John"
      },
      {
        "family": "Ansorge",
        "given": "Wilhelm"
      },
      {
        "family": "Ball",
        "given": "Catherine A."
      },
      {
        "family": "Causton",
        "given": "Helen C."
      },
      {
        "family": "Gaasterland",
        "given": "Terry"
      },
      {
        "family": "Glenisson",
        "given": "Patrick"
      },
      {
        "family": "Holstege",
        "given": "Frank C.P."
      },
      {
        "family": "Kim",
        "given": "Irene F."
      },
      {
        "family": "Markowitz",
        "given": "Victor"
      },
      {
        "family": "Matese",
        "given": "John C."
      },
      {
        "family": "Parkinson",
        "given": "Helen"
      },
      {
        "family": "Robinson",
        "given": "Alan"
      },
      {
        "family": "Sarkans",
        "given": "Ugis"
      },
      {
        "family": "Schulze-Kremer",
        "given": "Steffen"
      },
      {
        "family": "Stewart",
        "given": "Jason"
      },
      {
        "family": "Taylor",
        "given": "Ronald"
      },
      {
        "family": "Vilo",
        "given": "Jaak"
      },
      {
        "family": "Vingron",
        "given": "Martin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2001,
          12
        ]
      ]
    },
    "container-title": "Nature Genetics",
    "DOI": "10.1038/ng1201-365",
    "volume": "29",
    "issue": "4",
    "page": "365-371",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Minimum information about a microarray experiment (MIAME)—toward standards for microarray data",
    "URL": "https://doi.org/ck257n",
    "PMID": "11726920",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1038/ng1201-365"
  },
  {
    "type": "article-journal",
    "id": "mPnIAH38",
    "author": [
      {
        "family": "Leek",
        "given": "Jeffrey T."
      },
      {
        "family": "Scharpf",
        "given": "Robert B."
      },
      {
        "family": "Bravo",
        "given": "Héctor Corrada"
      },
      {
        "family": "Simcha",
        "given": "David"
      },
      {
        "family": "Langmead",
        "given": "Benjamin"
      },
      {
        "family": "Johnson",
        "given": "W. Evan"
      },
      {
        "family": "Geman",
        "given": "Donald"
      },
      {
        "family": "Baggerly",
        "given": "Keith"
      },
      {
        "family": "Irizarry",
        "given": "Rafael A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2010,
          9,
          14
        ]
      ]
    },
    "container-title": "Nature Reviews Genetics",
    "DOI": "10.1038/nrg2825",
    "volume": "11",
    "issue": "10",
    "page": "733-739",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Tackling the widespread and critical impact of batch effects in high-throughput data",
    "URL": "https://doi.org/cfr324",
    "PMCID": "PMC3880143",
    "PMID": "20838408",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1038/nrg2825"
  },
  {
    "type": "article-journal",
    "id": "lwg6sPLT",
    "author": [
      {
        "family": "Mardt",
        "given": "Andreas"
      },
      {
        "family": "Pasquali",
        "given": "Luca"
      },
      {
        "family": "Wu",
        "given": "Hao"
      },
      {
        "family": "Noé",
        "given": "Frank"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          1,
          2
        ]
      ]
    },
    "container-title": "Nature Communications",
    "DOI": "10.1038/s41467-017-02388-1",
    "volume": "9",
    "issue": "1",
    "publisher": "Springer Science and Business Media LLC",
    "title": "VAMPnets for deep learning of molecular kinetics",
    "URL": "https://doi.org/gcvf62",
    "PMCID": "PMC5750224",
    "PMID": "29295994",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41467-017-02388-1"
  },
  {
    "type": "article-journal",
    "id": "WGfstNkj",
    "author": [
      {
        "family": "Nielsen",
        "given": "Alec A. K."
      },
      {
        "family": "Voigt",
        "given": "Christopher A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          8,
          7
        ]
      ]
    },
    "container-title": "Nature Communications",
    "DOI": "10.1038/s41467-018-05378-z",
    "volume": "9",
    "issue": "1",
    "publisher": "(:unav)",
    "title": "Deep learning to predict the lab-of-origin of engineered DNA",
    "URL": "https://doi.org/gd27sw",
    "PMCID": "PMC6081423",
    "PMID": "30087331",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41467-018-05378-z"
  },
  {
    "type": "article-journal",
    "id": "GdO9NZJH",
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          10
        ]
      ]
    },
    "container-title": "Nature Biomedical Engineering",
    "DOI": "10.1038/s41551-018-0315-x",
    "volume": "2",
    "issue": "10",
    "page": "709-710",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Towards trustable machine learning",
    "URL": "https://doi.org/gfw9cn",
    "PMID": "31015650",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41551-018-0315-x"
  },
  {
    "type": "article-journal",
    "id": "VpgPDZxv",
    "author": [
      {
        "family": "Byrd",
        "given": "James Brian"
      },
      {
        "family": "Greene",
        "given": "Anna C."
      },
      {
        "family": "Prasad",
        "given": "Deepashree Venkatesh"
      },
      {
        "family": "Jiang",
        "given": "Xiaoqian"
      },
      {
        "family": "Greene",
        "given": "Casey S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          7,
          21
        ]
      ]
    },
    "container-title": "Nature Reviews Genetics",
    "DOI": "10.1038/s41576-020-0257-5",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Responsible, practical genomic data sharing that accelerates research",
    "URL": "https://doi.org/gg7c57",
    "PMID": "32694666",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41576-020-0257-5"
  },
  {
    "type": "article-journal",
    "id": "8vmpDcPH",
    "author": [
      {
        "family": "Gurovich",
        "given": "Yaron"
      },
      {
        "family": "Hanani",
        "given": "Yair"
      },
      {
        "family": "Bar",
        "given": "Omri"
      },
      {
        "family": "Nadav",
        "given": "Guy"
      },
      {
        "family": "Fleischer",
        "given": "Nicole"
      },
      {
        "family": "Gelbman",
        "given": "Dekel"
      },
      {
        "family": "Basel-Salmon",
        "given": "Lina"
      },
      {
        "family": "Krawitz",
        "given": "Peter M."
      },
      {
        "family": "Kamphausen",
        "given": "Susanne B."
      },
      {
        "family": "Zenker",
        "given": "Martin"
      },
      {
        "family": "Bird",
        "given": "Lynne M."
      },
      {
        "family": "Gripp",
        "given": "Karen W."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          1,
          7
        ]
      ]
    },
    "container-title": "Nature Medicine",
    "DOI": "10.1038/s41591-018-0279-0",
    "volume": "25",
    "issue": "1",
    "page": "60-64",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Identifying facial phenotypes of genetic disorders using deep learning",
    "URL": "https://doi.org/czdm",
    "PMID": "30617323",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41591-018-0279-0"
  },
  {
    "type": "article-journal",
    "id": "1DssZebFm",
    "author": [
      {
        "family": "Rajkomar",
        "given": "Alvin"
      },
      {
        "family": "Oren",
        "given": "Eyal"
      },
      {
        "family": "Chen",
        "given": "Kai"
      },
      {
        "family": "Dai",
        "given": "Andrew M."
      },
      {
        "family": "Hajaj",
        "given": "Nissan"
      },
      {
        "family": "Hardt",
        "given": "Michaela"
      },
      {
        "family": "Liu",
        "given": "Peter J."
      },
      {
        "family": "Liu",
        "given": "Xiaobing"
      },
      {
        "family": "Marcus",
        "given": "Jake"
      },
      {
        "family": "Sun",
        "given": "Mimi"
      },
      {
        "family": "Sundberg",
        "given": "Patrik"
      },
      {
        "family": "Yee",
        "given": "Hector"
      },
      {
        "family": "Zhang",
        "given": "Kun"
      },
      {
        "family": "Zhang",
        "given": "Yi"
      },
      {
        "family": "Flores",
        "given": "Gerardo"
      },
      {
        "family": "Duggan",
        "given": "Gavin E."
      },
      {
        "family": "Irvine",
        "given": "Jamie"
      },
      {
        "family": "Le",
        "given": "Quoc"
      },
      {
        "family": "Litsch",
        "given": "Kurt"
      },
      {
        "family": "Mossin",
        "given": "Alexander"
      },
      {
        "family": "Tansuwan",
        "given": "Justin"
      },
      {
        "family": "Wang",
        "given": "De"
      },
      {
        "family": "Wexler",
        "given": "James"
      },
      {
        "family": "Wilson",
        "given": "Jimbo"
      },
      {
        "family": "Ludwig",
        "given": "Dana"
      },
      {
        "family": "Volchenboum",
        "given": "Samuel L."
      },
      {
        "family": "Chou",
        "given": "Katherine"
      },
      {
        "family": "Pearson",
        "given": "Michael"
      },
      {
        "family": "Madabushi",
        "given": "Srinivasan"
      },
      {
        "family": "Shah",
        "given": "Nigam H."
      },
      {
        "family": "Butte",
        "given": "Atul J."
      },
      {
        "family": "Howell",
        "given": "Michael D."
      },
      {
        "family": "Cui",
        "given": "Claire"
      },
      {
        "family": "Corrado",
        "given": "Greg S."
      },
      {
        "family": "Dean",
        "given": "Jeffrey"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          5,
          8
        ]
      ]
    },
    "container-title": "npj Digital Medicine",
    "DOI": "10.1038/s41746-018-0029-1",
    "volume": "1",
    "issue": "1",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Scalable and accurate deep learning with electronic health records",
    "URL": "https://doi.org/gdqcc8",
    "PMCID": "PMC6550175",
    "PMID": "31304302",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41746-018-0029-1"
  },
  {
    "type": "article-journal",
    "id": "lvjgHDOe",
    "author": [
      {
        "family": "Chen",
        "given": "David"
      },
      {
        "family": "Liu",
        "given": "Sijia"
      },
      {
        "family": "Kingsbury",
        "given": "Paul"
      },
      {
        "family": "Sohn",
        "given": "Sunghwan"
      },
      {
        "family": "Storlie",
        "given": "Curtis B."
      },
      {
        "family": "Habermann",
        "given": "Elizabeth B."
      },
      {
        "family": "Naessens",
        "given": "James M."
      },
      {
        "family": "Larson",
        "given": "David W."
      },
      {
        "family": "Liu",
        "given": "Hongfang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          5,
          30
        ]
      ]
    },
    "container-title": "npj Digital Medicine",
    "DOI": "10.1038/s41746-019-0122-0",
    "volume": "2",
    "issue": "1",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Deep learning and alternative learning strategies for retrospective real-world clinical data",
    "URL": "https://doi.org/ghfwhh",
    "PMCID": "PMC6550223",
    "PMID": "31304389",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41746-019-0122-0"
  },
  {
    "type": "article-journal",
    "id": "PZMP42Ak",
    "author": [
      {
        "family": "Ching",
        "given": "Travers"
      },
      {
        "family": "Himmelstein",
        "given": "Daniel S."
      },
      {
        "family": "Beaulieu-Jones",
        "given": "Brett K."
      },
      {
        "family": "Kalinin",
        "given": "Alexandr A."
      },
      {
        "family": "Do",
        "given": "Brian T."
      },
      {
        "family": "Way",
        "given": "Gregory P."
      },
      {
        "family": "Ferrero",
        "given": "Enrico"
      },
      {
        "family": "Agapow",
        "given": "Paul-Michael"
      },
      {
        "family": "Zietz",
        "given": "Michael"
      },
      {
        "family": "Hoffman",
        "given": "Michael M."
      },
      {
        "family": "Xie",
        "given": "Wei"
      },
      {
        "family": "Rosen",
        "given": "Gail L."
      },
      {
        "family": "Lengerich",
        "given": "Benjamin J."
      },
      {
        "family": "Israeli",
        "given": "Johnny"
      },
      {
        "family": "Lanchantin",
        "given": "Jack"
      },
      {
        "family": "Woloszynek",
        "given": "Stephen"
      },
      {
        "family": "Carpenter",
        "given": "Anne E."
      },
      {
        "family": "Shrikumar",
        "given": "Avanti"
      },
      {
        "family": "Xu",
        "given": "Jinbo"
      },
      {
        "family": "Cofer",
        "given": "Evan M."
      },
      {
        "family": "Lavender",
        "given": "Christopher A."
      },
      {
        "family": "Turaga",
        "given": "Srinivas C."
      },
      {
        "family": "Alexandari",
        "given": "Amr M."
      },
      {
        "family": "Lu",
        "given": "Zhiyong"
      },
      {
        "family": "Harris",
        "given": "David J."
      },
      {
        "family": "DeCaprio",
        "given": "Dave"
      },
      {
        "family": "Qi",
        "given": "Yanjun"
      },
      {
        "family": "Kundaje",
        "given": "Anshul"
      },
      {
        "family": "Peng",
        "given": "Yifan"
      },
      {
        "family": "Wiley",
        "given": "Laura K."
      },
      {
        "family": "Segler",
        "given": "Marwin H. S."
      },
      {
        "family": "Boca",
        "given": "Simina M."
      },
      {
        "family": "Swamidass",
        "given": "S. Joshua"
      },
      {
        "family": "Huang",
        "given": "Austin"
      },
      {
        "family": "Gitter",
        "given": "Anthony"
      },
      {
        "family": "Greene",
        "given": "Casey S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          4,
          4
        ]
      ]
    },
    "container-title": "Journal of The Royal Society Interface",
    "DOI": "10.1098/rsif.2017.0387",
    "volume": "15",
    "issue": "141",
    "page": "20170387",
    "publisher": "The Royal Society",
    "title": "Opportunities and obstacles for deep learning in biology and medicine",
    "URL": "https://doi.org/gddkhn",
    "PMCID": "PMC5938574",
    "PMID": "29618526",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1098/rsif.2017.0387"
  },
  {
    "type": "article-journal",
    "id": "fbIH12yd",
    "author": [
      {
        "family": "Beaulieu-Jones",
        "given": "Brett K."
      },
      {
        "family": "Wu",
        "given": "Zhiwei Steven"
      },
      {
        "family": "Williams",
        "given": "Chris"
      },
      {
        "family": "Lee",
        "given": "Ran"
      },
      {
        "family": "Bhavnani",
        "given": "Sanjeev P."
      },
      {
        "family": "Byrd",
        "given": "James Brian"
      },
      {
        "family": "Greene",
        "given": "Casey S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          12,
          20
        ]
      ]
    },
    "container-title": "bioRxiv",
    "DOI": "10.1101/159756",
    "publisher": "bioRxiv",
    "title": "Privacy-preserving generative deep neural networks support clinical data sharing",
    "URL": "https://doi.org/gcnzrn",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1101/159756"
  },
  {
    "type": "article-journal",
    "id": "14cVrrqP1",
    "author": [
      {
        "family": "Avsec",
        "given": "Žiga"
      },
      {
        "family": "Kreuzhuber",
        "given": "Roman"
      },
      {
        "family": "Israeli",
        "given": "Johnny"
      },
      {
        "family": "Xu",
        "given": "Nancy"
      },
      {
        "family": "Cheng",
        "given": "Jun"
      },
      {
        "family": "Shrikumar",
        "given": "Avanti"
      },
      {
        "family": "Banerjee",
        "given": "Abhimanyu"
      },
      {
        "family": "Kim",
        "given": "Daniel S."
      },
      {
        "family": "Urban",
        "given": "Lara"
      },
      {
        "family": "Kundaje",
        "given": "Anshul"
      },
      {
        "family": "Stegle",
        "given": "Oliver"
      },
      {
        "family": "Gagneur",
        "given": "Julien"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          7,
          24
        ]
      ]
    },
    "abstract": "Advanced machine learning models applied to large-scale genomics datasets hold the promise to be major drivers for genome science. Once trained, such models can serve as a tool to probe the relationships between data modalities, including the effect of genetic variants on phenotype. However, lack of standardization and limited accessibility of trained models have hampered their impact in practice. To address this, we present Kipoi, a collaborative initiative to define standards and to foster reuse of trained models in genomics. Already, the Kipoi repository contains over 2,000 trained models that cover canonical prediction tasks in transcriptional and post-transcriptional gene regulation. The Kipoi model standard grants automated software installation and provides unified interfaces to apply and interpret models. We illustrate Kipoi through canonical use cases, including model benchmarking, transfer learning, variant effect prediction, and building new models from existing ones. By providing a unified framework to archive, share, access, use, and build on models developed by the community, Kipoi will foster the dissemination and use of machine learning models in genomics.",
    "container-title": "bioRxiv",
    "DOI": "10.1101/375345",
    "publisher": "bioRxiv",
    "title": "Kipoi: accelerating the community exchange and reuse of predictive models for genomics",
    "URL": "https://doi.org/gd24sx",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1101/375345"
  },
  {
    "type": "article-journal",
    "id": "x7a5SM90",
    "author": [
      {
        "family": "Razavian",
        "given": "Ali Sharif"
      },
      {
        "family": "Azizpour",
        "given": "Hossein"
      },
      {
        "family": "Sullivan",
        "given": "Josephine"
      },
      {
        "family": "Carlsson",
        "given": "Stefan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2014,
          6
        ]
      ]
    },
    "container-title": "Institute of Electrical and Electronics Engineers (IEEE)",
    "DOI": "10.1109/cvprw.2014.131",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "title": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition",
    "URL": "https://doi.org/f3np4s",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1109/cvprw.2014.131"
  },
  {
    "type": "article-journal",
    "id": "8seWxxzY",
    "author": [
      {
        "family": "Ravi",
        "given": "Daniele"
      },
      {
        "family": "Wong",
        "given": "Charence"
      },
      {
        "family": "Deligianni",
        "given": "Fani"
      },
      {
        "family": "Berthelot",
        "given": "Melissa"
      },
      {
        "family": "Andreu-Perez",
        "given": "Javier"
      },
      {
        "family": "Lo",
        "given": "Benny"
      },
      {
        "family": "Yang",
        "given": "Guang-Zhong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          1
        ]
      ]
    },
    "container-title": "IEEE Journal of Biomedical and Health Informatics",
    "DOI": "10.1109/jbhi.2016.2636665",
    "volume": "21",
    "issue": "1",
    "page": "4-21",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "title": "Deep Learning for Health Informatics",
    "URL": "https://doi.org/gfgtzx",
    "PMID": "28055930",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1109/jbhi.2016.2636665"
  },
  {
    "type": "article-journal",
    "id": "L7EocHX2",
    "author": [
      {
        "family": "Sze",
        "given": "Vivienne"
      },
      {
        "family": "Chen",
        "given": "Yu-Hsin"
      },
      {
        "family": "Yang",
        "given": "Tien-Ju"
      },
      {
        "family": "Emer",
        "given": "Joel S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          12
        ]
      ]
    },
    "container-title": "Proceedings of the IEEE",
    "DOI": "10.1109/jproc.2017.2761740",
    "volume": "105",
    "issue": "12",
    "page": "2295-2329",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey",
    "URL": "https://doi.org/gcnp38",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1109/jproc.2017.2761740"
  },
  {
    "type": "article-journal",
    "id": "ZwUaSNWa",
    "author": [
      {
        "family": "Zhang",
        "given": "Wenlu"
      },
      {
        "family": "Li",
        "given": "Rongjian"
      },
      {
        "family": "Zeng",
        "given": "Tao"
      },
      {
        "family": "Sun",
        "given": "Qian"
      },
      {
        "family": "Kumar",
        "given": "Sudhir"
      },
      {
        "family": "Ye",
        "given": "Jieping"
      },
      {
        "family": "Ji",
        "given": "Shuiwang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          6,
          1
        ]
      ]
    },
    "container-title": "IEEE Transactions on Big Data",
    "DOI": "10.1109/tbdata.2016.2573280",
    "volume": "6",
    "issue": "2",
    "page": "322-333",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "title": "Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis",
    "URL": "https://doi.org/gfvs28",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1109/tbdata.2016.2573280"
  },
  {
    "type": "article-journal",
    "id": "lBFmt4aO",
    "author": [
      {
        "family": "Ferreira",
        "given": "André C."
      },
      {
        "family": "Silva",
        "given": "Liliana R."
      },
      {
        "family": "Renna",
        "given": "Francesco"
      },
      {
        "family": "Brandl",
        "given": "Hanja B."
      },
      {
        "family": "Renoult",
        "given": "Julien P."
      },
      {
        "family": "Farine",
        "given": "Damien R."
      },
      {
        "family": "Covas",
        "given": "Rita"
      },
      {
        "family": "Doutrelant",
        "given": "Claire"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          7,
          26
        ]
      ]
    },
    "container-title": "Methods in Ecology and Evolution",
    "DOI": "10.1111/2041-210x.13436",
    "publisher": "Wiley",
    "title": "Deep learning‐based methods for individual recognition in small birds",
    "URL": "https://doi.org/d438",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1111/2041-210x.13436"
  },
  {
    "type": "article-journal",
    "id": "gTcMnARc",
    "author": [
      {
        "family": "Hu",
        "given": "Qiwen"
      },
      {
        "family": "Greene",
        "given": "Casey S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          11
        ]
      ]
    },
    "container-title": "World Scientific Pub Co Pte Lt",
    "DOI": "10.1142/9789813279827_0033",
    "publisher": "World Scientific Pub Co Pte Lt",
    "title": "Parameter tuning is a key part of dimensionality reduction via deep variational autoencoders for single cell RNA transcriptomics",
    "URL": "https://doi.org/gf5pc7",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1142/9789813279827_0033"
  },
  {
    "type": "article-journal",
    "id": "gSmt16Rh",
    "author": [
      {
        "family": "Caruana",
        "given": "Rich"
      },
      {
        "family": "Lou",
        "given": "Yin"
      },
      {
        "family": "Gehrke",
        "given": "Johannes"
      },
      {
        "family": "Koch",
        "given": "Paul"
      },
      {
        "family": "Sturm",
        "given": "Marc"
      },
      {
        "family": "Elhadad",
        "given": "Noemie"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "container-title": "Association for Computing Machinery (ACM)",
    "DOI": "10.1145/2783258.2788613",
    "publisher": "Association for Computing Machinery (ACM)",
    "title": "Intelligible Models for HealthCare",
    "URL": "https://doi.org/gftgxk",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1145/2783258.2788613"
  },
  {
    "type": "article-journal",
    "id": "zCqhgXvY",
    "author": [
      {
        "family": "Fredrikson",
        "given": "Matt"
      },
      {
        "family": "Jha",
        "given": "Somesh"
      },
      {
        "family": "Ristenpart",
        "given": "Thomas"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2015
        ]
      ]
    },
    "container-title": "Association for Computing Machinery (ACM)",
    "DOI": "10.1145/2810103.2813677",
    "publisher": "Association for Computing Machinery (ACM)",
    "title": "Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures",
    "URL": "https://doi.org/cwdm",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1145/2810103.2813677"
  },
  {
    "type": "article-journal",
    "id": "LiCxcgZp",
    "author": [
      {
        "family": "Abadi",
        "given": "Martin"
      },
      {
        "family": "Chu",
        "given": "Andy"
      },
      {
        "family": "Goodfellow",
        "given": "Ian"
      },
      {
        "family": "McMahan",
        "given": "H. Brendan"
      },
      {
        "family": "Mironov",
        "given": "Ilya"
      },
      {
        "family": "Talwar",
        "given": "Kunal"
      },
      {
        "family": "Zhang",
        "given": "Li"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "container-title": "Association for Computing Machinery (ACM)",
    "DOI": "10.1145/2976749.2978318",
    "publisher": "Association for Computing Machinery (ACM)",
    "title": "Deep Learning with Differential Privacy",
    "URL": "https://doi.org/gcrnp3",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1145/2976749.2978318"
  },
  {
    "type": "article-journal",
    "id": "fVSo2gZU",
    "author": [
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey E."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          5,
          24
        ]
      ]
    },
    "container-title": "Communications of the ACM",
    "DOI": "10.1145/3065386",
    "volume": "60",
    "issue": "6",
    "page": "84-90",
    "publisher": "Association for Computing Machinery (ACM)",
    "title": "ImageNet classification with deep convolutional neural networks",
    "URL": "https://doi.org/gbhhxs",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1145/3065386"
  },
  {
    "type": "article-journal",
    "id": "hJQdIoO3",
    "author": [
      {
        "family": "Dietterich",
        "given": "Thomas G."
      }
    ],
    "issued": {
      "date-parts": [
        [
          1998,
          10
        ]
      ]
    },
    "container-title": "Neural Computation",
    "DOI": "10.1162/089976698300017197",
    "volume": "10",
    "issue": "7",
    "page": "1895-1923",
    "publisher": "MIT Press - Journals",
    "title": "Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms",
    "URL": "https://doi.org/fqc9w5",
    "PMID": "9744903",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1162/089976698300017197"
  },
  {
    "type": "article-journal",
    "id": "p4Nl5If0",
    "author": [
      {
        "family": "Chicco",
        "given": "Davide"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          12,
          8
        ]
      ]
    },
    "container-title": "BioData Mining",
    "DOI": "10.1186/s13040-017-0155-3",
    "volume": "10",
    "issue": "1",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Ten quick tips for machine learning in computational biology",
    "URL": "https://doi.org/gdb9wr",
    "PMCID": "PMC5721660",
    "PMID": "29234465",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1186/s13040-017-0155-3"
  },
  {
    "type": "article-journal",
    "id": "19zfIm033",
    "author": [
      {
        "family": "Koutsoukas",
        "given": "Alexios"
      },
      {
        "family": "Monaghan",
        "given": "Keith J."
      },
      {
        "family": "Li",
        "given": "Xiaoli"
      },
      {
        "family": "Huan",
        "given": "Jun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          6,
          28
        ]
      ]
    },
    "container-title": "Journal of Cheminformatics",
    "DOI": "10.1186/s13321-017-0226-y",
    "volume": "9",
    "issue": "1",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Deep-learning: investigating deep neural networks hyper-parameters and comparison of performance to shallow methods for modeling bioactivity data",
    "URL": "https://doi.org/gfwv4d",
    "PMCID": "PMC5489441",
    "PMID": "29086090",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1186/s13321-017-0226-y"
  },
  {
    "type": "article-journal",
    "id": "Pf3steOn",
    "author": [
      {
        "family": "Sandve",
        "given": "Geir Kjetil"
      },
      {
        "family": "Nekrutenko",
        "given": "Anton"
      },
      {
        "family": "Taylor",
        "given": "James"
      },
      {
        "family": "Hovig",
        "given": "Eivind"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2013,
          10,
          24
        ]
      ]
    },
    "container-title": "PLoS Computational Biology",
    "DOI": "10.1371/journal.pcbi.1003285",
    "volume": "9",
    "issue": "10",
    "page": "e1003285",
    "publisher": "Public Library of Science (PLoS)",
    "title": "Ten Simple Rules for Reproducible Computational Research",
    "URL": "https://doi.org/pjb",
    "PMCID": "PMC3812051",
    "PMID": "24204232",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pcbi.1003285"
  },
  {
    "type": "article-journal",
    "id": "kEX5dgzK",
    "author": [
      {
        "family": "Perez-Riverol",
        "given": "Yasset"
      },
      {
        "family": "Gatto",
        "given": "Laurent"
      },
      {
        "family": "Wang",
        "given": "Rui"
      },
      {
        "family": "Sachsenberg",
        "given": "Timo"
      },
      {
        "family": "Uszkoreit",
        "given": "Julian"
      },
      {
        "family": "Leprevost",
        "given": "Felipe da Veiga"
      },
      {
        "family": "Fufezan",
        "given": "Christian"
      },
      {
        "family": "Ternent",
        "given": "Tobias"
      },
      {
        "family": "Eglen",
        "given": "Stephen J."
      },
      {
        "family": "Katz",
        "given": "Daniel S."
      },
      {
        "family": "Pollard",
        "given": "Tom J."
      },
      {
        "family": "Konovalov",
        "given": "Alexander"
      },
      {
        "family": "Flight",
        "given": "Robert M."
      },
      {
        "family": "Blin",
        "given": "Kai"
      },
      {
        "family": "Vizcaíno",
        "given": "Juan Antonio"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016,
          7,
          14
        ]
      ]
    },
    "container-title": "PLOS Computational Biology",
    "DOI": "10.1371/journal.pcbi.1004947",
    "volume": "12",
    "issue": "7",
    "page": "e1004947",
    "publisher": "Public Library of Science (PLoS)",
    "title": "Ten Simple Rules for Taking Advantage of Git and GitHub",
    "URL": "https://doi.org/gbrb39",
    "PMCID": "PMC4945047",
    "PMID": "27415786",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pcbi.1004947"
  },
  {
    "type": "article-journal",
    "id": "uXPlMpfq",
    "author": [
      {
        "family": "Zook",
        "given": "Matthew"
      },
      {
        "family": "Barocas",
        "given": "Solon"
      },
      {
        "family": "boyd",
        "given": "danah"
      },
      {
        "family": "Crawford",
        "given": "Kate"
      },
      {
        "family": "Keller",
        "given": "Emily"
      },
      {
        "family": "Gangadharan",
        "given": "Seeta Peña"
      },
      {
        "family": "Goodman",
        "given": "Alyssa"
      },
      {
        "family": "Hollander",
        "given": "Rachelle"
      },
      {
        "family": "Koenig",
        "given": "Barbara A."
      },
      {
        "family": "Metcalf",
        "given": "Jacob"
      },
      {
        "family": "Narayanan",
        "given": "Arvind"
      },
      {
        "family": "Nelson",
        "given": "Alondra"
      },
      {
        "family": "Pasquale",
        "given": "Frank"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          3,
          30
        ]
      ]
    },
    "container-title": "PLOS Computational Biology",
    "DOI": "10.1371/journal.pcbi.1005399",
    "volume": "13",
    "issue": "3",
    "page": "e1005399",
    "publisher": "Public Library of Science (PLoS)",
    "title": "Ten simple rules for responsible big data research",
    "URL": "https://doi.org/gdqfcn",
    "PMCID": "PMC5373508",
    "PMID": "28358831",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pcbi.1005399"
  },
  {
    "type": "article-journal",
    "id": "me326jb9",
    "author": [
      {
        "family": "Titus",
        "given": "Alexander J."
      },
      {
        "family": "Flower",
        "given": "Audrey"
      },
      {
        "family": "Hagerty",
        "given": "Patrick"
      },
      {
        "family": "Gamble",
        "given": "Paul"
      },
      {
        "family": "Lewis",
        "given": "Charlie"
      },
      {
        "family": "Stavish",
        "given": "Todd"
      },
      {
        "family": "O’Connell",
        "given": "Kevin P."
      },
      {
        "family": "Shipley",
        "given": "Greg"
      },
      {
        "family": "Rogers",
        "given": "Stephanie M."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          9,
          4
        ]
      ]
    },
    "container-title": "PLOS Computational Biology",
    "DOI": "10.1371/journal.pcbi.1006454",
    "volume": "14",
    "issue": "9",
    "page": "e1006454",
    "publisher": "Public Library of Science (PLoS)",
    "title": "SIG-DB: Leveraging homomorphic encryption to securely interrogate privately held genomic databases",
    "URL": "https://doi.org/gd6xd5",
    "PMCID": "PMC6138421",
    "PMID": "30180163",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pcbi.1006454"
  },
  {
    "type": "article-journal",
    "id": "NDyhvXoh",
    "author": [
      {
        "family": "Zech",
        "given": "John R."
      },
      {
        "family": "Badgeley",
        "given": "Marcus A."
      },
      {
        "family": "Liu",
        "given": "Manway"
      },
      {
        "family": "Costa",
        "given": "Anthony B."
      },
      {
        "family": "Titano",
        "given": "Joseph J."
      },
      {
        "family": "Oermann",
        "given": "Eric Karl"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          6
        ]
      ]
    },
    "container-title": "PLOS Medicine",
    "DOI": "10.1371/journal.pmed.1002683",
    "volume": "15",
    "issue": "11",
    "page": "e1002683",
    "publisher": "Public Library of Science (PLoS)",
    "title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
    "URL": "https://doi.org/gfj53h",
    "PMCID": "PMC6219764",
    "PMID": "30399157",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pmed.1002683"
  },
  {
    "title": "The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets.",
    "volume": "10",
    "issue": "3",
    "page": "e0118432",
    "container-title": "PloS one",
    "container-title-short": "PLoS One",
    "ISSN": "1932-6203",
    "issued": {
      "date-parts": [
        [
          2015,
          3,
          4
        ]
      ]
    },
    "author": [
      {
        "given": "Takaya",
        "family": "Saito"
      },
      {
        "given": "Marc",
        "family": "Rehmsmeier"
      }
    ],
    "PMID": "25738806",
    "PMCID": "PMC4349800",
    "DOI": "10.1371/journal.pone.0118432",
    "abstract": "Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets. ",
    "URL": "https://www.ncbi.nlm.nih.gov/pubmed/25738806",
    "type": "article-journal",
    "id": "KnxQ4G8",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: pubmed:25738806"
  },
  {
    "id": "wgOFUxdw",
    "type": "article-journal",
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "container-title": "The Journal of Machine Learning Research",
    "page": "1929–1958",
    "volume": "15",
    "issue": "1",
    "source": "January 2014",
    "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
    "ISSN": "1532-4435",
    "shortTitle": "Dropout",
    "journalAbbreviation": "J. Mach. Learn. Res.",
    "author": [
      {
        "family": "Srivastava",
        "given": "Nitish"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      },
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Salakhutdinov",
        "given": "Ruslan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014",
          1,
          1
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:http://dl.acm.org/citation.cfm?id=2670313"
  },
  {
    "id": "eR3C2hhK",
    "type": "paper-conference",
    "title": "A simple weight decay can improve generalization",
    "container-title": "Proceedings of the 4th International Conference on Neural Information Processing Systems",
    "collection-title": "NIPS'91",
    "publisher": "Morgan Kaufmann Publishers Inc.",
    "publisher-place": "Denver, Colorado",
    "page": "950–957",
    "source": "ACM Digital Library",
    "event-place": "Denver, Colorado",
    "abstract": "It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.",
    "ISBN": "9781558602229",
    "author": [
      {
        "family": "Krogh",
        "given": "Anders"
      },
      {
        "family": "Hertz",
        "given": "John A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1991",
          12,
          2
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          10,
          22
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:http://dl.acm.org/citation.cfm?id=2986916.2987033"
  },
  {
    "id": "R1RpVu06",
    "type": "article-journal",
    "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    "container-title": "Journal of Machine Learning Research",
    "page": "1929-1958",
    "volume": "15",
    "issue": "56",
    "source": "Journal of Machine Learning Research",
    "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
    "URL": "http://jmlr.org/papers/v15/srivastava14a.html",
    "shortTitle": "Dropout",
    "author": [
      {
        "family": "Srivastava",
        "given": "Nitish"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      },
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Salakhutdinov",
        "given": "Ruslan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          10,
          23
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:http://jmlr.csail.mit.edu/papers/v15/srivastava14a.html"
  },
  {
    "id": "pj5bK84R",
    "type": "book",
    "title": "Interpretable Machine Learning",
    "source": "christophm.github.io",
    "abstract": "Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.",
    "URL": "https://christophm.github.io/interpretable-ml-book/",
    "author": [
      {
        "family": "Molnar",
        "given": "Christoph"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2020",
          10,
          23
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:https://christophm.github.io/interpretable-ml-book"
  },
  {
    "id": "4oKcgKmU",
    "type": "paper-conference",
    "title": "Batch normalization: accelerating deep network training by reducing internal covariate shift",
    "container-title": "Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37",
    "collection-title": "ICML'15",
    "publisher": "JMLR.org",
    "publisher-place": "Lille, France",
    "page": "448–456",
    "source": "ACM Digital Library",
    "event-place": "Lille, France",
    "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",
    "shortTitle": "Batch normalization",
    "author": [
      {
        "family": "Ioffe",
        "given": "Sergey"
      },
      {
        "family": "Szegedy",
        "given": "Christian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2015",
          7,
          6
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          10,
          22
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:https://dl.acm.org/citation.cfm?id=3045118.3045167"
  },
  {
    "type": "webpage",
    "URL": "https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility",
    "title": "Deep Learning SDK Documentation",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          1
        ]
      ]
    },
    "author": [
      {
        "literal": "NVIDIA"
      }
    ],
    "note": "This CSL JSON Item was loaded by Manubot v0.3.1 from a manual reference file.\nmanual_reference_filename: manual-references.json\nstandard_id: url:https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility",
    "id": "1GSwNJdl7"
  },
  {
    "type": "webpage",
    "URL": "https://github.com/Benjamin-Lee/deep-rules",
    "title": "Benjamin-Lee/deep-rules GitHub repository",
    "container-title": "GitHub",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "author": [
      {
        "given": "Benjamin",
        "family": "Lee"
      }
    ],
    "note": "This CSL JSON Item was loaded by Manubot v0.3.1 from a manual reference file.\nmanual_reference_filename: manual-references.json\nstandard_id: url:https://github.com/Benjamin-Lee/deep-rules",
    "id": "ysdRl4lj"
  },
  {
    "id": "ndSzNxZQ",
    "type": "book",
    "title": "apple/turicreate",
    "publisher": "Apple",
    "genre": "C++",
    "source": "GitHub",
    "abstract": "Turi Create simplifies the development of custom machine learning models.",
    "URL": "https://github.com/apple/turicreate",
    "note": "original-date: 2017-12-01T00:42:04Z\nThis CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:https://github.com/apple/turicreate",
    "issued": {
      "date-parts": [
        [
          "2020",
          10,
          22
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          10,
          23
        ]
      ]
    }
  },
  {
    "id": "6acIRW4q",
    "type": "report",
    "title": "Open collaborative writing with Manubot",
    "publisher": "Manubot",
    "source": "greenelab.github.io",
    "URL": "https://greenelab.github.io/meta-review/",
    "language": "en-US",
    "author": [
      {
        "family": "Himmelstein",
        "given": "Daniel S."
      },
      {
        "family": "Rubinetti",
        "given": "Vincent"
      },
      {
        "family": "Slochower",
        "given": "David R."
      },
      {
        "family": "Hu",
        "given": "Dongbo"
      },
      {
        "family": "Malladi",
        "given": "Venkat S."
      },
      {
        "family": "Greene",
        "given": "Casey S."
      },
      {
        "family": "Gitter",
        "given": "Anthony"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2020",
          5,
          25
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          10,
          23
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:https://greenelab.github.io/meta-review"
  },
  {
    "id": "fMQbR11C",
    "type": "webpage",
    "title": "Keras: the Python deep learning API",
    "abstract": "Keras documentation",
    "URL": "https://keras.io/",
    "accessed": {
      "date-parts": [
        [
          "2020",
          10,
          23
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:https://keras.io"
  },
  {
    "id": "enhj7VT6",
    "type": "chapter",
    "title": "How transferable are features in deep neural networks?",
    "container-title": "Advances in Neural Information Processing Systems 27",
    "publisher": "Curran Associates, Inc.",
    "page": "3320–3328",
    "source": "Neural Information Processing Systems",
    "URL": "http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf",
    "author": [
      {
        "family": "Yosinski",
        "given": "Jason"
      },
      {
        "family": "Clune",
        "given": "Jeff"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      },
      {
        "family": "Lipson",
        "given": "Hod"
      }
    ],
    "editor": [
      {
        "family": "Ghahramani",
        "given": "Z."
      },
      {
        "family": "Welling",
        "given": "M."
      },
      {
        "family": "Cortes",
        "given": "C."
      },
      {
        "family": "Lawrence",
        "given": "N. D."
      },
      {
        "family": "Weinberger",
        "given": "K. Q."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          10,
          23
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:https://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks"
  },
  {
    "id": "mIx19cpn",
    "type": "chapter",
    "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning",
    "container-title": "Advances in Neural Information Processing Systems 30",
    "publisher": "Curran Associates, Inc.",
    "page": "4148–4158",
    "source": "Neural Information Processing Systems",
    "URL": "http://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf",
    "author": [
      {
        "family": "Wilson",
        "given": "Ashia C"
      },
      {
        "family": "Roelofs",
        "given": "Rebecca"
      },
      {
        "family": "Stern",
        "given": "Mitchell"
      },
      {
        "family": "Srebro",
        "given": "Nati"
      },
      {
        "family": "Recht",
        "given": "Benjamin"
      }
    ],
    "editor": [
      {
        "family": "Guyon",
        "given": "I."
      },
      {
        "family": "Luxburg",
        "given": "U. V."
      },
      {
        "family": "Bengio",
        "given": "S."
      },
      {
        "family": "Wallach",
        "given": "H."
      },
      {
        "family": "Fergus",
        "given": "R."
      },
      {
        "family": "Vishwanathan",
        "given": "S."
      },
      {
        "family": "Garnett",
        "given": "R."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2017"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2020",
          10,
          23
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.3.1 using citation-by-identifier.\nstandard_id: url:https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning"
  }
]
