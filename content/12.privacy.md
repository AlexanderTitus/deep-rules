## Tip 10: Don't share models trained on sensitive data {#privacy}

Practitioners may encounter datasets that cannot be shared, such as ones for which there would be significant ethical or legal issues associated with release [@doi:10.1371/journal.pcbi.1005399].
Examples of such data include classified or confidential data, biological data related to trade secrets (_e.g._ genomic assay data), medical records, and personally identifiable information [@doi:10.1038/s41576-020-0257-5].
While deep learning models can capture information-rich abstractions of multiple features of the data during the training process (which represents one of its great strengths), these features may be more prone to leak the data that they were trained over if the model is shared or allowed to be queried with arbitrary inputs.
In other words, the complex relationships learned about the input data can potentially be used to infer characterists about the original dataset.
This means that the strengths that embue deep learnings with its great predictive capacity may also serve as an achiles heal when it comes to data privacy.
Therefore, while there is tremendous promise for deep learning techniques to extract information that cannot readily be captured by traditional methods [@arxiv:1509.09292], it is imperative not to share models trained on sensitive data.
This also holds true for certain traditional machine learning methods that learn by capturing specific details of the full training data (e.g. _k_-nearest neighbors models). 

Techniques to train deep neural networks without sharing unencrypted access to data are being advanced through implementations of homomorphic encryption, which serves to enable equivalent prediction on data that is encrypted end to end [@doi:10.1371/journal.pcbi.1006454; @arxiv:1811.00778].
However, adversarial training techniques, such as model inversion attacks, can be used to exploit model predictions, as reflected by the capacity to recover recognizable images of people's faces used for training [@doi:10.1145/2810103.2813677].
Privacy preserving techniques [@arxiv:1811.04017], such as the differential privacy approach that describes group level data but not individual level data [@doi:10.1145/2976749.2978318; @doi:10.1101/159756; @arxiv:1812.01484], can help to mitigate risks as long as the assumptions underlying these techniques are met.
These approaches provide a path towards a future where models can be shared, but more software development and theoretical advances will be required to make the corresponding techniques easy to apply in many settings.
Until then, don't share models trained on sensitive data.
